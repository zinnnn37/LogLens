#!/usr/bin/env python3
"""
ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ë¡œê·¸ ë°ì´í„° ìƒì„± (1000ê°œ)
- ERROR: 5% (50ê°œ)
- WARN: 15% (150ê°œ)
- INFO: 80% (800ê°œ)
"""

import os
import sys
from datetime import datetime, timedelta
from opensearchpy import OpenSearch
from openai import OpenAI
from dotenv import load_dotenv
import random
import time

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

OPENSEARCH_HOST = os.getenv("OPENSEARCH_HOST", "localhost")
OPENSEARCH_PORT = int(os.getenv("OPENSEARCH_PORT", "9200"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")

# OpenSearch í´ë¼ì´ì–¸íŠ¸
os_client = OpenSearch(
    hosts=[{"host": OPENSEARCH_HOST, "port": OPENSEARCH_PORT}],
    http_auth=("admin", "admin"),
    use_ssl=False,
    verify_certs=False,
)

# OpenAI í´ë¼ì´ì–¸íŠ¸
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
)


def generate_timestamp(minutes_ago: int) -> str:
    """Në¶„ ì „ì˜ timestamp ìƒì„±"""
    timestamp = datetime.utcnow() - timedelta(minutes=minutes_ago)
    return timestamp.isoformat() + "Z"


def generate_embedding(text: str) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (1536ì°¨ì›)"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = openai_client.embeddings.create(
                model="text-embedding-3-large",
                input=text,
                encoding_format="float",
                dimensions=1536  # ëª…ì‹œì ìœ¼ë¡œ 1536ì°¨ì› ì§€ì •
            )
            vector = response.data[0].embedding
            if vector and len(vector) == 1536:
                return vector
            else:
                print(f"  âš ï¸  Embedding í˜•ì‹ ì˜¤ë¥˜ (attempt {attempt+1}): len={len(vector) if vector else 0}")
        except Exception as e:
            print(f"  âš ï¸  Embedding ìƒì„± ì‹¤íŒ¨ (attempt {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    print(f"  âŒ Embedding ìƒì„± ìµœì¢… ì‹¤íŒ¨")
    return None


# ë¡œê·¸ í…œí”Œë¦¿ ì •ì˜
ERROR_TEMPLATES = [
    "NullPointerException in {class_name}.{method}",
    "Database connection timeout in {class_name}",
    "Authentication failed for user request",
    "File not found: {file_path}",
    "OutOfMemoryError in {service}",
    "Invalid request parameter: {param}",
    "API rate limit exceeded",
    "Service {service} unavailable",
    "Payment transaction failed: {reason}",
    "Failed to parse JSON response",
]

WARN_TEMPLATES = [
    "Slow database query detected ({duration}ms)",
    "Cache miss for key: {key}",
    "Deprecated API endpoint used",
    "High memory usage: {usage}%",
    "Retry attempt {attempt} for {operation}",
    "SSL certificate expires in {days} days",
    "Unusual traffic pattern detected",
    "Queue size exceeding threshold",
    "Session timeout warning",
    "Disk usage above 80%",
]

INFO_TEMPLATES = [
    "User {user_id} logged in successfully",
    "Request processed in {duration}ms",
    "Cache hit for key: {key}",
    "Scheduled task {task} completed",
    "API call to {endpoint} successful",
    "File {filename} uploaded successfully",
    "Email sent to {recipient}",
    "Database backup completed",
    "Configuration reloaded",
    "Health check passed",
]

SERVICES = ["user-service", "payment-service", "auth-service", "product-service", "order-service"]
CLASSES = [
    "UserController", "PaymentService", "AuthController", "ProductRepository",
    "OrderProcessor", "CacheManager", "DatabaseConnection", "ApiGateway"
]
METHODS = [
    "createUser", "processPayment", "validateToken", "executeQuery",
    "handleRequest", "updateRecord", "deleteEntity", "fetchData"
]


def generate_log_message(level: str) -> str:
    """ë ˆë²¨ì— ë§ëŠ” ë¡œê·¸ ë©”ì‹œì§€ ìƒì„±"""
    if level == "ERROR":
        template = random.choice(ERROR_TEMPLATES)
        return template.format(
            class_name=random.choice(CLASSES),
            method=random.choice(METHODS),
            file_path=f"/data/file_{random.randint(1, 100)}.txt",
            service=random.choice(SERVICES),
            param=f"param_{random.randint(1, 10)}",
            reason=random.choice(["insufficient funds", "invalid card", "network error"])
        )
    elif level == "WARN":
        template = random.choice(WARN_TEMPLATES)
        return template.format(
            duration=random.randint(500, 5000),
            key=f"cache_key_{random.randint(1, 1000)}",
            usage=random.randint(70, 95),
            attempt=random.randint(1, 5),
            operation=random.choice(["database query", "API call", "file upload"]),
            days=random.randint(1, 30),
            task=f"task_{random.randint(1, 20)}"
        )
    else:  # INFO
        template = random.choice(INFO_TEMPLATES)
        return template.format(
            user_id=f"user_{random.randint(1000, 9999)}",
            duration=random.randint(10, 500),
            key=f"cache_key_{random.randint(1, 1000)}",
            task=f"task_{random.randint(1, 20)}",
            endpoint=f"/api/v1/{random.choice(['users', 'products', 'orders'])}",
            filename=f"document_{random.randint(1, 100)}.pdf",
            recipient=f"user{random.randint(1, 100)}@example.com"
        )


def insert_large_test_data():
    """ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…"""

    print("=" * 80)
    print("ğŸš€ ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ë¡œê·¸ ì‚½ì… ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 80)
    print()

    # OpenSearch ì—°ê²° í™•ì¸
    try:
        info = os_client.info()
        print(f"âœ… OpenSearch ì—°ê²° ì„±ê³µ: {info['version']['number']}")
    except Exception as e:
        print(f"âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # í”„ë¡œì íŠ¸ UUID
    project_uuid = "test-large-data-project"
    index_name = f"{project_uuid.replace('-', '_')}_2025_11"

    # ë¡œê·¸ ë¶„í¬ ì„¤ì •
    total_logs = 200  # 200ê°œë¡œ ì¶•ì†Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    error_count = 10   # 5%
    warn_count = 30   # 15%
    info_count = 160   # 80%

    print(f"ğŸ“ {total_logs}ê°œì˜ ë¡œê·¸ ì‚½ì… ì‹œì‘...")
    print(f"   ì¸ë±ìŠ¤: {index_name}")
    print(f"   ë¶„í¬: ERROR={error_count} (5%), WARN={warn_count} (15%), INFO={info_count} (80%)")
    print(f"   í˜„ì¬ UTC ì‹œê°„: {datetime.utcnow().isoformat()}Z")
    print()

    # ë¡œê·¸ ë ˆë²¨ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    levels = (
        ["ERROR"] * error_count +
        ["WARN"] * warn_count +
        ["INFO"] * info_count
    )
    random.shuffle(levels)  # ì„ì–´ì„œ ì‹œê°„ìˆœìœ¼ë¡œ ë‹¤ì–‘í•˜ê²Œ

    # ë°°ì¹˜ ì‚½ì… ì¤€ë¹„
    batch_size = 50
    batch_docs = []
    inserted_count = 0
    start_time = time.time()

    print(f"ğŸ”„ ë²¡í„° ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    print()

    for i, level in enumerate(levels, 1):
        # ë¡œê·¸ ë°ì´í„° ìƒì„±
        log_id = 1000 + i
        message = generate_log_message(level)
        service = random.choice(SERVICES)
        class_name = random.choice(CLASSES)
        method_name = random.choice(METHODS)

        # ì‹œê°„ì€ ìµœê·¼ 24ì‹œê°„ ë‚´ ëœë¤
        minutes_ago = random.randint(0, 24 * 60)
        timestamp = generate_timestamp(minutes_ago)

        # ë²¡í„° ìƒì„± í…ìŠ¤íŠ¸
        vector_text = f"{level} {service} {class_name}.{method_name} {message}"

        # Embedding ìƒì„± (ë°°ì¹˜ë§ˆë‹¤ í•œ ë²ˆì”© ì§„í–‰ ìƒí™© ì¶œë ¥)
        if i % batch_size == 1:
            print(f"  [{i}/{total_logs}] {level} ë¡œê·¸ ë²¡í„°í™” ì¤‘...")

        log_vector = generate_embedding(vector_text)

        if log_vector is None:
            print(f"  âš ï¸ ë¡œê·¸ {log_id} ë²¡í„° ìƒì„± ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
            continue

        # ë¡œê·¸ ë¬¸ì„œ
        doc = {
            "log_id": log_id,
            "project_uuid": project_uuid,
            "timestamp": timestamp,
            "level": level,
            "log_level": level,
            "message": message,
            "service_name": service,
            "logger": f"com.example.{service}",
            "source_type": "APPLICATION",
            "layer": "Service",
            "method_name": method_name,
            "class_name": f"com.example.{service}.{class_name}",
            "thread_name": f"http-nio-8080-exec-{random.randint(1, 10)}",
            "trace_id": f"trace-{random.randint(1000, 9999)}",
            "requester_ip": f"192.168.1.{random.randint(1, 254)}",
            "duration": random.randint(10, 1000),
            "indexed_at": datetime.utcnow().isoformat() + "Z",
            "@timestamp": timestamp,
            "log_vector": log_vector,
            "log_details": {
                "http_method": random.choice(["GET", "POST", "PUT", "DELETE"]),
                "response_status": 200 if level == "INFO" else (400 if level == "WARN" else 500),
                "request_uri": f"/api/v1/{random.choice(['users', 'products', 'orders'])}",
                "execution_time": random.randint(10, 1000),
                "class_name": f"com.example.{service}.{class_name}",
                "method_name": method_name
            }
        }

        batch_docs.append(doc)

        # ë°°ì¹˜ ì‚½ì…
        if len(batch_docs) >= batch_size:
            try:
                for doc in batch_docs:
                    os_client.index(index=index_name, body=doc, refresh=False)
                inserted_count += len(batch_docs)
                elapsed = time.time() - start_time
                rate = inserted_count / elapsed if elapsed > 0 else 0
                print(f"  âœ… {inserted_count}/{total_logs} ì‚½ì… ì™„ë£Œ ({rate:.1f} docs/sec)")
                batch_docs = []
            except Exception as e:
                print(f"  âŒ ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨: {e}")
                batch_docs = []

    # ë‚¨ì€ ë¬¸ì„œ ì‚½ì…
    if batch_docs:
        try:
            for doc in batch_docs:
                os_client.index(index=index_name, body=doc, refresh=False)
            inserted_count += len(batch_docs)
        except Exception as e:
            print(f"  âŒ ë§ˆì§€ë§‰ ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨: {e}")

    # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
    print()
    print("ğŸ”„ ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨ ì¤‘...")
    os_client.indices.refresh(index=index_name)

    elapsed = time.time() - start_time

    print()
    print("=" * 80)
    print("âœ… ì‚½ì… ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“Š í†µê³„:")
    print(f"   ì´ ì‚½ì… ë¬¸ì„œ: {inserted_count}ê°œ")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"   í‰ê·  ì†ë„: {inserted_count/elapsed:.1f} docs/sec")
    print()
    print(f"ğŸ“‹ ì¸ë±ìŠ¤ ì •ë³´:")

    # ì¸ë±ìŠ¤ í†µê³„
    try:
        count_result = os_client.count(index=index_name)
        print(f"   ì¸ë±ìŠ¤: {index_name}")
        print(f"   ì´ ë¬¸ì„œ ìˆ˜: {count_result['count']}ê°œ")

        # ë ˆë²¨ë³„ ì§‘ê³„
        agg_result = os_client.search(
            index=index_name,
            body={
                "size": 0,
                "aggs": {
                    "by_level": {
                        "terms": {"field": "level", "size": 10}
                    }
                }
            }
        )

        print(f"   ë ˆë²¨ë³„ ë¶„í¬:")
        for bucket in agg_result["aggregations"]["by_level"]["buckets"]:
            level = bucket["key"]
            count = bucket["doc_count"]
            percentage = count / count_result['count'] * 100
            print(f"     - {level}: {count}ê°œ ({percentage:.1f}%)")
    except Exception as e:
        print(f"   âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    print()
    print(f"âœ… í”„ë¡œì íŠ¸ UUID: {project_uuid}")
    print(f"âœ… ì¸ë±ìŠ¤: {index_name}")
    print()


if __name__ == "__main__":
    insert_large_test_data()
