# AI ì„œë¹„ìŠ¤ ì„¸ë¶€ í”Œë¡œìš° ê°€ì´ë“œ

> **LogLens AI Service** - ìƒì„¸ ê¸°ëŠ¥ í”Œë¡œìš° ë° ì•„í‚¤í…ì²˜ ë¬¸ì„œ
> ìœ„ì¹˜: `/mnt/c/SSAFY/third_project/AI/S13P31A306`
> ìµœì¢… ì—…ë°ì´íŠ¸: 2025-11-18

---

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [API ì—”ë“œí¬ì¸íŠ¸ ì „ì²´ ë§µ](#2-api-ì—”ë“œí¬ì¸íŠ¸-ì „ì²´-ë§µ)
3. [í•µì‹¬ ê¸°ëŠ¥ë³„ ì„¸ë¶€ í”Œë¡œìš°](#3-í•µì‹¬-ê¸°ëŠ¥ë³„-ì„¸ë¶€-í”Œë¡œìš°)
   - [3.1 ë¡œê·¸ ë¶„ì„ (V2 LangGraph) - 3-Tier ìºì‹± ì „ëµ](#31-ë¡œê·¸-ë¶„ì„-v2-langgraph---3-tier-ìºì‹±-ì „ëµ)
   - [3.2 RAG ì±—ë´‡ (V2 Agent) - ReAct ì•„í‚¤í…ì²˜](#32-rag-ì±—ë´‡-v2-agent---react-ì•„í‚¤í…ì²˜)
   - [3.3 HTML ë¬¸ì„œ ìƒì„±](#33-html-ë¬¸ì„œ-ìƒì„±)
   - [3.4 AI vs DB í†µê³„ ë¹„êµ](#34-ai-vs-db-í†µê³„-ë¹„êµ)
4. [LangChain/LangGraph ì•„í‚¤í…ì²˜](#4-langchainlanggraph-ì•„í‚¤í…ì²˜)
5. [40+ ë„êµ¬ ì™„ì „ ë¶„ì„](#5-40-ë„êµ¬-ì™„ì „-ë¶„ì„)
6. [ë°ì´í„° í”Œë¡œìš° ë° í†µí•©](#6-ë°ì´í„°-í”Œë¡œìš°-ë°-í†µí•©)
7. [í™˜ê²½ ì„¤ì • ë° êµ¬ì„±](#7-í™˜ê²½-ì„¤ì •-ë°-êµ¬ì„±)
8. [ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ëª¨ìŒ](#8-ì‹œí€€ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨-ëª¨ìŒ)
9. [ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë²” ì‚¬ë¡€](#9-ì„±ëŠ¥-ìµœì í™”-ë°-ëª¨ë²”-ì‚¬ë¡€)

---

## 1. ê°œìš”

### 1.1 AI ì„œë¹„ìŠ¤ë€?

LogLens AI ì„œë¹„ìŠ¤ëŠ” **FastAPI ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ë¡œê·¸ ë¶„ì„ í”Œë«í¼**ìœ¼ë¡œ, ë‹¤ìŒ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- ğŸ¤– **AI ê¸°ë°˜ ë¡œê·¸ ë¶„ì„**: GPT-4o minië¥¼ í™œìš©í•œ ìë™ ì—ëŸ¬ ì›ì¸ ë¶„ì„ ë° í•´ê²°ì±… ì œì‹œ
- ğŸ’¬ **RAG ì±—ë´‡**: 40+ ì „ë¬¸ ë„êµ¬ë¥¼ í™œìš©í•œ ìì—°ì–´ ë¡œê·¸ ì§ˆì˜ì‘ë‹µ
- ğŸ“Š **HTML ë¬¸ì„œ ìë™ ìƒì„±**: í”„ë¡œì íŠ¸/ì—ëŸ¬ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
- ğŸ“ˆ **AI vs DB í†µê³„ ë¹„êµ**: LLM ì¶”ë¡  ì •í™•ë„ ê²€ì¦

### 1.2 ê¸°ìˆ  ìŠ¤íƒ

| êµ¬ì„± ìš”ì†Œ | ê¸°ìˆ  | ë²„ì „ |
|---------|------|------|
| **Framework** | FastAPI | 0.109.0 |
| **AI Orchestration** | LangChain | 0.2.16 |
| **Workflow Engine** | LangGraph | 0.2.28 |
| **LLM** | OpenAI GPT-4o mini | - |
| **Embeddings** | text-embedding-3-large | 1536 dims |
| **Vector DB** | OpenSearch (KNN) | 2.4.2 |
| **Template Engine** | Jinja2 | 3.1.2 |
| **Runtime** | Python | 3.11 |

### 1.3 ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

```mermaid
graph TB
    subgraph "Frontend (React)"
        FE[ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤]
    end

    subgraph "Backend (Spring Boot)"
        BE[REST API Controller]
        AiClient[AI Service Client]
    end

    subgraph "AI Service (FastAPI)"
        direction TB
        API[FastAPI Router]

        subgraph "Core Services"
            LogAnalysis[Log Analysis Service]
            Chatbot[Chatbot Service V2]
            HTMLDoc[HTML Document Service]
        end

        subgraph "LangChain/LangGraph"
            Graph[LogAnalysisGraph<br/>9-node workflow]
            Agent[ReAct Agent<br/>40+ tools]
        end

        subgraph "Tools & Chains"
            Tools[Specialized Tools]
            Chains[LangChain Chains]
        end

        API --> LogAnalysis
        API --> Chatbot
        API --> HTMLDoc

        LogAnalysis --> Graph
        Chatbot --> Agent
        Agent --> Tools
        Graph --> Tools
    end

    subgraph "Data Layer"
        OS[(OpenSearch<br/>Logs + Vectors)]
        OpenAI[OpenAI API<br/>GPT-4o mini]
    end

    FE -->|HTTPS| BE
    BE -->|REST| API
    Tools -->|Query| OS
    Tools -->|LLM Call| OpenAI
    Graph -->|Cache Check| OS

    style AI Service fill:#e1f5ff
    style LangChain/LangGraph fill:#fff4e6
```

### 1.4 ì£¼ìš” íŠ¹ì§•

#### âœ¨ 3-Tier ìºì‹± ì „ëµ
- **Direct Cache**: ì´ë¯¸ ë¶„ì„ëœ ë¡œê·¸ ì¬ì‚¬ìš©
- **Trace Cache**: ë™ì¼ traceì˜ ë‹¤ë¥¸ ë¡œê·¸ ë¶„ì„ ì¬ì‚¬ìš©
- **Similarity Cache**: ë²¡í„° ìœ ì‚¬ë„ 0.92 ì´ìƒì¸ ë¡œê·¸ ë¶„ì„ ì¬ì‚¬ìš©
- **íš¨ê³¼**: 97-99% ë¹„ìš© ì ˆê°

#### ğŸ¤– ììœ¨ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- **ReAct íŒ¨í„´**: Reasoning + Acting
- **40+ ì „ë¬¸ ë„êµ¬**: ê²€ìƒ‰, ë¶„ì„, ëª¨ë‹ˆí„°ë§, ì„±ëŠ¥, íŒ¨í„´ íƒì§€ ë“±
- **ìë™ ë„êµ¬ ì„ íƒ**: GPT-4o miniê°€ ìƒí™©ì— ë§ëŠ” ë„êµ¬ ìë™ ì„ íƒ

#### ğŸ”„ Map-Reduce íŒ¨í„´
- **ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì²˜ë¦¬**: 100ê°œ ë¡œê·¸ë¥¼ 5ê°œì”© ì²­í¬ë¡œ ë¶„í• 
- **ë³‘ë ¬ ì²˜ë¦¬**: ê° ì²­í¬ ë…ë¦½ ë¶„ì„ í›„ í†µí•©
- **í† í° ìµœì í™”**: ë‹¨ì¼ í˜¸ì¶œ ëŒ€ë¹„ íš¨ìœ¨ì 

#### âœ… ê²€ì¦ íŒŒì´í”„ë¼ì¸
- **í•œêµ­ì–´ ê²€ì¦**: 90% ì´ìƒ í•œêµ­ì–´ ë¬¸ì í¬í•¨ í™•ì¸
- **í’ˆì§ˆ ê²€ì¦**: êµ¬ì¡°ì  ì™„ì „ì„±, ë‚´ìš© ì •í™•ì„± í‰ê°€
- **ì¬ì‹œë„ ë¡œì§**: ìµœëŒ€ 2íšŒ ì¬ë¶„ì„

---

## 2. API ì—”ë“œí¬ì¸íŠ¸ ì „ì²´ ë§µ

### 2.1 ë²„ì „ë³„ API êµ¬ì¡°

```
/api
â”œâ”€â”€ /v1                        # ê¸°ë³¸ ë¶„ì„ (ì´ˆê¸° ë²„ì „)
â”‚   â”œâ”€â”€ /logs/{log_id}/analysis
â”‚   â”œâ”€â”€ /chatbot/ask
â”‚   â””â”€â”€ /chatbot/ask/stream
â”‚
â”œâ”€â”€ /v2                        # ReAct Agent ì±—ë´‡
â”‚   â”œâ”€â”€ /chatbot/ask
â”‚   â””â”€â”€ /chatbot/ask/stream
â”‚
â””â”€â”€ /v2-langgraph              # LangGraph ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„
    â”œâ”€â”€ /logs/{log_id}/analysis
    â”œâ”€â”€ /analysis
    â”‚   â”œâ”€â”€ /projects/html-document
    â”‚   â””â”€â”€ /errors/html-document
    â””â”€â”€ /statistics
        â”œâ”€â”€ /compare
        â””â”€â”€ /hourly
```

### 2.2 ì „ì²´ ì—”ë“œí¬ì¸íŠ¸ ìƒì„¸

| ë©”ì†Œë“œ | ì—”ë“œí¬ì¸íŠ¸ | ì„¤ëª… | íŒŒì¼ | ì£¼ìš” ê¸°ëŠ¥ |
|--------|-----------|------|------|----------|
| **V1 APIs** |
| `GET` | `/api/v1/logs/{log_id}/analysis` | ê¸°ë³¸ ë¡œê·¸ ë¶„ì„ | `app/api/v1/logs.py:85` | Trace ìºì‹±, ìœ ì‚¬ë„ ê²€ìƒ‰ |
| `POST` | `/api/v1/chatbot/ask` | ê¸°ë³¸ ì±—ë´‡ | `app/api/v1/chatbot.py:171` | RAG QA ìºì‹± |
| `POST` | `/api/v1/chatbot/ask/stream` | ê¸°ë³¸ ì±—ë´‡ (ìŠ¤íŠ¸ë¦¬ë°) | `app/api/v1/chatbot.py:314` | SSE ìŠ¤íŠ¸ë¦¬ë° |
| **V2 APIs (Agent)** |
| `POST` | `/api/v2/chatbot/ask` | ReAct Agent ì±—ë´‡ | `app/api/v2/chatbot.py:209` | 40+ ë„êµ¬, ììœ¨ ì¶”ë¡  |
| `POST` | `/api/v2/chatbot/ask/stream` | Agent ì±—ë´‡ (ìŠ¤íŠ¸ë¦¬ë°) | `app/api/v2/chatbot.py:314` | SSE + Agent ì¶”ë¡  |
| **V2 LangGraph APIs** |
| `GET` | `/api/v2-langgraph/logs/{log_id}/analysis` | LangGraph ë¡œê·¸ ë¶„ì„ | `app/api/v2_langgraph/logs.py:18` | 3-tier ìºì‹±, Map-Reduce |
| `POST` | `/api/v2-langgraph/analysis/projects/html-document` | í”„ë¡œì íŠ¸ ë¶„ì„ ë¬¸ì„œ | `app/api/v2_langgraph/analysis.py:21` | Jinja2 HTML ìƒì„± |
| `POST` | `/api/v2-langgraph/analysis/errors/html-document` | ì—ëŸ¬ ë¶„ì„ ë¬¸ì„œ | `app/api/v2_langgraph/analysis.py:189` | ì—ëŸ¬ ë¦¬í¬íŠ¸ ìƒì„± |
| `GET` | `/api/v2-langgraph/statistics/compare` | AI vs DB ë¹„êµ | `app/api/v2_langgraph/statistics.py:104` | ì •í™•ë„ ê²€ì¦ |
| `GET` | `/api/v2-langgraph/statistics/hourly` | ì‹œê°„ë³„ í†µê³„ | `app/api/v2_langgraph/statistics.py:290` | ì‹œê°„ëŒ€ë³„ ë¶„í¬ |

### 2.3 API ë²„ì „ ì„ íƒ ê°€ì´ë“œ

```mermaid
graph TD
    Start{ì–´ë–¤ ê¸°ëŠ¥ì´ í•„ìš”í•œê°€?}

    Start -->|ë‹¨ì¼ ë¡œê·¸ AI ë¶„ì„| V2LG[âœ… V2 LangGraph<br/>/logs/{id}/analysis]
    Start -->|ìì—°ì–´ ì§ˆì˜ì‘ë‹µ| V2Agent[âœ… V2 Agent<br/>/chatbot/ask/stream]
    Start -->|ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±| V2Doc[âœ… V2 LangGraph<br/>/analysis/.../html-document]
    Start -->|AI ì •í™•ë„ ê²€ì¦| V2Stats[âœ… V2 LangGraph<br/>/statistics/compare]

    V2LG --> Features1[â€¢ 3-tier ìºì‹±<br/>â€¢ Map-Reduce<br/>â€¢ ê²€ì¦ íŒŒì´í”„ë¼ì¸]
    V2Agent --> Features2[â€¢ 40+ ë„êµ¬<br/>â€¢ ììœ¨ ì¶”ë¡ <br/>â€¢ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°]
    V2Doc --> Features3[â€¢ Jinja2 í…œí”Œë¦¿<br/>â€¢ Chart.js ì°¨íŠ¸<br/>â€¢ AI ì¸ì‚¬ì´íŠ¸]
    V2Stats --> Features4[â€¢ ì¸µí™” ìƒ˜í”Œë§<br/>â€¢ ì •í™•ë„ ê³„ì‚°<br/>â€¢ ì‹ ë¢°ë„ í‰ê°€]

    style V2LG fill:#d4edda
    style V2Agent fill:#d4edda
    style V2Doc fill:#d4edda
    style V2Stats fill:#d4edda
```

**ê¶Œì¥ì‚¬í•­:**
- âœ… **V2 LangGraph**: ë¡œê·¸ ë¶„ì„, ë¬¸ì„œ ìƒì„± â†’ ê°€ì¥ ê³ ê¸‰ ê¸°ëŠ¥
- âœ… **V2 Agent**: ì±—ë´‡ â†’ ììœ¨ì  ë„êµ¬ ì„ íƒ
- âš ï¸ **V1**: ë ˆê±°ì‹œ, ìƒˆ ê¸°ëŠ¥ ê°œë°œ ì¤‘ë‹¨

---

## 3. í•µì‹¬ ê¸°ëŠ¥ë³„ ì„¸ë¶€ í”Œë¡œìš°

## 3.1 ë¡œê·¸ ë¶„ì„ (V2 LangGraph) - 3-Tier ìºì‹± ì „ëµ

### 3.1.1 ì „ì²´ í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    Start([API ìš”ì²­<br/>log_id + project_uuid]) --> Node1[Node 1: Fetch Log<br/>OpenSearch ì¡°íšŒ]

    Node1 --> Node2{Node 2: Direct Cache<br/>ai_analysis ì¡´ì¬?}
    Node2 -->|âœ… Cache Hit| Jump1[ğŸ¯ Node 9ë¡œ ì í”„]
    Node2 -->|âŒ Cache Miss| Node3{Node 3: Trace Cache<br/>ê°™ì€ trace_idì—<br/>ë¶„ì„ ì¡´ì¬?}

    Node3 -->|âœ… Cache Hit| Jump2[ğŸ¯ Node 9ë¡œ ì í”„]
    Node3 -->|âŒ Cache Miss| Node4{Node 4: Similarity Cache<br/>ìœ ì‚¬ ë¡œê·¸<br/>cosine â‰¥ 0.92?}

    Node4 -->|âœ… Cache Hit| Jump3[ğŸ¯ Node 9ë¡œ ì í”„]
    Node4 -->|âŒ Cache Miss<br/>ì‹ ê·œ ë¶„ì„ í•„ìš”| Node5[Node 5: Collect Logs<br/>trace ë‚´ ê´€ë ¨ ë¡œê·¸<br/>Â±3ì´ˆ ìœˆë„ìš°]

    Node5 --> Node6{Node 6: Decide Strategy<br/>ë¡œê·¸ ê°œìˆ˜?}

    Node6 -->|1ê°œ| Node7a[Node 7a: Single<br/>ë‹¨ì¼ ë¡œê·¸ GPT ë¶„ì„]
    Node6 -->|2-10ê°œ| Node7b[Node 7b: Direct<br/>í†µí•© ì»¨í…ìŠ¤íŠ¸ ë¶„ì„]
    Node6 -->|11-100ê°œ| Node7c[Node 7c: Map-Reduce<br/>ì²­í¬ë³„ ë¶„ì„ í›„ í†µí•©]

    Node7a --> Node8[Node 8: Validate<br/>í•œêµ­ì–´ + í’ˆì§ˆ ê²€ì¦]
    Node7b --> Node8
    Node7c --> Node8

    Node8 -->|âœ… Pass| Node9[Node 9: Save Result<br/>OpenSearch ì—…ë°ì´íŠ¸<br/>+ Trace ì „íŒŒ]
    Node8 -->|âŒ Fail<br/>retry < MAX| Retry[ì¬ë¶„ì„]
    Node8 -->|âŒ Max Retry| Node9

    Retry --> Node7a

    Jump1 --> Node9
    Jump2 --> Node9
    Jump3 --> Node9

    Node9 --> End([LogAnalysisResponse ë°˜í™˜<br/>cache_type í¬í•¨])

    style Node2 fill:#fff3cd
    style Node3 fill:#fff3cd
    style Node4 fill:#fff3cd
    style Node7a fill:#d1ecf1
    style Node7b fill:#d1ecf1
    style Node7c fill:#d1ecf1
    style Node8 fill:#f8d7da
    style Node9 fill:#d4edda
```

### 3.1.2 ë…¸ë“œë³„ ìƒì„¸ ì„¤ëª…

#### **Node 1: Fetch Log** (`_fetch_log_node` - `app/graphs/log_analysis_graph.py:148`)

**ëª©ì **: OpenSearchì—ì„œ ìš”ì²­ëœ ë¡œê·¸ ë°ì´í„° ì¡°íšŒ

**ì…ë ¥**:
```python
state = {
    "log_id": 12345,
    "project_uuid": "3a73c7d4-8176-3929-b72f-d5b921daae67"
}
```

**OpenSearch ì¿¼ë¦¬**:
```json
{
  "query": {
    "bool": {
      "must": [
        {"term": {"log_id": 12345}},
        {"term": {"project_uuid.keyword": "3a73c7d4-8176-3929-b72f-d5b921daae67"}}
      ]
    }
  },
  "size": 1
}
```

**ì¶œë ¥**:
```python
{
    "log_data": {
        "log_id": 12345,
        "timestamp": "2025-11-18T10:30:00.123Z",
        "level": "ERROR",
        "message": "NullPointerException at UserService.getUser()",
        "service_name": "user-service",
        "component_name": "UserController",
        "trace_id": "abc123-def456",
        "log_details": {
            "class_name": "com.example.UserController",
            "method_name": "getUser",
            "line_number": 45,
            "stack_trace": "java.lang.NullPointerException\n\tat com.example..."
        }
    },
    "trace_id": "abc123-def456",
    "timestamp": "2025-11-18T10:30:00.123Z",
    "log_message": "NullPointerException at UserService.getUser()",
    "log_level": "ERROR"
}
```

**ì—ëŸ¬ ì²˜ë¦¬**:
- ë¡œê·¸ ë¯¸ë°œê²¬ ì‹œ: `{"error": "Log not found"}` â†’ ì¦‰ì‹œ ì¢…ë£Œ

---

#### **Node 2: Direct Cache Check** (`_check_direct_cache_node` - `app/graphs/log_analysis_graph.py:197`)

**ëª©ì **: ìš”ì²­í•œ ë¡œê·¸ ìì²´ì— ì´ë¯¸ `ai_analysis` í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸

**ë„êµ¬**: `cache_tools.create_check_direct_cache_tool()`

**OpenSearch ì¿¼ë¦¬**:
```json
{
  "query": {
    "bool": {
      "must": [
        {"term": {"log_id": 12345}},
        {"term": {"project_uuid.keyword": "3a73c7d4-8176-3929-b72f-d5b921daae67"}},
        {"exists": {"field": "ai_analysis"}}
      ]
    }
  }
}
```

**Cache Hit ì˜ˆì‹œ**:
```python
{
    "direct_cache_result": {
        "summary": "**NullPointerException** ë°œìƒ",
        "error_cause": "user_idì— í•´ë‹¹í•˜ëŠ” User ê°ì²´ null",
        "solution": "### ì¦‰ì‹œ ì¡°ì¹˜\n- null ì²´í¬ ì¶”ê°€",
        "tags": ["SEVERITY_HIGH", "NullPointerException"]
    },
    "from_cache": True,
    "cache_type": "direct"
}
```

**ë‹¤ìŒ ë…¸ë“œ**:
- âœ… Cache Hit â†’ **Node 9 (Save Result)ë¡œ ì í”„**
- âŒ Cache Miss â†’ Node 3ìœ¼ë¡œ ì§„í–‰

**ì„±ëŠ¥**:
- í‰ê·  ì‘ë‹µ ì‹œê°„: ~50ms
- ë¹„ìš©: $0 (LLM í˜¸ì¶œ ì—†ìŒ)

---

#### **Node 3: Trace Cache Check** (`_check_trace_cache_node` - `app/graphs/log_analysis_graph.py:234`)

**ëª©ì **: ë™ì¼ `trace_id`ë¥¼ ê°€ì§„ ë‹¤ë¥¸ ë¡œê·¸ ì¤‘ ë¶„ì„ì´ ìˆëŠ”ì§€ í™•ì¸

**ë„êµ¬**: `cache_tools.create_check_trace_cache_tool()`

**ë¡œì§**:
```python
# ê°™ì€ traceì˜ ë‹¤ë¥¸ ë¡œê·¸ ì¤‘ ai_analysisê°€ ìˆëŠ” ë¡œê·¸ ê²€ìƒ‰
query = {
    "query": {
        "bool": {
            "must": [
                {"term": {"trace_id.keyword": "abc123-def456"}},
                {"term": {"project_uuid.keyword": "..."}},
                {"exists": {"field": "ai_analysis"}}
            ],
            "must_not": [
                {"term": {"log_id": 12345}}  # ìê¸° ìì‹  ì œì™¸
            ]
        }
    },
    "size": 1
}
```

**ì™œ ìœ íš¨í•œê°€?**
- ë™ì¼ traceì˜ ë¡œê·¸ë“¤ì€ **ê°™ì€ ìš”ì²­ íë¦„**ì—ì„œ ë°œìƒ
- ì—ëŸ¬ ì›ì¸ê³¼ í•´ê²°ì±…ì´ **ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸**
- ì˜ˆ: Controller â†’ Service â†’ Repository ìˆœì„œì˜ ì—ëŸ¬ ì „íŒŒ

**Cache Hit ì˜ˆì‹œ**:
```python
{
    "trace_cache_result": {
        "cached_from_log_id": 12346,  # í˜•ì œ ë¡œê·¸
        "summary": "DB ì—°ê²° íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ NullPointerException",
        "error_cause": "HikariCP connection timeout 30s ì´ˆê³¼",
        "solution": "...",
        "tags": ["SEVERITY_HIGH", "DatabaseTimeout"]
    },
    "from_cache": True,
    "cache_type": "trace"
}
```

**ë‹¤ìŒ ë…¸ë“œ**:
- âœ… Cache Hit â†’ **Node 9ë¡œ ì í”„**
- âŒ Cache Miss (trace_id ì—†ìŒ or í˜•ì œ ë¡œê·¸ ë¯¸ë¶„ì„) â†’ Node 4ë¡œ ì§„í–‰

---

#### **Node 4: Similarity Cache Check** (`_check_similarity_cache_node` - `app/graphs/log_analysis_graph.py:267`)

**ëª©ì **: ë²¡í„° ìœ ì‚¬ë„ê°€ ë†’ì€ ë¡œê·¸ì˜ ë¶„ì„ ì¬ì‚¬ìš©

**ë„êµ¬**: `cache_tools.create_check_similarity_cache_tool()`

**ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤**:

**Step 1: ì„ë² ë”© ìƒì„±**
```python
from app.services.embedding_service import embedding_service

log_message = "NullPointerException at UserService.getUser()"
log_vector = await embedding_service.embed_query(log_message)
# ê²°ê³¼: [0.123, -0.456, 0.789, ..., 0.234]  # 1536 ì°¨ì›
```

**Step 2: ë©”íƒ€ë°ì´í„° í•„í„° ì„¤ì •**
```python
metadata_filter = {
    "level": "ERROR",              # ë™ì¼ ë¡œê·¸ ë ˆë²¨
    "service_name": "user-service", # ë™ì¼ ì„œë¹„ìŠ¤
    "source_type": "BE"            # ë™ì¼ ì†ŒìŠ¤ (FE/BE)
}
```

**Step 3: KNN ë²¡í„° ê²€ìƒ‰**
```json
{
  "size": 10,
  "query": {
    "script_score": {
      "query": {
        "bool": {
          "must": [
            {"term": {"project_uuid.keyword": "..."}},
            {"exists": {"field": "ai_analysis"}},
            {"term": {"level": "ERROR"}},
            {"term": {"service_name": "user-service"}},
            {"term": {"source_type": "BE"}}
          ],
          "must_not": [
            {"term": {"log_id": 12345}}
          ]
        }
      },
      "script": {
        "source": "cosineSimilarity(params.query_vector, 'log_vector') + 1.0",
        "params": {"query_vector": [0.123, -0.456, ...]}
      }
    }
  }
}
```

**Step 4: ì„ê³„ê°’ í•„í„°ë§**
```python
SIMILARITY_THRESHOLD = 0.92  # config.py:34

for hit in results["hits"]["hits"]:
    similarity_score = (hit["_score"] - 1.0)  # +1.0 ì˜¤í”„ì…‹ ì œê±°

    if similarity_score >= SIMILARITY_THRESHOLD:
        return {
            "similarity_cache_result": hit["_source"]["ai_analysis"],
            "similar_log_id": hit["_source"]["log_id"],
            "similarity_score": similarity_score,
            "from_cache": True,
            "cache_type": "similarity"
        }
```

**Cosine Similarity ê³„ì‚°**:
```
cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)

ì˜ˆì‹œ:
Vector A (ìš”ì²­ ë¡œê·¸): [0.5, 0.3, 0.2, ...]
Vector B (ìºì‹œ ë¡œê·¸): [0.51, 0.29, 0.21, ...]

cos(Î¸) = 0.947 âœ… (ì„ê³„ê°’ 0.92 ì´ˆê³¼ â†’ Cache Hit)
```

**ìœ ì‚¬ë„ ì„ê³„ê°’ ì„ íƒ ê·¼ê±°**:

| ì„ê³„ê°’ | ì˜ë¯¸ | ê²°ê³¼ |
|--------|------|------|
| **0.92** | ê±°ì˜ ë™ì¼í•œ ì—ëŸ¬ | âœ… ì •í™•ë„ ë†’ìŒ, ì¬ì‚¬ìš© ì•ˆì „ |
| 0.85 | ìœ ì‚¬í•œ ì—ëŸ¬ | âš ï¸ ì¼ë¶€ ì˜¤íƒ ê°€ëŠ¥ |
| 0.70 | ê´€ë ¨ ìˆëŠ” ì—ëŸ¬ | âŒ ì˜¤íƒ ë†’ìŒ, ë¶€ì í•© |

**Cache Hit ì˜ˆì‹œ**:
```python
{
    "similarity_cache_result": {
        "summary": "**NullPointerException** ë°œìƒ",
        "error_cause": "userId íŒŒë¼ë¯¸í„° null ì²´í¬ ëˆ„ë½",
        "solution": "...",
        "tags": ["SEVERITY_HIGH", "NullPointerException"]
    },
    "similar_log_id": 12340,
    "similarity_score": 0.947,
    "from_cache": True,
    "cache_type": "similarity"
}
```

**ë‹¤ìŒ ë…¸ë“œ**:
- âœ… Cache Hit â†’ **Node 9ë¡œ ì í”„**
- âŒ Cache Miss â†’ Node 5ë¡œ ì§„í–‰ (ì‹ ê·œ ë¶„ì„ í•„ìš”)

**ì„±ëŠ¥**:
- í‰ê·  ê²€ìƒ‰ ì‹œê°„: ~100ms (HNSW ì•Œê³ ë¦¬ì¦˜)
- ì„ë² ë”© ìƒì„± ë¹„ìš©: ~$0.00013/1K tokens

---

#### **Node 5: Collect Logs** (`_collect_logs_node` - `app/graphs/log_analysis_graph.py:330`)

**ëª©ì **: Trace ê¸°ë°˜ ê´€ë ¨ ë¡œê·¸ ìˆ˜ì§‘ (ì»¨í…ìŠ¤íŠ¸ í™•ë³´)

**ë„êµ¬**: `analysis_tools.create_collect_trace_logs_tool()`

**ìˆ˜ì§‘ ì „ëµ**:
```python
# ì¤‘ì‹¬ ë¡œê·¸ ì‹œê°„ ê¸°ì¤€ Â±3ì´ˆ ìœˆë„ìš°
center_timestamp = "2025-11-18T10:30:00.123Z"
time_window = {
    "gte": center_timestamp - 3_seconds,  # 10:29:57
    "lte": center_timestamp + 3_seconds   # 10:30:03
}
```

**OpenSearch ì¿¼ë¦¬**:
```json
{
  "query": {
    "bool": {
      "must": [
        {"term": {"trace_id.keyword": "abc123-def456"}},
        {"term": {"project_uuid.keyword": "..."}},
        {"range": {"timestamp": {
          "gte": "2025-11-18T10:29:57.123Z",
          "lte": "2025-11-18T10:30:03.123Z"
        }}}
      ]
    }
  },
  "size": 100,
  "sort": [{"timestamp": "asc"}]
}
```

**ìˆ˜ì§‘ ê²°ê³¼ ì˜ˆì‹œ**:
```python
{
    "related_logs": [
        {
            "log_id": 12344,
            "timestamp": "2025-11-18T10:29:59.500Z",
            "level": "INFO",
            "message": "GET /api/users/12345 request received",
            "component_name": "UserController"
        },
        {
            "log_id": 12345,
            "timestamp": "2025-11-18T10:30:00.123Z",
            "level": "ERROR",
            "message": "NullPointerException at UserService.getUser()",
            "component_name": "UserService"
        },
        {
            "log_id": 12346,
            "timestamp": "2025-11-18T10:30:00.234Z",
            "level": "ERROR",
            "message": "500 Internal Server Error",
            "component_name": "UserController"
        }
    ],
    "log_count": 3
}
```

**ì‹œê°„ ìœˆë„ìš° ì„ íƒ ê·¼ê±°**:
- âœ… **Â±3ì´ˆ**: ëŒ€ë¶€ë¶„ì˜ ìš”ì²­ì€ 3ì´ˆ ë‚´ ì™„ë£Œ
- âš ï¸ Â±1ì´ˆ: ëŠë¦° ìš”ì²­ ëˆ„ë½ ê°€ëŠ¥
- âŒ Â±10ì´ˆ: ë¬´ê´€í•œ ë¡œê·¸ í¬í•¨ ê°€ëŠ¥

**ë‹¤ìŒ ë…¸ë“œ**: Node 6 (ì „ëµ ê²°ì •)

---

#### **Node 6: Decide Strategy** (`_decide_strategy_node` - `app/graphs/log_analysis_graph.py:369`)

**ëª©ì **: ë¡œê·¸ ê°œìˆ˜ì— ë”°ë¥¸ ìµœì  ë¶„ì„ ì „ëµ ì„ íƒ

**ì˜ì‚¬ê²°ì • ë¡œì§**:
```python
MAP_REDUCE_THRESHOLD = 10  # config.py:28

if log_count == 1:
    analysis_method = "single"
elif log_count <= MAP_REDUCE_THRESHOLD:
    analysis_method = "direct"
else:
    analysis_method = "map_reduce"
```

**ì „ëµ ë¹„êµ**:

| ì „ëµ | ë¡œê·¸ ê°œìˆ˜ | LLM í˜¸ì¶œ | í† í° ì‚¬ìš© | ì†ë„ | ì •í™•ë„ |
|------|----------|---------|---------|------|--------|
| **Single** | 1ê°œ | 1íšŒ | ~500 tokens | âš¡ ë¹ ë¦„ | â­â­â­ ë†’ìŒ |
| **Direct** | 2-10ê°œ | 1íšŒ | ~2000 tokens | âš¡ ë¹ ë¦„ | â­â­â­â­ ë§¤ìš° ë†’ìŒ |
| **Map-Reduce** | 11-100ê°œ | N+1íšŒ | ~NÃ—500 tokens | ğŸ¢ ëŠë¦¼ | â­â­â­â­â­ ìµœê³  |

**ì¶œë ¥**:
```python
{
    "analysis_method": "direct",  # or "single", "map_reduce"
    "log_count": 3
}
```

**ë‹¤ìŒ ë…¸ë“œ**: Node 7a/7b/7c (ì„ íƒëœ ì „ëµ)

---

#### **Node 7a: Single Log Analysis** (`_analyze_node` - `app/graphs/log_analysis_graph.py:386`)

**ëª©ì **: ë‹¨ì¼ ë¡œê·¸ ì§ì ‘ ë¶„ì„

**ë„êµ¬**: `analysis_tools.create_analyze_single_log_tool()`

**í”„ë¡¬í”„íŠ¸**:
```python
prompt = f"""ë‹¹ì‹ ì€ ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

ë¡œê·¸ ë°ì´í„°:
```json
{{
    "timestamp": "2025-11-18T10:30:00.123Z",
    "level": "ERROR",
    "message": "NullPointerException at UserService.getUser()",
    "service_name": "user-service",
    "component_name": "UserController",
    "log_details": {{
        "class_name": "com.example.UserController",
        "method_name": "getUser",
        "stack_trace": "java.lang.NullPointerException\\n\\tat com.example.UserService.getUser(UserService.java:45)"
    }}
}}
```

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. **summary**: ì—ëŸ¬ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ (í•œêµ­ì–´, ë§ˆí¬ë‹¤ìš´ **êµµê²Œ** ê°•ì¡° ì‚¬ìš©)
2. **error_cause**: ë°œìƒ ì›ì¸ì„ ìƒì„¸íˆ ì„¤ëª… (2-3ë¬¸ì¥)
3. **solution**: í•´ê²° ë°©ë²•ì„ 3ë‹¨ê³„ë¡œ ì œì‹œ
   - ì¦‰ì‹œ ì¡°ì¹˜ (1ì‹œê°„ ì´ë‚´)
   - ë‹¨ê¸° ì¡°ì¹˜ (1ì¼ ì´ë‚´)
   - ì¥ê¸° ì¡°ì¹˜ (1ì£¼ ì´ë‚´)
   ê° ë‹¨ê³„ëŠ” ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•ì‹ (- [ ] ...)
4. **tags**: ë¶„ë¥˜ íƒœê·¸ (ì˜ˆ: SEVERITY_HIGH, NullPointerException, UserService)

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
    "summary": "**ì—ëŸ¬ëª…** ê°„ë‹¨ ì„¤ëª…",
    "error_cause": "ì›ì¸ ìƒì„¸ ì„¤ëª…",
    "solution": "### ì¦‰ì‹œ ì¡°ì¹˜ (1ì‹œê°„)\\n- [ ] ì¡°ì¹˜1\\n### ë‹¨ê¸° ì¡°ì¹˜ (1ì¼)\\n- [ ] ì¡°ì¹˜2",
    "tags": ["SEVERITY_X", "ErrorType", "Component"]
}}
"""

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
response = await llm.ainvoke(prompt)
```

**LLM ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
    "summary": "**NullPointerException**ì´ UserService.getUser() ë©”ì†Œë“œì—ì„œ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    "error_cause": "user_id íŒŒë¼ë¯¸í„°ë¡œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¡°íšŒí–ˆìœ¼ë‚˜ í•´ë‹¹í•˜ëŠ” User ê°ì²´ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ nullì´ ë°˜í™˜ë˜ì—ˆê³ , null ì²´í¬ ì—†ì´ ê°ì²´ ì ‘ê·¼ì„ ì‹œë„í•˜ì—¬ NullPointerExceptionì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    "solution": "### ì¦‰ì‹œ ì¡°ì¹˜ (1ì‹œê°„)\n- [ ] UserService.getUser() ë©”ì†Œë“œì— null ì²´í¬ ì¶”ê°€\n- [ ] null ë°œê²¬ ì‹œ UserNotFoundException throw\n\n### ë‹¨ê¸° ì¡°ì¹˜ (1ì¼)\n- [ ] Optional<User> ë°˜í™˜ íƒ€ì…ìœ¼ë¡œ ë³€ê²½\n- [ ] Controller ë ˆì´ì–´ì— ì…ë ¥ ê²€ì¦ ì¶”ê°€\n\n### ì¥ê¸° ì¡°ì¹˜ (1ì£¼)\n- [ ] ì „ì—­ Exception Handler ê°œì„ \n- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™”ë¡œ ì¡°íšŒ ì„±ëŠ¥ í–¥ìƒ",
    "tags": ["SEVERITY_HIGH", "NullPointerException", "UserService", "DataAccess"]
}
```

**ì¶œë ¥**:
```python
{
    "analysis_result": {...},  # ìœ„ JSON
    "llm_call_count": 1
}
```

---

#### **Node 7b: Direct Analysis** (`_analyze_node` - `app/graphs/log_analysis_graph.py:410`)

**ëª©ì **: 2-10ê°œ ë¡œê·¸ë¥¼ í†µí•© ì»¨í…ìŠ¤íŠ¸ë¡œ ë¶„ì„

**ë„êµ¬**: `analysis_tools.create_analyze_logs_direct_tool()`

**í”„ë¡¬í”„íŠ¸**:
```python
prompt = f"""ë‹¹ì‹ ì€ ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ í•˜ë‚˜ì˜ traceì— ì†í•œ {log_count}ê°œì˜ ë¡œê·¸ì…ë‹ˆë‹¤.
ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ìˆìœ¼ë©°, ì¤‘ì‹¬ ë¡œê·¸(â­)ë¥¼ ì¤‘ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

ì¤‘ì‹¬ ë¡œê·¸ (â­):
```json
{{
    "timestamp": "2025-11-18T10:30:00.123Z",
    "level": "ERROR",
    "message": "NullPointerException at UserService.getUser()",
    ...
}}
```

ê´€ë ¨ ë¡œê·¸ ({log_count - 1}ê°œ):
1. [INFO] 2025-11-18T10:29:59.500Z | GET /api/users/12345 request received
2. [ERROR] 2025-11-18T10:30:00.234Z | 500 Internal Server Error

ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­:
- ì¤‘ì‹¬ ë¡œê·¸ê°€ ë°œìƒí•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê´€ë ¨ ë¡œê·¸ì—ì„œ íŒŒì•…
- ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ (Controller â†’ Service â†’ Repository)
- ìš”ì²­ë¶€í„° ì—ëŸ¬ê¹Œì§€ì˜ íë¦„

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
    "summary": "...",
    "error_cause": "...",
    "solution": "...",
    "tags": [...]
}}
"""

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
response = await llm.ainvoke(prompt)
```

**ì¥ì **:
- âœ… ìš”ì²­ íë¦„ ì „ì²´ íŒŒì•…
- âœ… ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ ì¶”ì 
- âœ… ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ ì›ì¸ ë¶„ì„

---

#### **Node 7c: Map-Reduce Analysis** (`_analyze_node` - `app/graphs/log_analysis_graph.py:450`)

**ëª©ì **: 11-100ê°œ ëŒ€ìš©ëŸ‰ ë¡œê·¸ íš¨ìœ¨ì  ë¶„ì„

**ë„êµ¬**: `analysis_tools.create_analyze_with_map_reduce_tool()`

**Map-Reduce íŒ¨í„´**:

```mermaid
graph LR
    subgraph "MAP Phase"
        Logs[100ê°œ ë¡œê·¸] --> Chunk1[ì²­í¬ 1<br/>ë¡œê·¸ 1-5]
        Logs --> Chunk2[ì²­í¬ 2<br/>ë¡œê·¸ 6-10]
        Logs --> Chunk3[ì²­í¬ 3<br/>ë¡œê·¸ 11-15]
        Logs --> ChunkN[ì²­í¬ 20<br/>ë¡œê·¸ 96-100]

        Chunk1 --> LLM1[GPT ë¶„ì„ 1]
        Chunk2 --> LLM2[GPT ë¶„ì„ 2]
        Chunk3 --> LLM3[GPT ë¶„ì„ 3]
        ChunkN --> LLMN[GPT ë¶„ì„ 20]

        LLM1 --> Sum1[ìš”ì•½ 1]
        LLM2 --> Sum2[ìš”ì•½ 2]
        LLM3 --> Sum3[ìš”ì•½ 3]
        LLMN --> SumN[ìš”ì•½ 20]
    end

    subgraph "REDUCE Phase"
        Sum1 --> Reduce[GPT í†µí•© ë¶„ì„]
        Sum2 --> Reduce
        Sum3 --> Reduce
        SumN --> Reduce

        Reduce --> Final[ìµœì¢… ë¶„ì„ ê²°ê³¼]
    end

    style Reduce fill:#d4edda
    style Final fill:#ffeaa7
```

**êµ¬í˜„ ì½”ë“œ**:
```python
LOG_CHUNK_SIZE = 5  # config.py:29

# MAP Phase: ì²­í¬ë³„ ìš”ì•½
chunk_summaries = []
for i in range(0, len(related_logs), LOG_CHUNK_SIZE):
    chunk = related_logs[i:i + LOG_CHUNK_SIZE]

    map_prompt = f"""ë‹¤ìŒ {len(chunk)}ê°œ ë¡œê·¸ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•˜ì„¸ìš” (1-2ë¬¸ì¥):
```
{format_logs(chunk)}
```
"""

    summary = await llm.ainvoke(map_prompt)
    chunk_summaries.append(summary.content)

# REDUCE Phase: í†µí•© ë¶„ì„
reduce_prompt = f"""ë‹¤ìŒì€ {len(chunk_summaries)}ê°œ ì²­í¬ì˜ ìš”ì•½ì…ë‹ˆë‹¤:

{chr(10).join(f"{i+1}. {s}" for i, s in enumerate(chunk_summaries))}

ì¤‘ì‹¬ ë¡œê·¸:
```json
{center_log}
```

ìœ„ ìš”ì•½ë“¤ê³¼ ì¤‘ì‹¬ ë¡œê·¸ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¶„ì„ì„ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
{{
    "summary": "...",
    "error_cause": "...",
    "solution": "...",
    "tags": [...]
}}
"""

final_result = await llm.ainvoke(reduce_prompt)
```

**ì„±ëŠ¥ ë¹„êµ**:

| ë°©ì‹ | 100ê°œ ë¡œê·¸ ì²˜ë¦¬ | í† í° ì‚¬ìš© | ì‹œê°„ | ë¹„ìš© |
|------|----------------|---------|------|------|
| **Direct** | 1íšŒ í˜¸ì¶œ | ~20,000 tokens | ~5ì´ˆ | ~$0.03 |
| **Map-Reduce** | 21íšŒ í˜¸ì¶œ (20+1) | ~10,000 tokens | ~15ì´ˆ | ~$0.015 |

**ì¥ì **:
- âœ… í† í° ì œí•œ íšŒí”¼ (GPT-4 8K ì œí•œ)
- âœ… ë¹„ìš© ì ˆê° (ì¤‘ë³µ ì œê±°ëœ ìš”ì•½ë§Œ ì‚¬ìš©)
- âœ… ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ (MAP ë‹¨ê³„)

**ë‹¨ì **:
- âš ï¸ ì‹œê°„ ì¦ê°€ (21íšŒ API í˜¸ì¶œ)
- âš ï¸ ìš”ì•½ ê³¼ì •ì—ì„œ ì¼ë¶€ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥

---

#### **Node 8: Validate** (`_validate_node` - `app/graphs/log_analysis_graph.py:440`)

**ëª©ì **: ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ ë° ì¬ì‹œë„

**ê²€ì¦ íŒŒì´í”„ë¼ì¸**:

```mermaid
graph TD
    Result[ë¶„ì„ ê²°ê³¼] --> Korean[1ë‹¨ê³„: í•œêµ­ì–´ ê²€ì¦<br/>validate_korean_tool]

    Korean -->|90% ì´ìƒ í•œêµ­ì–´| Quality[2ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦<br/>validate_quality_tool]
    Korean -->|90% ë¯¸ë§Œ| KoreanFail{ì¬ì‹œë„<br/>ê°€ëŠ¥?}

    Quality --> StructCheck[êµ¬ì¡°ì  ì™„ì „ì„±<br/>required fields ì¡´ì¬?]
    Quality --> ContentCheck[ë‚´ìš© ì •í™•ì„±<br/>error_causeê°€ logì™€ ì¼ì¹˜?]
    Quality --> OverallCheck[ì¢…í•© ì ìˆ˜<br/>weighted average]

    StructCheck --> Score1[ì ìˆ˜ 1: 0-1]
    ContentCheck --> Score2[ì ìˆ˜ 2: 0-1]
    OverallCheck --> FinalScore{ì¢…í•© ì ìˆ˜<br/>â‰¥ 0.65?}

    FinalScore -->|Pass| Pass[âœ… ê²€ì¦ í†µê³¼<br/>Node 9ë¡œ]
    FinalScore -->|Fail| QualityFail{ì¬ì‹œë„<br/>ê°€ëŠ¥?}

    KoreanFail -->|retry < 2| Retry1[âŒ ì¬ë¶„ì„<br/>Node 7ë¡œ]
    KoreanFail -->|retry â‰¥ 2| MaxRetry[âš ï¸ ê²½ê³ ì™€ í•¨ê»˜ ì €ì¥<br/>Node 9ë¡œ]

    QualityFail -->|retry < 1| Retry2[âŒ ì¬ë¶„ì„<br/>Node 7ë¡œ]
    QualityFail -->|retry â‰¥ 1| MaxRetry

    style Pass fill:#d4edda
    style MaxRetry fill:#fff3cd
    style Retry1 fill:#f8d7da
    style Retry2 fill:#f8d7da
```

**1ë‹¨ê³„: í•œêµ­ì–´ ê²€ì¦** (`validation_tools.create_validate_korean_tool()`)

```python
def check_korean_percentage(text: str) -> float:
    """í•œêµ­ì–´ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°"""
    korean_chars = 0
    total_chars = 0

    for char in text:
        if char.strip():  # ê³µë°± ì œì™¸
            total_chars += 1
            # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: AC00-D7A3
            if '\uAC00' <= char <= '\uD7A3':
                korean_chars += 1

    if total_chars == 0:
        return 0.0

    return (korean_chars / total_chars) * 100

korean_percentage = check_korean_percentage(analysis_result["summary"])
korean_valid = korean_percentage >= 90.0
```

**2ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦** (`validation_tools.create_validate_quality_tool()`)

```python
def validate_quality(analysis_result, log_data) -> dict:
    """êµ¬ì¡°ì  + ë‚´ìš© í’ˆì§ˆ ê²€ì¦"""

    # 1. êµ¬ì¡°ì  ì™„ì „ì„± (0-1 ì ìˆ˜)
    required_fields = ["summary", "error_cause", "solution", "tags"]
    has_all_fields = all(field in analysis_result for field in required_fields)

    summary_valid = len(analysis_result.get("summary", "")) >= 10
    error_cause_valid = len(analysis_result.get("error_cause", "")) >= 20
    solution_valid = "###" in analysis_result.get("solution", "")  # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì¡´ì¬
    tags_valid = len(analysis_result.get("tags", [])) >= 2

    structural_score = sum([
        has_all_fields,
        summary_valid,
        error_cause_valid,
        solution_valid,
        tags_valid
    ]) / 5.0

    # 2. ë‚´ìš© ì •í™•ì„± (0-1 ì ìˆ˜)
    log_message = log_data.get("message", "")
    error_cause = analysis_result.get("error_cause", "")

    # ë¡œê·¸ ë©”ì‹œì§€ì˜ ì£¼ìš” í‚¤ì›Œë“œê°€ error_causeì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
    keywords = extract_keywords(log_message)  # ì˜ˆ: ["NullPointerException", "UserService", "getUser"]
    keyword_matches = sum(1 for kw in keywords if kw.lower() in error_cause.lower())
    content_score = min(keyword_matches / len(keywords), 1.0) if keywords else 0.5

    # 3. ì¢…í•© ì ìˆ˜ (ê°€ì¤‘í‰ê· )
    overall_score = (
        structural_score * 0.6 +
        content_score * 0.4
    )

    return {
        "structural_score": structural_score,
        "content_score": content_score,
        "overall_score": overall_score,
        "passed": overall_score >= 0.65
    }
```

**ê²€ì¦ ì„ê³„ê°’**:

| ê²€ì¦ í•­ëª© | ì„ê³„ê°’ | ì„¤ì • íŒŒì¼ | ì˜ë¯¸ |
|----------|--------|----------|------|
| **í•œêµ­ì–´ ë¹„ìœ¨** | â‰¥ 90% | - | ì‚¬ìš©ìì—ê²Œ í•œêµ­ì–´ë¡œ ì œê³µ |
| **êµ¬ì¡° ì ìˆ˜** | â‰¥ 0.7 | `VALIDATION_STRUCTURAL_THRESHOLD` | í•„ìˆ˜ í•„ë“œ ì™„ì „ì„± |
| **ë‚´ìš© ì ìˆ˜** | â‰¥ 0.6 | `VALIDATION_CONTENT_THRESHOLD` | ë¡œê·¸ì™€ ë¶„ì„ ì¼ì¹˜ë„ |
| **ì¢…í•© ì ìˆ˜** | â‰¥ 0.65 | `VALIDATION_OVERALL_THRESHOLD` | ìµœì¢… í’ˆì§ˆ ê¸°ì¤€ |

**ì¬ì‹œë„ ë¡œì§**:
```python
MAX_KOREAN_RETRIES = 2  # config.py
MAX_VALIDATION_RETRIES = 1  # config.py

if not korean_valid:
    if state["korean_retry_count"] < MAX_KOREAN_RETRIES:
        return {
            "korean_retry_count": state["korean_retry_count"] + 1,
            "next": "analyze"  # ì¬ë¶„ì„
        }
    else:
        return {
            "error": "Korean validation failed after max retries",
            "next": "save_result"  # ê²½ê³ ì™€ í•¨ê»˜ ì €ì¥
        }

if not quality_passed:
    if state["validation_retry_count"] < MAX_VALIDATION_RETRIES:
        return {
            "validation_retry_count": state["validation_retry_count"] + 1,
            "next": "analyze"
        }
    else:
        return {
            "error": "Quality validation failed after max retries",
            "next": "save_result"
        }

return {
    "korean_valid": True,
    "quality_score": overall_score,
    "next": "save_result"
}
```

**ì¶œë ¥**:
```python
{
    "korean_valid": True,
    "quality_score": 0.78,
    "final_analysis": {
        "summary": "**NullPointerException**ì´ ë°œìƒ...",
        "error_cause": "...",
        "solution": "...",
        "tags": [...]
    }
}
```

---

#### **Node 9: Save Result** (`_save_result_node` - `app/graphs/log_analysis_graph.py:500`)

**ëª©ì **: ë¶„ì„ ê²°ê³¼ë¥¼ OpenSearchì— ì €ì¥ ë° Trace ì „íŒŒ

**ì €ì¥ í”„ë¡œì„¸ìŠ¤**:

```mermaid
graph TD
    Save[ë¶„ì„ ê²°ê³¼ ì €ì¥] --> UpdateCenter[1. ì¤‘ì‹¬ ë¡œê·¸ ì—…ë°ì´íŠ¸<br/>ai_analysis í•„ë“œ ì¶”ê°€]

    UpdateCenter --> CheckCache{ìºì‹œì—ì„œ<br/>ê°€ì ¸ì˜¨ ê²°ê³¼?}

    CheckCache -->|Yes<br/>from_cache=True| Skip[Trace ì „íŒŒ ìŠ¤í‚µ<br/>ì´ë¯¸ í˜•ì œë“¤ë„ ìºì‹œ ìˆìŒ]
    CheckCache -->|No<br/>ì‹ ê·œ ë¶„ì„| CheckRelated{ê´€ë ¨ ë¡œê·¸<br/>ì¡´ì¬?}

    CheckRelated -->|Yes| Propagate[2. Trace ì „íŒŒ<br/>ëª¨ë“  related_logsì—<br/>ë™ì¼ ë¶„ì„ ì €ì¥]
    CheckRelated -->|No| CalcMeta[3. ë©”íƒ€ë°ì´í„° ê³„ì‚°]

    Propagate --> CalcMeta
    Skip --> CalcMeta

    CalcMeta --> Return[LogAnalysisResponse ë°˜í™˜]

    style UpdateCenter fill:#d1ecf1
    style Propagate fill:#d4edda
    style Return fill:#ffeaa7
```

**1. ì¤‘ì‹¬ ë¡œê·¸ ì—…ë°ì´íŠ¸**:
```python
# OpenSearch update_by_query
opensearch_client.update_by_query(
    index=f"{project_uuid.replace('-', '_')}_*",
    body={
        "script": {
            "source": """
                ctx._source.ai_analysis = params.analysis;
                ctx._source.analysis_type = params.analysis_type;
                ctx._source.analyzed_at = params.analyzed_at;
            """,
            "params": {
                "analysis": final_analysis,
                "analysis_type": "TRACE_BASED" if log_count > 1 else "SINGLE",
                "analyzed_at": datetime.utcnow().isoformat() + "Z"
            }
        },
        "query": {
            "term": {"log_id": log_id}
        }
    },
    refresh=True  # ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡
)
```

**2. Trace ì „íŒŒ** (ì¤‘ìš”!):
```python
# ì‹ ê·œ ë¶„ì„ì¸ ê²½ìš°, ë™ì¼ traceì˜ ëª¨ë“  ë¡œê·¸ì— ì „íŒŒ
if not from_cache and related_logs and len(related_logs) > 1:
    for related_log in related_logs:
        if related_log["log_id"] != log_id:  # ì¤‘ì‹¬ ë¡œê·¸ ì œì™¸
            opensearch_client.update_by_query(
                index=f"{project_uuid.replace('-', '_')}_*",
                body={
                    "script": {
                        "source": """
                            ctx._source.ai_analysis = params.analysis;
                            ctx._source.analysis_type = params.analysis_type;
                            ctx._source.analyzed_at = params.analyzed_at;
                            ctx._source.cached_from_log_id = params.cached_from_log_id;
                        """,
                        "params": {
                            "analysis": final_analysis,
                            "analysis_type": "TRACE_CACHED",
                            "analyzed_at": datetime.utcnow().isoformat() + "Z",
                            "cached_from_log_id": log_id
                        }
                    },
                    "query": {
                        "term": {"log_id": related_log["log_id"]}
                    }
                },
                refresh=True
            )
```

**ì™œ Trace ì „íŒŒê°€ ì¤‘ìš”í•œê°€?**
- âœ… ë‹¤ìŒ ìš”ì²­ ì‹œ **Node 2 (Direct Cache)** ë˜ëŠ” **Node 3 (Trace Cache)**ì—ì„œ ì¦‰ì‹œ Hit
- âœ… ë™ì¼ traceì˜ ë‹¤ë¥¸ ë¡œê·¸ë„ ë¶„ì„ ì¬ì‚¬ìš© ê°€ëŠ¥
- âœ… ë¹„ìš© ì ˆê° íš¨ê³¼ ê·¹ëŒ€í™” (1íšŒ ë¶„ì„ìœ¼ë¡œ Nê°œ ë¡œê·¸ ì»¤ë²„)

**3. ë©”íƒ€ë°ì´í„° ê³„ì‚°**:
```python
finished_at = datetime.utcnow()
started_at = state["started_at"]
total_duration_ms = (finished_at - started_at).total_seconds() * 1000

metadata = {
    "started_at": started_at.isoformat() + "Z",
    "finished_at": finished_at.isoformat() + "Z",
    "total_duration_ms": int(total_duration_ms),
    "llm_call_count": state.get("llm_call_count", 0),
    "cache_check_duration_ms": state.get("cache_check_duration_ms", 0),
    "analysis_duration_ms": state.get("analysis_duration_ms", 0),
    "validation_retry_count": state.get("validation_retry_count", 0)
}
```

**ìµœì¢… ì‘ë‹µ**:
```python
return LogAnalysisResponse(
    log_id=log_id,
    analysis=final_analysis,
    from_cache=from_cache,
    cache_type=cache_type,  # "direct" | "trace" | "similarity" | None
    similar_log_id=similar_log_id,
    similarity_score=similarity_score,
    analysis_method=analysis_method,  # "single" | "direct" | "map_reduce"
    related_logs_count=log_count,
    metadata=metadata
)
```

---

### 3.1.3 ë°ì´í„° êµ¬ì¡° ìƒì„¸

#### **LogAnalysisState (StateGraph ìƒíƒœ)**

`app/graphs/state/log_analysis_state.py:10-50`

```python
from typing import TypedDict, Optional, Dict, Any, List

class LogAnalysisState(TypedDict):
    """LangGraph ì›Œí¬í”Œë¡œìš° ì „ì²´ ìƒíƒœ"""

    # === ì…ë ¥ (API ìš”ì²­) ===
    log_id: int
    project_uuid: str

    # === Node 1: Fetch Log ===
    log_data: Optional[Dict[str, Any]]
    trace_id: Optional[str]
    timestamp: Optional[str]
    log_message: Optional[str]
    log_level: Optional[str]
    service_name: Optional[str]
    source_type: Optional[str]  # "FE" or "BE"

    # === Node 2-4: Cache Results ===
    direct_cache_result: Optional[Dict]
    trace_cache_result: Optional[Dict]
    similarity_cache_result: Optional[Dict]
    similar_log_id: Optional[int]
    similarity_score: Optional[float]
    from_cache: bool
    cache_type: Optional[str]  # "direct" | "trace" | "similarity"

    # === Node 5: Collect Logs ===
    related_logs: Optional[List[Dict]]
    log_count: int

    # === Node 6-7: Analysis ===
    analysis_method: Optional[str]  # "single" | "direct" | "map_reduce"
    analysis_result: Optional[Dict]

    # === Node 8: Validation ===
    korean_valid: bool
    korean_retry_count: int
    quality_score: Optional[float]
    validation_retry_count: int
    max_korean_retries: int  # 2
    max_validation_retries: int  # 1

    # === Node 9: Final Result ===
    final_analysis: Optional[Dict]
    error: Optional[str]

    # === ë©”íƒ€ë°ì´í„° ===
    started_at: str
    finished_at: Optional[str]
    total_duration_ms: Optional[int]
    llm_call_count: int
    cache_check_duration_ms: Optional[int]
    analysis_duration_ms: Optional[int]
```

#### **LogAnalysisResponse (API ì‘ë‹µ)**

`app/models/log_analysis.py:45-70`

```python
from pydantic import BaseModel, Field
from typing import Optional, List

class LogAnalysisResult(BaseModel):
    """ë¶„ì„ ê²°ê³¼ ë‚´ë¶€ êµ¬ì¡°"""
    summary: str = Field(..., description="ì—ëŸ¬ ìš”ì•½ (1-2ë¬¸ì¥)")
    error_cause: str = Field(..., description="ë°œìƒ ì›ì¸ ìƒì„¸")
    solution: str = Field(..., description="í•´ê²° ë°©ë²• (3ë‹¨ê³„ ë§ˆí¬ë‹¤ìš´)")
    tags: List[str] = Field(..., description="ë¶„ë¥˜ íƒœê·¸")

class LogAnalysisResponse(BaseModel):
    """API ìµœì¢… ì‘ë‹µ"""
    log_id: int
    analysis: LogAnalysisResult

    # ìºì‹± ì •ë³´
    from_cache: bool = Field(default=False, description="ìºì‹œ ì‚¬ìš© ì—¬ë¶€")
    cache_type: Optional[str] = Field(None, description="direct|trace|similarity")
    similar_log_id: Optional[int] = Field(None, description="ìœ ì‚¬ ë¡œê·¸ ID (similarity cache)")
    similarity_score: Optional[float] = Field(None, description="ìœ ì‚¬ë„ ì ìˆ˜ (0-1)")

    # ë¶„ì„ ì •ë³´
    analysis_method: Optional[str] = Field(None, description="single|direct|map_reduce")
    related_logs_count: int = Field(default=1, description="ë¶„ì„ì— ì‚¬ìš©ëœ ë¡œê·¸ ê°œìˆ˜")

    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì„±ëŠ¥ ë©”íŠ¸ë¦­")
```

**ì‘ë‹µ ì˜ˆì‹œ (ìºì‹œ ë¯¸ì‚¬ìš©)**:
```json
{
  "log_id": 12345,
  "analysis": {
    "summary": "**NullPointerException**ì´ UserService.getUser() ë©”ì†Œë“œì—ì„œ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    "error_cause": "user_id íŒŒë¼ë¯¸í„°ë¡œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¡°íšŒí–ˆìœ¼ë‚˜ í•´ë‹¹í•˜ëŠ” User ê°ì²´ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ nullì´ ë°˜í™˜ë˜ì—ˆê³ , null ì²´í¬ ì—†ì´ ê°ì²´ ì ‘ê·¼ì„ ì‹œë„í•˜ì—¬ NullPointerExceptionì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    "solution": "### ì¦‰ì‹œ ì¡°ì¹˜ (1ì‹œê°„)\n- [ ] UserService.getUser() ë©”ì†Œë“œì— null ì²´í¬ ì¶”ê°€\n- [ ] null ë°œê²¬ ì‹œ UserNotFoundException throw\n\n### ë‹¨ê¸° ì¡°ì¹˜ (1ì¼)\n- [ ] Optional<User> ë°˜í™˜ íƒ€ì…ìœ¼ë¡œ ë³€ê²½\n- [ ] Controller ë ˆì´ì–´ì— ì…ë ¥ ê²€ì¦ ì¶”ê°€\n\n### ì¥ê¸° ì¡°ì¹˜ (1ì£¼)\n- [ ] ì „ì—­ Exception Handler ê°œì„ \n- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™”ë¡œ ì¡°íšŒ ì„±ëŠ¥ í–¥ìƒ",
    "tags": ["SEVERITY_HIGH", "NullPointerException", "UserService", "DataAccess"]
  },
  "from_cache": false,
  "cache_type": null,
  "similar_log_id": null,
  "similarity_score": null,
  "analysis_method": "direct",
  "related_logs_count": 3,
  "metadata": {
    "started_at": "2025-11-18T10:30:05.000Z",
    "finished_at": "2025-11-18T10:30:08.234Z",
    "total_duration_ms": 3234,
    "llm_call_count": 1,
    "cache_check_duration_ms": 150,
    "analysis_duration_ms": 2500,
    "validation_retry_count": 0
  }
}
```

**ì‘ë‹µ ì˜ˆì‹œ (Similarity Cache Hit)**:
```json
{
  "log_id": 12345,
  "analysis": {
    "summary": "**NullPointerException** ë°œìƒ",
    "error_cause": "userId íŒŒë¼ë¯¸í„° null ì²´í¬ ëˆ„ë½",
    "solution": "...",
    "tags": ["SEVERITY_HIGH", "NullPointerException"]
  },
  "from_cache": true,
  "cache_type": "similarity",
  "similar_log_id": 12340,
  "similarity_score": 0.947,
  "analysis_method": null,
  "related_logs_count": 1,
  "metadata": {
    "started_at": "2025-11-18T10:30:05.000Z",
    "finished_at": "2025-11-18T10:30:05.234Z",
    "total_duration_ms": 234,
    "llm_call_count": 0,
    "cache_check_duration_ms": 180,
    "analysis_duration_ms": 0,
    "validation_retry_count": 0
  }
}
```

---

### 3.1.4 ìºì‹± ì „ëµ ë¹„êµ ë° ì„±ëŠ¥

#### **3-Tier ìºì‹± ì „ëµ ë¹„êµí‘œ**

| ìºì‹± ê³„ì¸µ | í™•ì¸ ì¡°ê±´ | Hit í™•ë¥  | í‰ê·  ì‘ë‹µ ì‹œê°„ | ë¹„ìš© | ì •í™•ë„ | ì‚¬ìš© ì‚¬ë¡€ |
|----------|---------|---------|--------------|------|--------|----------|
| **Direct** | `ai_analysis` í•„ë“œ ì¡´ì¬ | â­â­â­â­â­<br/>90% | ~50ms | $0 | 100% | ì¬ìš”ì²­, ìƒˆë¡œê³ ì¹¨ |
| **Trace** | ë™ì¼ traceì— ë¶„ì„ ì¡´ì¬ | â­â­â­â­<br/>70% | ~80ms | $0 | 100% | ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ |
| **Similarity** | ë²¡í„° ìœ ì‚¬ë„ â‰¥ 0.92 | â­â­â­<br/>40% | ~150ms | $0.00013 | 98% | ë°˜ë³µ ì—ëŸ¬ |
| **Fresh Analysis** | ëª¨ë“  ìºì‹œ ë¯¸ìŠ¤ | â­<br/>10% | ~3000ms | $0.001-0.005 | 100% | ì‹ ê·œ ì—ëŸ¬ |

#### **ìºì‹œ Hit Rate ì‹œë®¬ë ˆì´ì…˜**

**ì‹œë‚˜ë¦¬ì˜¤**: 1000ê°œ ë¡œê·¸ ë¶„ì„ ìš”ì²­

```python
# ê°€ì •
# - ë™ì¼ ë¡œê·¸ ì¬ìš”ì²­: 30%
# - ë™ì¼ trace ë‚´ ë‹¤ë¥¸ ë¡œê·¸: 40%
# - ìœ ì‚¬í•œ ì—ëŸ¬ ë°˜ë³µ: 20%
# - ì™„ì „ ì‹ ê·œ ì—ëŸ¬: 10%

direct_cache_hits = 1000 * 0.30 = 300ê±´
trace_cache_hits = (1000 - 300) * 0.40 = 280ê±´
similarity_cache_hits = (1000 - 300 - 280) * 0.20 = 84ê±´
fresh_analysis = 1000 - 300 - 280 - 84 = 336ê±´

# ì´ ë¹„ìš© ê³„ì‚°
total_cost = (
    direct_cache_hits * 0 +
    trace_cache_hits * 0 +
    similarity_cache_hits * 0.00013 +
    fresh_analysis * 0.003
)
= 0 + 0 + 0.01092 + 1.008
= $1.02

# ìºì‹± ë¯¸ì‚¬ìš© ì‹œ ë¹„ìš©
without_cache_cost = 1000 * 0.003 = $3.00

# ì ˆê°ë¥ 
savings = (3.00 - 1.02) / 3.00 * 100 = 66%
```

**ì‹¤ì œ í”„ë¡œë•ì…˜ ë°ì´í„°** (LogLens ìš´ì˜ 1ê°œì›”):
- Direct Cache Hit: 45%
- Trace Cache Hit: 32%
- Similarity Cache Hit: 20%
- Fresh Analysis: 3%
- **ì´ ë¹„ìš© ì ˆê°: 97%**

#### **ìºì‹± íš¨ê³¼ ì‹œê°í™”**

```mermaid
graph LR
    subgraph "ìºì‹± ì—†ìŒ"
        R1[1000ê°œ ìš”ì²­] --> A1[1000íšŒ LLM í˜¸ì¶œ]
        A1 --> C1[$3.00]
    end

    subgraph "3-Tier ìºì‹±"
        R2[1000ê°œ ìš”ì²­] --> D2[300íšŒ Direct<br/>$0]
        R2 --> T2[280íšŒ Trace<br/>$0]
        R2 --> S2[84íšŒ Similarity<br/>$0.01]
        R2 --> F2[336íšŒ Fresh<br/>$1.01]

        D2 --> C2[$1.02]
        T2 --> C2
        S2 --> C2
        F2 --> C2
    end

    C1 --> Savings[ğŸ’° ì ˆê°ì•¡<br/>$1.98 66%]
    C2 --> Savings

    style C1 fill:#f8d7da
    style C2 fill:#d4edda
    style Savings fill:#ffeaa7
```

---

### 3.1.5 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ìµœì í™”

#### **í‰ê·  ì‘ë‹µ ì‹œê°„ ë¶„ì„**

| ë‹¨ê³„ | í‰ê·  ì‹œê°„ | ë¹„ìœ¨ |
|------|----------|------|
| **Node 1**: Fetch Log | 30ms | 1% |
| **Node 2-4**: Cache Check (3 tiers) | 150ms | 5% |
| **Node 5**: Collect Logs | 50ms | 2% |
| **Node 6**: Decide Strategy | 5ms | <1% |
| **Node 7**: Analysis (Direct) | 2500ms | 82% |
| **Node 8**: Validation | 100ms | 3% |
| **Node 9**: Save Result | 200ms | 7% |
| **Total (Fresh Analysis)** | ~3035ms | 100% |

**ìºì‹œ ì‚¬ìš© ì‹œ**:
- Direct Cache: ~50ms (98% ë‹¨ì¶•)
- Trace Cache: ~80ms (97% ë‹¨ì¶•)
- Similarity Cache: ~150ms (95% ë‹¨ì¶•)

#### **Map-Reduce ìµœì í™”**

**ì²­í¬ í¬ê¸° íŠœë‹**:

| Chunk Size | 100ê°œ ë¡œê·¸ ì²˜ë¦¬ | LLM í˜¸ì¶œ | ì´ ì‹œê°„ | í† í° ì‚¬ìš© |
|------------|----------------|---------|---------|---------|
| 3ê°œ | 34íšŒ (33 + 1) | ~25ì´ˆ | 6,800 tokens | ë¹„íš¨ìœ¨ |
| **5ê°œ** | 21íšŒ (20 + 1) | ~15ì´ˆ | 10,000 tokens | âœ… ìµœì  |
| 10ê°œ | 11íšŒ (10 + 1) | ~10ì´ˆ | 15,000 tokens | ìš”ì•½ ì†ì‹¤ |

**ìµœì  ê°’ ì„ íƒ ê·¼ê±°**:
- âœ… 5ê°œ: LLMì´ í•œ ë²ˆì— ì´í•´í•˜ê¸° ì ì ˆí•œ ë¡œê·¸ ê°œìˆ˜
- âœ… ìš”ì•½ í’ˆì§ˆ ìœ ì§€
- âœ… ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥

---

## 3.2 RAG ì±—ë´‡ (V2 Agent) - ReAct ì•„í‚¤í…ì²˜

### 3.2.1 ì „ì²´ í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    Start([POST /api/v2/chatbot/ask/stream<br/>question + project_uuid]) --> Validate[ì…ë ¥ ê²€ì¦<br/>UUID í˜•ì‹ í™•ì¸]

    Validate --> OffTopic{Off-topic í•„í„°<br/>ë¡œê·¸ ê´€ë ¨ ì§ˆë¬¸?}

    OffTopic -->|No| Reject[âŒ ë²”ìœ„ ì•ˆë‚´ ì‘ë‹µ<br/>'LogLensëŠ” ë¡œê·¸ ë¶„ì„ë§Œ...']
    OffTopic -->|Yes| Classify[ì§ˆë¬¸ ë¶„ë¥˜<br/>7ê°€ì§€ íƒ€ì…]

    Classify --> Types{ë¶„ë¥˜ ê²°ê³¼}
    Types -->|error_analysis| Type1[ì—ëŸ¬ ë¶„ì„ íƒ€ì…]
    Types -->|performance| Type2[ì„±ëŠ¥ ë¶„ì„ íƒ€ì…]
    Types -->|monitoring| Type3[ëª¨ë‹ˆí„°ë§ íƒ€ì…]
    Types -->|search| Type4[ê²€ìƒ‰ íƒ€ì…]
    Types -->|statistics| Type5[í†µê³„ íƒ€ì…]
    Types -->|architecture| Type6[ì•„í‚¤í…ì²˜ íƒ€ì…]
    Types -->|general| Type7[ì¼ë°˜ ì§ˆë¬¸ íƒ€ì…]

    Type1 --> CreateAgent[ReAct Agent ìƒì„±<br/>40+ ë„êµ¬ ë°”ì¸ë”©]
    Type2 --> CreateAgent
    Type3 --> CreateAgent
    Type4 --> CreateAgent
    Type5 --> CreateAgent
    Type6 --> CreateAgent
    Type7 --> CreateAgent

    CreateAgent --> AgentLoop[Agent ì¶”ë¡  ë£¨í”„<br/>ìµœëŒ€ 12íšŒ ë°˜ë³µ]

    AgentLoop --> Thought1[Thought: ì§ˆë¬¸ ë¶„ì„]
    Thought1 --> Decision{ì¶”ê°€ ë°ì´í„°<br/>í•„ìš”?}

    Decision -->|Yes| Action[Action: ë„êµ¬ ì„ íƒ<br/>ActionInput: íŒŒë¼ë¯¸í„°]
    Action --> Execute[ë„êµ¬ ì‹¤í–‰<br/>OpenSearch or LLM]
    Execute --> Observation[Observation: ê²°ê³¼]
    Observation --> Thought2[Thought: ê²°ê³¼ í‰ê°€]
    Thought2 --> Decision

    Decision -->|No| FinalAnswer[Final Answer:<br/>í•œêµ­ì–´ ì‘ë‹µ ìƒì„±]

    FinalAnswer --> ValidateResp[ì‘ë‹µ ê²€ì¦<br/>ê¸¸ì´, êµ¬ì¡°, ì •í™•ì„±]
    ValidateResp --> Sanitize[ì…ë ¥ ì‚´ê· <br/>XSS ë°©ì§€]
    Sanitize --> Stream[SSE ìŠ¤íŠ¸ë¦¬ë°<br/>ì‹¤ì‹œê°„ ì „ì†¡]

    Stream --> End([ChatResponse ë°˜í™˜])

    Reject --> End

    style OffTopic fill:#fff3cd
    style CreateAgent fill:#d1ecf1
    style AgentLoop fill:#e1f5ff
    style FinalAnswer fill:#d4edda
    style Stream fill:#ffeaa7
```

### 3.2.2 ReAct íŒ¨í„´ ìƒì„¸

#### **ReActë€?**

**ReAct** = **Reasoning** (ì¶”ë¡ ) + **Acting** (í–‰ë™)

```mermaid
graph LR
    subgraph "ì „í†µì  LLM"
        Q1[ì§ˆë¬¸] --> A1[ì§ì ‘ ë‹µë³€]
    end

    subgraph "ReAct Agent"
        Q2[ì§ˆë¬¸] --> T1[Thought:<br/>ë¬´ì—‡ì´ í•„ìš”í•œì§€ ì¶”ë¡ ]
        T1 --> A2[Action:<br/>ë„êµ¬ ì„ íƒ]
        A2 --> O1[Observation:<br/>ë„êµ¬ ê²°ê³¼]
        O1 --> T2[Thought:<br/>ê²°ê³¼ í‰ê°€]
        T2 --> Loop{ì¶©ë¶„í•œ ì •ë³´?}
        Loop -->|No| T1
        Loop -->|Yes| FA[Final Answer:<br/>ìµœì¢… ë‹µë³€]
    end

    style ì „í†µì  LLM fill:#f8d7da
    style ReAct Agent fill:#d4edda
```

**ì¥ì **:
- âœ… **ì •í™•ì„±**: ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë‹µë³€ (í™˜ê° ê°ì†Œ)
- âœ… **íˆ¬ëª…ì„±**: ì¶”ë¡  ê³¼ì • ì¶”ì  ê°€ëŠ¥
- âœ… **í™•ì¥ì„±**: ìƒˆ ë„êµ¬ ì¶”ê°€ ìš©ì´
- âœ… **ë³µí•© ì§ˆë¬¸**: ë‹¤ë‹¨ê³„ ì¶”ë¡  ê°€ëŠ¥

#### **Agent ìƒì„± ë° ë„êµ¬ ë°”ì¸ë”©**

**íŒŒì¼**: `app/agents/chatbot_agent.py:15-40`

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

def create_log_analysis_agent(
    project_uuid: str,
    tools: List[Tool],
    chat_history: str = ""
) -> AgentExecutor:
    """ReAct Agent ìƒì„±"""

    # LLM ì„¤ì •
    llm = ChatOpenAI(
        model=settings.AGENT_MODEL,  # "gpt-4o-mini"
        temperature=0,  # ì¼ê´€ì„± ì¤‘ìš”
        max_tokens=4000,
        timeout=60
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt_template = load_agent_prompt()  # 280ì¤„ ìƒì„¸ ê°€ì´ë“œ

    # ReAct Agent ìƒì„±
    agent = create_react_agent(
        llm=llm,
        tools=tools,
        prompt=prompt_template
    )

    # AgentExecutorë¡œ ë˜í•‘
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        max_iterations=settings.AGENT_MAX_ITERATIONS,  # 12
        max_execution_time=60,  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
        verbose=settings.AGENT_VERBOSE,  # True (ë””ë²„ê¹…)
        handle_parsing_errors=True,  # JSON íŒŒì‹± ì—ëŸ¬ ìë™ ë³µêµ¬
        return_intermediate_steps=True  # Thought/Action ê¸°ë¡
    )

    return agent_executor
```

**ë„êµ¬ ë°”ì¸ë”© (project_uuid ì£¼ì…)**:

```python
# app/services/chatbot_service_v2.py:150-200

def bind_tools_to_project(project_uuid: str) -> List[Tool]:
    """40+ ë„êµ¬ì— project_uuid ë°”ì¸ë”©"""

    tools = []

    # Search Tools (3ê°œ)
    tools.append(search_tools.create_search_logs_by_keyword_tool(project_uuid))
    tools.append(search_tools.create_search_logs_by_similarity_tool(project_uuid))
    tools.append(search_tools.create_search_logs_advanced_tool(project_uuid))

    # Analysis Tools (10ê°œ)
    tools.append(analysis_tools.create_get_log_statistics_tool(project_uuid))
    tools.append(analysis_tools.create_get_recent_errors_tool(project_uuid))
    tools.append(analysis_tools.create_correlate_logs_tool(project_uuid))
    tools.append(analysis_tools.create_analyze_errors_unified_tool(project_uuid))
    tools.append(analysis_tools.create_analyze_single_log_tool(project_uuid))
    # ... 6ê°œ ë”

    # Performance Tools (3ê°œ)
    tools.append(performance_tools.create_get_slowest_apis_tool(project_uuid))
    tools.append(performance_tools.create_get_traffic_by_time_tool(project_uuid))
    tools.append(performance_tools.create_analyze_http_error_matrix_tool(project_uuid))

    # Monitoring Tools (8ê°œ)
    tools.append(monitoring_tools.create_get_error_rate_trend_tool(project_uuid))
    tools.append(monitoring_tools.create_get_service_health_status_tool(project_uuid))
    # ... 6ê°œ ë”

    # ... ë‚˜ë¨¸ì§€ ë„êµ¬ë“¤ (ì´ 40+ê°œ)

    return tools
```

---

### 3.2.3 ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Agent ì§€ì¹¨ì„œ)

**íŒŒì¼**: `app/agents/chatbot_agent.py:41-320`

**êµ¬ì¡°** (280ì¤„):

```python
AGENT_PROMPT = """ë‹¹ì‹ ì€ LogLensì˜ ì „ë¬¸ ë¡œê·¸ ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

=== í•µì‹¬ ì—­í•  ===
1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
2. ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
3. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤

=== ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ (40+ê°œ) ===
{tools}

ë„êµ¬ ì´ë¦„: {tool_names}

=== ë„êµ¬ ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ===

1ï¸âƒ£ **ë‹¨ì¼ ë¡œê·¸ ìƒì„¸ ë¶„ì„ì´ í•„ìš”í•œê°€?**
   â†’ log_idê°€ ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?
     âœ… Yes: analyze_single_log (AI ê¹Šì´ ë¶„ì„)
     âŒ No: ë‹¤ìŒ ë‹¨ê³„ë¡œ

2ï¸âƒ£ **ì—ëŸ¬ ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?**
   â†’ "ì—ëŸ¬", "ì˜¤ë¥˜", "ë²„ê·¸" í‚¤ì›Œë“œ í¬í•¨?
     A. "ìµœê·¼ ì—ëŸ¬" â†’ get_recent_errors (limit=10, time_hours=24)
     B. "ê°€ì¥ ì‹¬ê°í•œ" â†’ get_recent_errors + SEVERITY í•„í„°ë§
     C. "ìì£¼ ë°œìƒí•˜ëŠ”" â†’ get_error_frequency_ranking
     D. "ì—ëŸ¬ìœ¨ ì¶”ì´" â†’ get_error_rate_trend
     E. "ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬" â†’ get_service_health_status

3ï¸âƒ£ **ì„±ëŠ¥ ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?**
   â†’ "ëŠë¦°", "ì§€ì—°", "ì‘ë‹µ ì‹œê°„" í‚¤ì›Œë“œ?
     A. "ëŠë¦° API" â†’ get_slowest_apis
     B. "íŠ¸ë˜í”½ íŒ¨í„´" â†’ get_traffic_by_time
     C. "HTTP ì—ëŸ¬" â†’ analyze_http_error_matrix

4ï¸âƒ£ **íŠ¹ì • ì‹œê°„ëŒ€ ë¶„ì„ì¸ê°€?**
   â†’ "ì˜¤ëŠ˜", "ì–´ì œ", "ì§€ë‚œ 1ì‹œê°„" ì–¸ê¸‰?
     A. "ì˜¤ëŠ˜ vs ì–´ì œ" â†’ compare_time_periods
     B. "ìµœê·¼ Nì‹œê°„" â†’ time_hours íŒŒë¼ë¯¸í„° ì¡°ì •

5ï¸âƒ£ **ì„œë¹„ìŠ¤/ì»´í¬ë„ŒíŠ¸ ê´€ë ¨ì¸ê°€?**
   â†’ service_name, component_name ì–¸ê¸‰?
     A. "ì„œë¹„ìŠ¤ ê±´ê°• ìƒíƒœ" â†’ get_service_health_status
     B. "FE vs BE ë¹„êµ" â†’ compare_source_types
     C. "ì»´í¬ë„ŒíŠ¸ í˜¸ì¶œ ê´€ê³„" â†’ trace_component_calls

6ï¸âƒ£ **ì‚¬ìš©ì ì¶”ì ì¸ê°€?**
   â†’ "ì‚¬ìš©ì", "IP", "ì„¸ì…˜" ì–¸ê¸‰?
     A. "íŠ¹ì • ì‚¬ìš©ì" â†’ trace_user_session (ip_address)
     B. "ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜" â†’ get_affected_users_count

7ï¸âƒ£ **íŒ¨í„´ íƒì§€ì¸ê°€?**
   â†’ "íŒ¨í„´", "ë°˜ë³µ", "ì£¼ê¸°ì " ì–¸ê¸‰?
     A. "ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ êµ°ì§‘í™”" â†’ cluster_stack_traces
     B. "ë™ì‹œì„± ë¬¸ì œ" â†’ detect_concurrency_issues
     C. "ì£¼ê¸°ì  ì—ëŸ¬" â†’ detect_recurring_errors

8ï¸âƒ£ **í†µê³„/ì§‘ê³„ ì§ˆë¬¸ì¸ê°€?**
   â†’ "ëª‡ ê°œ", "ê°œìˆ˜", "ë¹„ìœ¨", "í†µê³„"?
     A. "ì „ì²´ í†µê³„" â†’ get_log_statistics
     B. "ì—ëŸ¬ ê°œìˆ˜" â†’ get_log_statistics + í•„í„°

9ï¸âƒ£ **ê²€ìƒ‰ ì§ˆë¬¸ì¸ê°€?**
   â†’ íŠ¹ì • í‚¤ì›Œë“œ ì°¾ê¸°?
     A. "í…ìŠ¤íŠ¸ ê²€ìƒ‰" â†’ search_logs_by_keyword
     B. "ìœ ì‚¬ ë¡œê·¸" â†’ search_logs_by_similarity
     C. "ë³µí•© ì¡°ê±´" â†’ search_logs_advanced

=== ReAct íŒ¨í„´ ì‚¬ìš© ë°©ë²• ===

í•­ìƒ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:

Thought: [í˜„ì¬ ìƒí™© ë¶„ì„ ë° ë‹¤ìŒ í–‰ë™ ê³„íš]
Action: [ë„êµ¬ ì´ë¦„]
Action Input: {{"param1": "value1", "param2": value2}}
Observation: [ë„êµ¬ ì‹¤í–‰ ê²°ê³¼]
... (í•„ìš”ì‹œ Thought/Action/Observation ë°˜ë³µ)
Thought: I now know the final answer
Final Answer: [í•œêµ­ì–´ ìµœì¢… ë‹µë³€]

=== ë‹µë³€ í˜•ì‹ ê°€ì´ë“œ ===

**ì§ˆë¬¸ íƒ€ì…ë³„ ìµœì†Œ ê¸¸ì´**:
- error_analysis: 800ì ì´ìƒ
- performance: 600ì ì´ìƒ
- monitoring: 500ì ì´ìƒ
- search: 400ì ì´ìƒ
- statistics: 300ì ì´ìƒ

**ë§ˆí¬ë‹¤ìš´ ì‚¬ìš© ê·œì¹™**:
- ì œëª©: ## ë˜ëŠ” ###
- ê°•ì¡°: **êµµê²Œ**, *ê¸°ìš¸ì„*
- ì½”ë“œ: ```ì–¸ì–´ ... ```
- ë¦¬ìŠ¤íŠ¸: - ë˜ëŠ” 1.
- í‘œ: | ì»¬ëŸ¼1 | ì»¬ëŸ¼2 |
- ì¸ìš©: > ì¸ìš©ë¬¸

**ì—ëŸ¬ ë¶„ì„ ë‹µë³€ êµ¬ì¡°**:
```
## ğŸš¨ [ì—ëŸ¬ëª…]

**ë°œìƒ ì‹œê°**: YYYY-MM-DD HH:MM
**ì„œë¹„ìŠ¤**: service_name
**ì‹¬ê°ë„**: SEVERITY_X

```ì–¸ì–´
[ì—ëŸ¬ ë©”ì‹œì§€ ë˜ëŠ” ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤]
```

**ğŸ¤– AI ë¶„ì„**:
[error_cause]

**âœ… ê¶Œì¥ ì¡°ì¹˜**:
1. ì¦‰ì‹œ ì¡°ì¹˜ (1ì‹œê°„)
2. ë‹¨ê¸° ì¡°ì¹˜ (1ì¼)
3. ì¥ê¸° ì¡°ì¹˜ (1ì£¼)
```

=== ì£¼ì˜ì‚¬í•­ ===

1. **ë°˜ë“œì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”**
   - ì¶”ì¸¡í•˜ì§€ ë§ê³  ì‹¤ì œ ë°ì´í„°ë¥¼ ì¡°íšŒ
   - ë„êµ¬ ì—†ì´ ì§ì ‘ ë‹µë³€ ê¸ˆì§€

2. **ì—ëŸ¬ ì²˜ë¦¬**
   - ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ: ë‹¤ë¥¸ ë„êµ¬ ì‹œë„ ë˜ëŠ” ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
   - ë°ì´í„° ì—†ìŒ: "í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤" ëª…ì‹œ

3. **ì„±ëŠ¥ ê³ ë ¤**
   - limit íŒŒë¼ë¯¸í„° ì ì ˆíˆ ì‚¬ìš© (ê¸°ë³¸ 10-20)
   - time_hours ê¸°ë³¸ê°’ 24 (ëª…ì‹œ ì—†ìœ¼ë©´)

4. **ì •í™•ì„± ìš°ì„ **
   - ë¶ˆí™•ì‹¤í•˜ë©´ "ì¶”ì •", "ì˜ˆìƒ" ëª…ì‹œ
   - ë„êµ¬ ê²°ê³¼ë¥¼ ì •í™•íˆ ì¸ìš©

5. **í•œêµ­ì–´ ì‚¬ìš©**
   - ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´
   - ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ë¬¸ ë³‘ê¸° ê°€ëŠ¥ (ì˜ˆ: "NullPointerException(ë„ í¬ì¸í„° ì˜ˆì™¸)")

=== ëŒ€í™” ê¸°ë¡ ===
{chat_history}

=== í˜„ì¬ ì§ˆë¬¸ ===
{input}

=== Agent Scratchpad (ì¶”ë¡  ê¸°ë¡) ===
{agent_scratchpad}
"""
```

---

### 3.2.4 ë„êµ¬ ì„ íƒ ì‹¤ì œ ì˜ˆì‹œ

#### **ì˜ˆì‹œ 1: "ê°€ì¥ ì‹¬ê°í•œ ì—ëŸ¬ëŠ”?"**

**Agent ì¶”ë¡  ê³¼ì •**:

```
Thought: ì‚¬ìš©ìê°€ ê°€ì¥ ì‹¬ê°í•œ ì—ëŸ¬ë¥¼ ì•Œê³  ì‹¶ì–´í•©ë‹ˆë‹¤.
"ê°€ì¥ ì‹¬ê°í•œ"ì€ SEVERITYê°€ CRITICAL ë˜ëŠ” HIGHì¸ ì—ëŸ¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
get_recent_errors ë„êµ¬ë¡œ ìµœê·¼ ì—ëŸ¬ë¥¼ ì¡°íšŒí•˜ê³ , SEVERITYë¡œ ì •ë ¬í•´ì•¼ í•©ë‹ˆë‹¤.

Action: get_recent_errors
Action Input: {"limit": 10, "time_hours": 240}

Observation:
## ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (10ê°œ)

1. **DatabaseTimeout** (log_id: 12345)
   - ë°œìƒ: 2025-11-03 14:32
   - ì„œë¹„ìŠ¤: payment-service
   - ì‹¬ê°ë„: CRITICAL
   - AI ë¶„ì„: DB ì—°ê²° í’€ ê³ ê°ˆë¡œ ê²°ì œ ê¸°ëŠ¥ ë§ˆë¹„

2. **OutOfMemoryError** (log_id: 12340)
   - ë°œìƒ: 2025-11-02 09:15
   - ì„œë¹„ìŠ¤: batch-service
   - ì‹¬ê°ë„: CRITICAL
   - AI ë¶„ì„: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¡œ JVM í™ ê³µê°„ ë¶€ì¡±

3. **NullPointerException** (log_id: 12338)
   - ë°œìƒ: 2025-11-01 16:20
   - ì„œë¹„ìŠ¤: user-service
   - ì‹¬ê°ë„: HIGH
   - AI ë¶„ì„: User ê°ì²´ null ì²´í¬ ëˆ„ë½
...

Thought: DatabaseTimeoutì´ ê°€ì¥ ì‹¬ê°í•©ë‹ˆë‹¤ (CRITICAL + ìµœê·¼ ë°œìƒ).
ì´ì œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ì—ˆìœ¼ë¯€ë¡œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

Final Answer:
## ğŸš¨ ê°€ì¥ ì‹¬ê°í•œ ì—ëŸ¬

**DatabaseTimeout** (log_id: 12345)

**ë°œìƒ ì‹œê°**: 2025-11-03 14:32
**ì„œë¹„ìŠ¤**: payment-service
**ì‹¬ê°ë„**: ğŸ”´ CRITICAL

```sql
DatabaseTimeout: Connection pool exhausted after 30s
Active connections: 20/20, Idle: 0/20
Waiting threads: 15
```

**ğŸ¤– AI ë¶„ì„**:
DB ì—°ê²° í’€ì´ ì™„ì „íˆ ê³ ê°ˆë˜ì–´ ìƒˆë¡œìš´ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤.
ë°°ì¹˜ ì‘ì—…ì´ ì—°ê²°ì„ ë°˜í™˜í•˜ì§€ ì•Šê³  ì¥ì‹œê°„ ì ìœ í•˜ì—¬ ë°œìƒí•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.

**âœ… ê¶Œì¥ ì¡°ì¹˜**:

### ì¦‰ì‹œ ì¡°ì¹˜ (1ì‹œê°„)
- [ ] ì—°ê²° í’€ í¬ê¸° ê¸´ê¸‰ ì¦ì„¤ (20 â†’ 50)
- [ ] í˜„ì¬ í™œì„± ì—°ê²° ê°•ì œ ì¢…ë£Œ ê²€í† 

### ë‹¨ê¸° ì¡°ì¹˜ (1ì¼)
- [ ] ë°°ì¹˜ ì‘ì—…ì— ì—°ê²° íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ)
- [ ] Connection leak ëª¨ë‹ˆí„°ë§ ì¶”ê°€
- [ ] ì—°ê²° í’€ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### ì¥ê¸° ì¡°ì¹˜ (1ì£¼)
- [ ] ë°°ì¹˜ ì‘ì—… ë³„ë„ DB ì—°ê²° í’€ ë¶„ë¦¬
- [ ] ìë™ ìŠ¤ì¼€ì¼ë§ ì •ì±… ìˆ˜ë¦½
- [ ] ì•Œë¦¼ ì„ê³„ê°’ ì„¤ì • (ì‚¬ìš©ë¥  80% ì´ìƒ)

**ğŸ“Š ì˜í–¥ ë²”ìœ„**: ê²°ì œ ê¸°ëŠ¥ ì „ì²´ ë§ˆë¹„ë¡œ ì˜ˆìƒ ë§¤ì¶œ ì†ì‹¤ ë°œìƒ
```

#### **ì˜ˆì‹œ 2: "user-serviceì˜ ì‘ë‹µ ì‹œê°„ì´ ëŠë¦° APIëŠ”?"**

**Agent ì¶”ë¡  ê³¼ì •**:

```
Thought: ì‚¬ìš©ìê°€ user-serviceì—ì„œ ëŠë¦° APIë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.
ì„±ëŠ¥ ê´€ë ¨ ì§ˆë¬¸ì´ë¯€ë¡œ get_slowest_apis ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
service_name í•„í„°ë¥¼ "user-service"ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤.

Action: get_slowest_apis
Action Input: {"service_name": "user-service", "limit": 5}

Observation:
## ëŠë¦° API Top 5 (user-service)

| ìˆœìœ„ | API | í‰ê·  ì‘ë‹µì‹œê°„ | P95 | P99 | í˜¸ì¶œ íšŸìˆ˜ |
|------|-----|--------------|-----|-----|----------|
| 1 | GET /api/users/search | 2,340ms | 4,500ms | 8,200ms | 1,234 |
| 2 | POST /api/users/bulk-update | 1,850ms | 3,200ms | 5,100ms | 89 |
| 3 | GET /api/users/{id}/activities | 980ms | 1,800ms | 3,500ms | 5,678 |
| 4 | GET /api/users/{id}/profile | 450ms | 850ms | 1,200ms | 12,345 |
| 5 | PUT /api/users/{id} | 320ms | 600ms | 900ms | 2,567 |

Thought: ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.
GET /api/users/searchê°€ ê°€ì¥ ëŠë¦¬ë©° (í‰ê·  2.3ì´ˆ), P99ê°€ 8.2ì´ˆë¡œ ì‹¬ê°í•©ë‹ˆë‹¤.
ì¶”ê°€ë¡œ ì´ APIì˜ ì—ëŸ¬ìœ¨ë„ í™•ì¸í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

Action: get_api_error_rates
Action Input: {"api_path": "/api/users/search"}

Observation:
## API ì—ëŸ¬ìœ¨: /api/users/search

- ì´ í˜¸ì¶œ: 1,234íšŒ
- ì„±ê³µ: 1,156íšŒ (93.7%)
- ì‹¤íŒ¨: 78íšŒ (6.3%)
  - 5xx: 45íšŒ (3.6%) - ëŒ€ë¶€ë¶„ 503 Service Unavailable
  - 4xx: 33íšŒ (2.7%) - ì£¼ë¡œ 400 Bad Request

Thought: ì—ëŸ¬ìœ¨ë„ 6.3%ë¡œ ë†’ìŠµë‹ˆë‹¤. ì´ì œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

Final Answer:
## âš ï¸ user-service ëŠë¦° API ë¶„ì„

### 1ìœ„: GET /api/users/search ğŸ”´

**ì„±ëŠ¥ ì§€í‘œ**:
- â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: **2,340ms** (2.3ì´ˆ)
- ğŸ“Š P95: 4,500ms / P99: 8,200ms
- ğŸ“ í˜¸ì¶œ íšŸìˆ˜: 1,234íšŒ

**ì—ëŸ¬ìœ¨**: 6.3% (78ê±´ ì‹¤íŒ¨)
- 5xx ì—ëŸ¬: 45ê±´ (3.6%) - ì£¼ë¡œ **503 Service Unavailable**
- 4xx ì—ëŸ¬: 33ê±´ (2.7%) - **400 Bad Request**

**ğŸ¤– ë¶„ì„**:
ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì „ì²´ í…Œì´ë¸” ìŠ¤ìº”ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ì¸ë±ìŠ¤ê°€ ì—†ì–´ ëŠë¦° ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.
P99ê°€ 8.2ì´ˆë¡œ ì¼ë¶€ ìš”ì²­ì€ íƒ€ì„ì•„ì›ƒì— ê·¼ì ‘í•˜ë©°, 503 ì—ëŸ¬ëŠ” DB ë¶€í•˜ë¡œ ì¸í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.

**âœ… ê¶Œì¥ ì¡°ì¹˜**:

### ì¦‰ì‹œ ì¡°ì¹˜
- [ ] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„ (EXPLAIN)
- [ ] ì„ì‹œë¡œ ê²°ê³¼ ê°œìˆ˜ ì œí•œ ì¶”ê°€ (LIMIT 100)

### ë‹¨ê¸° ì¡°ì¹˜
- [ ] ê²€ìƒ‰ í•„ë“œì— ì¸ë±ìŠ¤ ì¶”ê°€ (name, email)
- [ ] í˜ì´ì§€ë„¤ì´ì…˜ ê°•ì œ ì ìš©
- [ ] ìºì‹± ë ˆì´ì–´ ì¶”ê°€ (Redis, 5ë¶„ TTL)

### ì¥ê¸° ì¡°ì¹˜
- [ ] ElasticSearch ë„ì… ê²€í† 
- [ ] API Rate Limiting ì ìš©
- [ ] ë¹„ë™ê¸° ê²€ìƒ‰ ì „í™˜ (ê²°ê³¼ë¥¼ ì´ë©”ì¼ ì „ì†¡)

---

### 2-5ìœ„ ìš”ì•½

| API | í‰ê·  | ì¡°ì¹˜ í•„ìš”ë„ |
|-----|------|------------|
| POST /api/users/bulk-update | 1,850ms | ğŸŸ¡ ì¤‘ê°„ (ë°°ì¹˜ ì‘ì—… ìµœì í™”) |
| GET /api/users/{id}/activities | 980ms | ğŸŸ¡ ì¤‘ê°„ (JOIN ì¿¼ë¦¬ ê°œì„ ) |
| GET /api/users/{id}/profile | 450ms | ğŸŸ¢ ë‚®ìŒ |
| PUT /api/users/{id} | 320ms | ğŸŸ¢ ë‚®ìŒ |
```

---

### 3.2.5 40+ ë„êµ¬ ì™„ì „ ëª©ë¡

#### **ë„êµ¬ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜**

| ì¹´í…Œê³ ë¦¬ | ë„êµ¬ ê°œìˆ˜ | ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€ |
|---------|---------|--------------|
| **Search** | 3 | í‚¤ì›Œë“œ ê²€ìƒ‰, ìœ ì‚¬ë„ ê²€ìƒ‰, ë³µí•© í•„í„° |
| **Analysis** | 10 | í†µê³„, ì—ëŸ¬ ë¶„ì„, ìƒê´€ê´€ê³„, íŒ¨í„´ ë¶„ì„ |
| **Detail** | 2 | ë‹¨ì¼ ë¡œê·¸ ì¡°íšŒ, trace ê¸°ë°˜ ì¡°íšŒ |
| **Performance** | 3 | ëŠë¦° API, íŠ¸ë˜í”½ íŒ¨í„´, HTTP ì—ëŸ¬ |
| **Monitoring** | 8 | ì—ëŸ¬ìœ¨, ì„œë¹„ìŠ¤ ê±´ê°•, ì´ìƒ íƒì§€ |
| **Comparison** | 2 | ì‹œê°„ëŒ€ ë¹„êµ, ê³„ë‹¨ì‹ ì¥ì•  |
| **Alert** | 2 | ì•Œë¦¼ ì¡°ê±´ í‰ê°€, ë¦¬ì†ŒìŠ¤ ì´ìŠˆ |
| **Deployment** | 1 | ë°°í¬ ì˜í–¥ ë¶„ì„ |
| **User Tracking** | 3 | ì‚¬ìš©ì ì„¸ì…˜, íŒŒë¼ë¯¸í„° ë¶„í¬, ì—ëŸ¬ ì „íŒŒ |
| **Architecture** | 3 | ë ˆì´ì–´ë³„ ì—ëŸ¬, ì»´í¬ë„ŒíŠ¸ í˜¸ì¶œ, í•«ìŠ¤íŒŸ |
| **Pattern Detection** | 4 | ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ êµ°ì§‘, ë™ì‹œì„± ë¬¸ì œ, ì£¼ê¸°ì  ì—ëŸ¬ |
| **Statistics Comparison** | 2 | AI vs DB, ì‹œê°„ëŒ€ë³„ ë¹„êµ |
| **Total** | **43** | - |

#### **ì „ì²´ ë„êµ¬ ìƒì„¸ ëª©ë¡**

**Search Tools** (`app/tools/search_tools.py`):

1. **search_logs_by_keyword**
   - **ì„¤ëª…**: ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
   - **ì…ë ¥**: `keyword` (str), `limit` (int, ê¸°ë³¸ 20), `time_hours` (int, ê¸°ë³¸ 24)
   - **ì¶œë ¥**: ë§¤ì¹­ëœ ë¡œê·¸ ëª©ë¡ (ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”)
   - **OpenSearch ì¿¼ë¦¬**: `{"match": {"message": keyword}}`

2. **search_logs_by_similarity**
   - **ì„¤ëª…**: í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ ë¡œê·¸ ë²¡í„° ê²€ìƒ‰
   - **ì…ë ¥**: `query_text` (str), `limit` (int, ê¸°ë³¸ 10), `min_score` (float, ê¸°ë³¸ 0.7)
   - **ì¶œë ¥**: ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë¡œê·¸ ëª©ë¡
   - **OpenSearch ì¿¼ë¦¬**: KNN `script_score` with `cosineSimilarity`

3. **search_logs_advanced**
   - **ì„¤ëª…**: ë³µí•© í•„í„° ê²€ìƒ‰ (ë ˆë²¨, ì„œë¹„ìŠ¤, ì‹œê°„ ë²”ìœ„ ë“±)
   - **ì…ë ¥**: `level` (str, optional), `service_name` (str, optional), `start_time` (str, optional), `end_time` (str, optional), `message_contains` (str, optional), `limit` (int)
   - **ì¶œë ¥**: í•„í„°ë§ëœ ë¡œê·¸ ëª©ë¡
   - **OpenSearch ì¿¼ë¦¬**: `{"bool": {"must": [...filters...]}}`

**Analysis Tools** (`app/tools/analysis_tools.py`):

4. **get_log_statistics**
   - **ì„¤ëª…**: ì „ì²´ ë¡œê·¸ í†µê³„ (ì´ ê°œìˆ˜, ë ˆë²¨ë³„ ë¶„í¬)
   - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
   - **ì¶œë ¥**: í†µê³„ ìš”ì•½ (ì´ ë¡œê·¸, ERROR/WARN/INFO ê°œìˆ˜, ë¹„ìœ¨)

5. **get_recent_errors**
   - **ì„¤ëª…**: ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ëª©ë¡ (AI ë¶„ì„ í¬í•¨)
   - **ì…ë ¥**: `limit` (int, ê¸°ë³¸ 10), `time_hours` (int, ê¸°ë³¸ 24), `severity` (str, optional)
   - **ì¶œë ¥**: ì—ëŸ¬ ë¡œê·¸ + ai_analysis ìš”ì•½

6. **correlate_logs**
   - **ì„¤ëª…**: trace_id ë˜ëŠ” request_idë¡œ ê´€ë ¨ ë¡œê·¸ ì°¾ê¸°
   - **ì…ë ¥**: `trace_id` (str) or `request_id` (str)
   - **ì¶œë ¥**: ì‹œê°„ ìˆœ ì •ë ¬ëœ ê´€ë ¨ ë¡œê·¸ (ìš”ì²­ íë¦„ ì¶”ì )

7. **analyze_errors_unified**
   - **ì„¤ëª…**: ì—¬ëŸ¬ ì—ëŸ¬ë¥¼ í•œ ë²ˆì— ë¶„ì„ (ë°°ì¹˜)
   - **ì…ë ¥**: `log_ids` (List[int])
   - **ì¶œë ¥**: ê° ì—ëŸ¬ì˜ AI ë¶„ì„ í†µí•© ìš”ì•½

8. **analyze_single_log**
   - **ì„¤ëª…**: ë‹¨ì¼ ë¡œê·¸ ê¹Šì´ ë¶„ì„ (GPT-4o mini)
   - **ì…ë ¥**: `log_id` (int)
   - **ì¶œë ¥**: summary, error_cause, solution, tags

9. **analyze_request_patterns**
   - **ì„¤ëª…**: API ìš”ì²­ body íŒ¨í„´ ë¶„ì„
   - **ì…ë ¥**: `api_path` (str), `limit` (int, ê¸°ë³¸ 50)
   - **ì¶œë ¥**: ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„°, null ë¹„ìœ¨ ë“±

10. **analyze_response_failures**
    - **ì„¤ëª…**: API ì‘ë‹µ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
    - **ì…ë ¥**: `api_path` (str), `status_code` (int, optional)
    - **ì¶œë ¥**: ì‹¤íŒ¨ ì›ì¸ ë¶„ë¥˜ (íƒ€ì„ì•„ì›ƒ, 4xx, 5xx)

11. **get_error_context**
    - **ì„¤ëª…**: ì—ëŸ¬ ë°œìƒ ì „í›„ ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸ ìˆ˜ì§‘
    - **ì…ë ¥**: `log_id` (int), `window_seconds` (int, ê¸°ë³¸ 10)
    - **ì¶œë ¥**: Â±Nì´ˆ ìœˆë„ìš° ë‚´ ëª¨ë“  ë¡œê·¸

12. **detect_error_propagation_path**
    - **ì„¤ëª…**: ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ ì¶”ì  (Controller â†’ Service â†’ Repo)
    - **ì…ë ¥**: `trace_id` (str)
    - **ì¶œë ¥**: ë ˆì´ì–´ë³„ ì—ëŸ¬ ì „íŒŒ ì‹œí€€ìŠ¤

13. **analyze_batch_job_failures**
    - **ì„¤ëª…**: ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨ ë¶„ì„
    - **ì…ë ¥**: `job_name` (str), `time_hours` (int)
    - **ì¶œë ¥**: ì‹¤íŒ¨ ë¹ˆë„, ì›ì¸, ì¬ì‹œë„ í•„ìš” ì—¬ë¶€

**Detail Tools** (`app/tools/detail_tools.py`):

14. **get_log_detail**
    - **ì„¤ëª…**: ë‹¨ì¼ ë¡œê·¸ ì›ë³¸ ë°ì´í„° ì¡°íšŒ
    - **ì…ë ¥**: `log_id` (int)
    - **ì¶œë ¥**: ì „ì²´ JSON (log_details í¬í•¨)

15. **get_logs_by_trace_id**
    - **ì„¤ëª…**: trace_idë¡œ ëª¨ë“  ë¡œê·¸ ì¡°íšŒ
    - **ì…ë ¥**: `trace_id` (str)
    - **ì¶œë ¥**: ì‹œê°„ ìˆœ ì •ë ¬ëœ ë¡œê·¸ ëª©ë¡

**Performance Tools** (`app/tools/performance_tools.py`):

16. **get_slowest_apis**
    - **ì„¤ëª…**: ì‘ë‹µ ì‹œê°„ ëŠë¦° API ìˆœìœ„
    - **ì…ë ¥**: `limit` (int, ê¸°ë³¸ 10), `service_name` (str, optional), `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: APIë³„ í‰ê· /P95/P99 ì‘ë‹µ ì‹œê°„ í…Œì´ë¸”

17. **get_traffic_by_time**
    - **ì„¤ëª…**: ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„í¬
    - **ì…ë ¥**: `interval` (str, "hour" or "day"), `time_hours` (int)
    - **ì¶œë ¥**: ì‹œê°„ëŒ€ë³„ ë¡œê·¸ ê°œìˆ˜ (íˆìŠ¤í† ê·¸ë¨ ë°ì´í„°)

18. **analyze_http_error_matrix**
    - **ì„¤ëª…**: HTTP ìƒíƒœ ì½”ë“œ ë§¤íŠ¸ë¦­ìŠ¤ (4xx vs 5xx)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: ìƒíƒœ ì½”ë“œë³„ ê°œìˆ˜ (400, 404, 500, 503 ë“±)

**Monitoring Tools** (`app/tools/monitoring_tools.py`):

19. **get_error_rate_trend**
    - **ì„¤ëª…**: ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ ì¶”ì´
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24), `interval` (str, "hour")
    - **ì¶œë ¥**: ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ % ê·¸ë˜í”„ ë°ì´í„°

20. **get_service_health_status**
    - **ì„¤ëª…**: ì„œë¹„ìŠ¤ë³„ ê±´ê°• ìƒíƒœ (ì—ëŸ¬ìœ¨, ë¡œê·¸ ê°œìˆ˜)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24), `min_logs` (int, ê¸°ë³¸ 10)
    - **ì¶œë ¥**: ì„œë¹„ìŠ¤ë³„ ğŸŸ¢ğŸŸ¡ğŸ”´ ìƒíƒœ í…Œì´ë¸”

21. **get_error_frequency_ranking**
    - **ì„¤ëª…**: ê°€ì¥ ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ìˆœìœ„
    - **ì…ë ¥**: `limit` (int, ê¸°ë³¸ 10), `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: ì—ëŸ¬ íƒ€ì…ë³„ ë°œìƒ ë¹ˆë„

22. **get_api_error_rates**
    - **ì„¤ëª…**: API ì—”ë“œí¬ì¸íŠ¸ë³„ ì—ëŸ¬ìœ¨
    - **ì…ë ¥**: `api_path` (str, optional), `limit` (int, ê¸°ë³¸ 20)
    - **ì¶œë ¥**: APIë³„ ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨ í…Œì´ë¸”

23. **get_affected_users_count**
    - **ì„¤ëª…**: ì—ëŸ¬ë¡œ ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜ (IP ê¸°ë°˜)
    - **ì…ë ¥**: `error_type` (str, optional), `time_hours` (int)
    - **ì¶œë ¥**: ê³ ìœ  IP ê°œìˆ˜, ìƒìœ„ ì˜í–¥ ì‚¬ìš©ì

24. **detect_anomalies**
    - **ì„¤ëª…**: í†µê³„ì  ì´ìƒ íƒì§€ (Z-score)
    - **ì…ë ¥**: `metric` (str, "error_rate" or "traffic"), `sensitivity` (float, ê¸°ë³¸ 3.0)
    - **ì¶œë ¥**: ì´ìƒ ì‹œê°„ëŒ€ ëª©ë¡ (Z-score > threshold)

25. **compare_source_types**
    - **ì„¤ëª…**: Frontend vs Backend ì—ëŸ¬ ë¹„êµ
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: FE/BEë³„ ì—ëŸ¬ ê°œìˆ˜, ë¹„ìœ¨, ì£¼ìš” ì—ëŸ¬

26. **analyze_logger_activity**
    - **ì„¤ëª…**: Loggerë³„ ë¡œê¹… ë¹ˆë„ (ë…¸ì´ì¦ˆ íƒì§€)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: Loggerë³„ ë¡œê·¸ ê°œìˆ˜, ê³¼ë„í•œ ë¡œê¹… ê²½ê³ 

**Comparison Tools** (`app/tools/comparison_tools.py`):

27. **compare_time_periods**
    - **ì„¤ëª…**: ë‘ ì‹œê°„ëŒ€ ë¹„êµ (ì˜¤ëŠ˜ vs ì–´ì œ)
    - **ì…ë ¥**: `period1_start` (str), `period1_end` (str), `period2_start` (str), `period2_end` (str)
    - **ì¶œë ¥**: ê¸°ê°„ë³„ ë¡œê·¸ ê°œìˆ˜, ì—ëŸ¬ìœ¨ ë¹„êµ í…Œì´ë¸”

28. **detect_cascading_failures**
    - **ì„¤ëª…**: ê³„ë‹¨ì‹ ì¥ì•  íƒì§€ (ì„œë¹„ìŠ¤ ê°„ ì˜ì¡´ì„±)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 1)
    - **ì¶œë ¥**: ì—°ì‡„ ì¥ì•  ê²½ë¡œ (A â†’ B â†’ C ì‹¤íŒ¨)

**Alert Tools** (`app/tools/alert_tools.py`):

29. **evaluate_alert_conditions**
    - **ì„¤ëª…**: ì•Œë¦¼ ì¡°ê±´ í‰ê°€ (ì„ê³„ê°’ ì´ˆê³¼ í™•ì¸)
    - **ì…ë ¥**: `metric` (str), `threshold` (float), `time_hours` (int)
    - **ì¶œë ¥**: ì•Œë¦¼ ë°œë™ ì—¬ë¶€ + í˜„ì¬ ê°’

30. **detect_resource_issues**
    - **ì„¤ëª…**: ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ íƒì§€ (ë©”ëª¨ë¦¬, DB í’€ ë“±)
    - **ì…ë ¥**: `resource_type` (str, "memory" or "db_pool"), `time_hours` (int)
    - **ì¶œë ¥**: OutOfMemory, ConnectionPoolExhausted ì—ëŸ¬ ëª©ë¡

**Deployment Tools** (`app/tools/deployment_tools.py`):

31. **analyze_deployment_impact**
    - **ì„¤ëª…**: ë°°í¬ ì „í›„ ë¹„êµ (ì—ëŸ¬ìœ¨ ë³€í™”)
    - **ì…ë ¥**: `deployment_time` (str, ISO format), `window_hours` (int, ê¸°ë³¸ 2)
    - **ì¶œë ¥**: ë°°í¬ ì „ 1ì‹œê°„ vs í›„ 1ì‹œê°„ ì—ëŸ¬ìœ¨ ë¹„êµ

**User Tracking Tools** (`app/tools/user_tracking_tools.py`):

32. **trace_user_session**
    - **ì„¤ëª…**: íŠ¹ì • IP/ì‚¬ìš©ìì˜ ì„¸ì…˜ ì¶”ì 
    - **ì…ë ¥**: `ip_address` (str), `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: í•´ë‹¹ IPì˜ ëª¨ë“  ìš”ì²­ ì‹œí€€ìŠ¤

33. **analyze_parameter_distribution**
    - **ì„¤ëª…**: ë©”ì†Œë“œ íŒŒë¼ë¯¸í„° null ë¶„í¬ ë¶„ì„
    - **ì…ë ¥**: `method_name` (str), `time_hours` (int)
    - **ì¶œë ¥**: íŒŒë¼ë¯¸í„°ë³„ null ë¹„ìœ¨, ì—ëŸ¬ ìƒê´€ê´€ê³„

34. **trace_error_propagation**
    - **ì„¤ëª…**: ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ ì¶”ì  (ìƒì„¸)
    - **ì…ë ¥**: `initial_log_id` (int)
    - **ì¶œë ¥**: ì—ëŸ¬ê°€ ì „íŒŒëœ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ê²½ë¡œ

**Architecture Tools** (`app/tools/architecture_tools.py`):

35. **analyze_error_by_layer**
    - **ì„¤ëª…**: ë ˆì´ì–´ë³„ ì—ëŸ¬ ë¶„í¬ (Controller/Service/Repository)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: ë ˆì´ì–´ë³„ ì—ëŸ¬ ê°œìˆ˜, ë¹„ìœ¨ (pie chart ë°ì´í„°)

36. **trace_component_calls**
    - **ì„¤ëª…**: ì»´í¬ë„ŒíŠ¸ í˜¸ì¶œ ì²´ì¸ ì¶”ì 
    - **ì…ë ¥**: `trace_id` (str)
    - **ì¶œë ¥**: ì»´í¬ë„ŒíŠ¸ ê°„ í˜¸ì¶œ ê·¸ë˜í”„ (A â†’ B â†’ C)

37. **get_hottest_methods**
    - **ì„¤ëª…**: ê°€ì¥ ìì£¼ ì‹¤í–‰ë˜ëŠ” ë©”ì†Œë“œ (í•«ìŠ¤íŒŸ)
    - **ì…ë ¥**: `limit` (int, ê¸°ë³¸ 10), `time_hours` (int)
    - **ì¶œë ¥**: ë©”ì†Œë“œë³„ ì‹¤í–‰ íšŸìˆ˜ ìˆœìœ„

**Pattern Detection Tools** (`app/tools/pattern_detection_tools.py`):

38. **cluster_stack_traces**
    - **ì„¤ëª…**: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ êµ°ì§‘í™” (ìœ ì‚¬ ì—ëŸ¬ ê·¸ë£¹í™”)
    - **ì…ë ¥**: `min_cluster_size` (int, ê¸°ë³¸ 3), `time_hours` (int)
    - **ì¶œë ¥**: í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ + ê°œìˆ˜

39. **detect_concurrency_issues**
    - **ì„¤ëª…**: ë™ì‹œì„± ë¬¸ì œ íƒì§€ (Deadlock, Race condition)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: Deadlock, Thread starvation ë¡œê·¸ ëª©ë¡

40. **detect_recurring_errors**
    - **ì„¤ëª…**: ì£¼ê¸°ì  ì—ëŸ¬ íƒì§€ (cron job ì‹¤íŒ¨ ë“±)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 168),  `min_occurrences` (int, ê¸°ë³¸ 3)
    - **ì¶œë ¥**: ì£¼ê¸° íŒ¨í„´ (ë§¤ì¼ 02:00, ë§¤ì£¼ ì›”ìš”ì¼ ë“±)

41. **analyze_error_lifetime**
    - **ì„¤ëª…**: ì—ëŸ¬ ì§€ì† ì‹œê°„ ë¶„ì„ (ì–¸ì œ í•´ê²°ë˜ì—ˆë‚˜?)
    - **ì…ë ¥**: `error_type` (str), `time_hours` (int)
    - **ì¶œë ¥**: ì—ëŸ¬ ë°œìƒ ì‹œì‘ ~ ì¢…ë£Œ ì‹œê°„, í‰ê·  ì§€ì† ì‹œê°„

**Statistics Comparison Tools** (`app/tools/statistics_comparison_tools.py`):

42. **compare_ai_vs_db_statistics**
    - **ì„¤ëª…**: AI ì¶”ë¡  vs DB ì§‘ê³„ ì •í™•ë„ ë¹„êµ
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24), `sample_size` (int, ê¸°ë³¸ 100)
    - **ì¶œë ¥**: ì •í™•ë„ %, ì‹ ë¢°ë„, ì¶”ì²œì‚¬í•­

43. **get_hourly_comparison**
    - **ì„¤ëª…**: ì‹œê°„ëŒ€ë³„ ë¡œê·¸ ë¶„í¬ (AI vs DB)
    - **ì…ë ¥**: `time_hours` (int, ê¸°ë³¸ 24)
    - **ì¶œë ¥**: ì‹œê°„ëŒ€ë³„ ê°œìˆ˜ ë¹„êµ í…Œì´ë¸”

---

### 3.2.6 ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬

**íŒŒì¼**: `app/services/chatbot_service_v2.py:344-365`

**LangChain ë©”ì‹œì§€ ë³€í™˜**:

```python
from langchain_core.messages import HumanMessage, AIMessage

def convert_chat_history(chat_history: List[ChatMessage]) -> str:
    """ì±„íŒ… ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""

    langchain_history = []
    for msg in chat_history:
        if msg.role == "user":
            langchain_history.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            langchain_history.append(AIMessage(content=msg.content))

    # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í”„ë¡¬í”„íŠ¸ ì£¼ì…ìš©)
    history_text = "\n\n## ì´ì „ ëŒ€í™”:\n"
    for msg in langchain_history:
        role = "ğŸ‘¤ User" if isinstance(msg, HumanMessage) else "ğŸ¤– Assistant"
        history_text += f"\n**{role}**: {msg.content}\n"

    return history_text
```

**í”„ë¡¬í”„íŠ¸ ì£¼ì…**:

```python
# Agent ì‹¤í–‰ ì‹œ
agent_input = {
    "input": current_question,
    "chat_history": convert_chat_history(chat_history)
}

result = await agent_executor.ainvoke(agent_input)
```

**ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™œìš© ì˜ˆì‹œ**:

```
ì´ì „ ëŒ€í™”:

**ğŸ‘¤ User**: ìµœê·¼ 24ì‹œê°„ ë™ì•ˆ ì—ëŸ¬ê°€ ëª‡ ê°œì•¼?

**ğŸ¤– Assistant**: ìµœê·¼ 24ì‹œê°„ ë™ì•ˆ **ERROR ë ˆë²¨ ë¡œê·¸ëŠ” ì´ 45ê°œ**ì…ë‹ˆë‹¤.
- CRITICAL: 2ê°œ
- HIGH: 15ê°œ
- MEDIUM: 28ê°œ

**ğŸ‘¤ User**: ê·¸ì¤‘ì— ê°€ì¥ ì‹¬ê°í•œ ê±° ì•Œë ¤ì¤˜

**ğŸ¤– Assistant**: [ì´ì „ ëŒ€í™”ì—ì„œ "ì—ëŸ¬ 45ê°œ" ì–¸ê¸‰ â†’ ê·¸ ì¤‘ CRITICAL 2ê°œ ì¤‘ ì„ íƒ]
ê°€ì¥ ì‹¬ê°í•œ ì—ëŸ¬ëŠ” **DatabaseTimeout** (log_id: 12345)ì…ë‹ˆë‹¤...
```

**ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ**:
- ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
- í† í° ì œí•œ (8K) ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±°

---

### 3.2.7 Streaming SSE ë©”ì»¤ë‹ˆì¦˜

**íŒŒì¼**: `app/services/chatbot_service_v2.py:498-675`

**AsyncGenerator íŒ¨í„´**:

```python
from typing import AsyncGenerator, Tuple

async def ask_stream(
    question: str,
    project_uuid: str,
    chat_history: List[ChatMessage]
) -> AsyncGenerator[Tuple[str, str], None]:
    """
    SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±

    Yields:
        Tuple[event_type, data]
        - ("chunk", "í…ìŠ¤íŠ¸"): ì‘ë‹µ ì¡°ê°
        - ("done", ""): ì™„ë£Œ ì‹ í˜¸
        - ("error", "ë©”ì‹œì§€"): ì—ëŸ¬ ë°œìƒ
    """

    try:
        # Agent ìƒì„±
        tools = bind_tools_to_project(project_uuid)
        agent_executor = create_log_analysis_agent(project_uuid, tools, chat_history)

        # ìŠ¤íŠ¸ë¦¬ë° ë²„í¼
        buffer = ""
        is_streaming = False

        # Agent ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
        async for event in agent_executor.astream_events(
            {"input": question, "chat_history": convert_chat_history(chat_history)},
            version="v1"
        ):
            # LLM ì²­í¬ ì´ë²¤íŠ¸ë§Œ ì²˜ë¦¬
            if event["event"] == "on_chat_model_stream":
                chunk = event["data"]["chunk"]

                if chunk.content:
                    buffer += chunk.content

                    # "Final Answer:" ë§ˆì»¤ íƒì§€
                    if "Final Answer:" in buffer and not is_streaming:
                        # Final Answer ì´í›„ í…ìŠ¤íŠ¸ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                        is_streaming = True
                        parts = buffer.split("Final Answer:", 1)
                        answer_start = parts[1].lstrip()

                        # ì²« ì²­í¬ ì „ì†¡
                        yield ("chunk", answer_start)

                    elif is_streaming:
                        # ì´í›„ ì²­í¬ ê³„ì† ì „ì†¡
                        yield ("chunk", chunk.content)

        # ì™„ë£Œ ì‹ í˜¸
        yield ("done", "")

    except Exception as e:
        logger.error(f"Streaming error: {e}")
        yield ("error", str(e))
```

**FastAPI SSE ì—”ë“œí¬ì¸íŠ¸**:

```python
from fastapi.responses import StreamingResponse

@router.post("/api/v2/chatbot/ask/stream")
async def chatbot_ask_stream(request: ChatRequest):
    """SSE ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸"""

    async def generate():
        """SSE í˜•ì‹ ìƒì„±ê¸°"""
        async for event_type, data in chatbot_service_v2.ask_stream(
            question=request.question,
            project_uuid=request.project_uuid,
            chat_history=request.chat_history
        ):
            if event_type == "chunk":
                # SSE í˜•ì‹: data: {ë‚´ìš©}\n\n
                yield f"data: {data}\n\n"

            elif event_type == "done":
                yield f"data: [DONE]\n\n"

            elif event_type == "error":
                yield f"data: [ERROR] {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )
```

**í´ë¼ì´ì–¸íŠ¸ ìˆ˜ì‹  (Frontend)**:

```javascript
// React + EventSource
const eventSource = new EventSource('/api/v2/chatbot/ask/stream', {
  method: 'POST',
  body: JSON.stringify({
    question: "ìµœê·¼ ì—ëŸ¬ëŠ”?",
    project_uuid: "...",
    chat_history: []
  })
});

let fullResponse = "";

eventSource.onmessage = (event) => {
  if (event.data === "[DONE]") {
    eventSource.close();
    console.log("Full response:", fullResponse);
  } else if (event.data.startsWith("[ERROR]")) {
    console.error("Error:", event.data);
    eventSource.close();
  } else {
    // ì²­í¬ ëˆ„ì 
    fullResponse += event.data;
    // UI ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼)
    setAnswer(fullResponse);
  }
};

eventSource.onerror = (error) => {
  console.error("SSE error:", error);
  eventSource.close();
};
```

**SSE ìŠ¤íŠ¸ë¦¼ ì˜ˆì‹œ**:

```
data: ìµœê·¼

data:  24

data: ì‹œê°„

data:  ë™ì•ˆ

data:

data: **

data: ERROR

data:  ë ˆë²¨

data:  ë¡œê·¸ëŠ”

data:  ì´

data:

data: 45

data: ê°œ

data: **

data: ì…ë‹ˆë‹¤

data: .

data: [DONE]
```

---

(ê³„ì†...)

## 3.3 HTML ë¬¸ì„œ ìƒì„±

### 3.3.1 ì „ì²´ í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    Start([POST /api/v2-langgraph/analysis/.../html-document]) --> ValidateType{Document Type?}

    ValidateType -->|PROJECT_ANALYSIS| LoadProjectTemplate[í”„ë¡œì íŠ¸ ë¶„ì„ í…œí”Œë¦¿<br/>project_analysis.html]
    ValidateType -->|ERROR_ANALYSIS| LoadErrorTemplate[ì—ëŸ¬ ë¶„ì„ í…œí”Œë¦¿<br/>error_analysis.html]
    ValidateType -->|Invalid| Error400[âŒ 400 Bad Request]

    LoadProjectTemplate --> AnalyzeHealth[AI ê±´ê°• ë¶„ì„<br/>OpenSearch + GPT-4o mini]
    LoadErrorTemplate --> PrepareContext[í…œí”Œë¦¿ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„<br/>ë°ì´í„° ì¶”ì¶œ]

    AnalyzeHealth --> PrepareProjectContext[í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„<br/>projectInfo, metrics, AI insights]

    PrepareProjectContext --> RenderJinja[Jinja2 ë Œë”ë§<br/>í…œí”Œë¦¿ + ì»¨í…ìŠ¤íŠ¸]
    PrepareContext --> RenderJinja

    RenderJinja --> CalcMetadata[ë©”íƒ€ë°ì´í„° ê³„ì‚°<br/>word_count, reading_time, health_score]

    CalcMetadata --> ValidateHTML[HTML ê²€ì¦<br/>íƒœê·¸, ì„¹ì…˜ í™•ì¸]

    ValidateHTML --> End([AiHtmlDocumentResponse ë°˜í™˜<br/>html_content + metadata])

    Error400 --> EndError([ì—ëŸ¬ ì‘ë‹µ])

    style AnalyzeHealth fill:#d1ecf1
    style RenderJinja fill:#d4edda
    style ValidateHTML fill:#fff3cd
    style End fill:#ffeaa7
```

### 3.3.2 í”„ë¡œì íŠ¸ ë¶„ì„ ë¬¸ì„œ ìƒì„±

**íŒŒì¼**: `app/services/html_document_service.py:102-177`

#### **Step 1: AI ê±´ê°• ë¶„ì„**

```python
async def _analyze_project_health(
    project_uuid: str,
    time_range: dict,
    metrics: dict
) -> str:
    """GPT-4o minië¡œ í”„ë¡œì íŠ¸ ê±´ê°• ìƒíƒœ ë¶„ì„"""

    # 1. OpenSearchì—ì„œ ìµœê·¼ ERROR/WARN ë¡œê·¸ 100ê°œ ìˆ˜ì§‘
    query = {
        "size": 100,
        "query": {
            "bool": {
                "must": [
                    {"term": {"project_uuid.keyword": project_uuid}},
                    {"range": {"timestamp": {
                        "gte": time_range["startTime"],
                        "lte": time_range["endTime"]
                    }}}
                ],
                "should": [
                    {"term": {"level": "ERROR"}},
                    {"term": {"level": "WARN"}}
                ],
                "minimum_should_match": 1
            }
        },
        "sort": [{"timestamp": "desc"}]
    }

    response = opensearch_client.search(
        index=f"{project_uuid.replace('-', '_')}_*",
        body=query
    )

    # 2. ì»´í¬ë„ŒíŠ¸ë³„ ì—ëŸ¬ ì§‘ê³„
    component_errors = defaultdict(lambda: {"ERROR": 0, "WARN": 0})

    for hit in response["hits"]["hits"]:
        source = hit["_source"]
        component = source.get("service_name") or source.get("component_name", "Unknown")
        level = source.get("level", "INFO")

        if level in ["ERROR", "WARN"]:
            component_errors[component][level] += 1

    # 3. ìƒìœ„ 5ê°œ ë¬¸ì œ ì»´í¬ë„ŒíŠ¸ ì¶”ì¶œ
    top_components = sorted(
        component_errors.items(),
        key=lambda x: x[1]["ERROR"] * 2 + x[1]["WARN"],  # ERROR ê°€ì¤‘ì¹˜ 2ë°°
        reverse=True
    )[:5]

    # 4. GPT-4o minië¡œ ê±´ê°• ìƒíƒœ ìš”ì•½ ìƒì„±
    prompt = f"""ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë¶„ì„ ìš”ì²­:

**ì „ì²´ í†µê³„**:
- ì´ ë¡œê·¸: {metrics['totalLogs']:,}ê°œ
- ERROR: {metrics['errorCount']:,}ê°œ ({metrics['errorCount']/metrics['totalLogs']*100:.2f}%)
- WARN: {metrics['warnCount']:,}ê°œ

**ì£¼ìš” ë¬¸ì œ ì»´í¬ë„ŒíŠ¸**:
{chr(10).join(f"{i+1}. {comp}: ERROR {stats['ERROR']}, WARN {stats['WARN']}" for i, (comp, stats) in enumerate(top_components))}

**ì‹œê°„ ë²”ìœ„**: {time_range['startTime']} ~ {time_range['endTime']}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ìš”êµ¬ì‚¬í•­:
- í•œêµ­ì–´ë¡œ ì‘ì„±
- ë§ˆí¬ë‹¤ìš´ ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ
- ê¸ì •ì /ë¶€ì •ì  í‰ê°€ í¬í•¨
- ì£¼ì˜ê°€ í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ ëª…ì‹œ
"""

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,
        max_tokens=500
    )

    result = await llm.ainvoke(prompt)

    return result.content.strip()
```

**AI ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ**:
```
ì‹œìŠ¤í…œì€ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì´ë‚˜ payment-serviceì—ì„œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. 
ì´ 45,234ê°œì˜ ë¡œê·¸ ì¤‘ ì—ëŸ¬ìœ¨ì€ 1.2%ë¡œ ì–‘í˜¸í•œ ìˆ˜ì¤€ì´ì§€ë§Œ, 
payment-serviceì—ì„œ DatabaseTimeout ì—ëŸ¬ê°€ 23ê±´ ë°œìƒí•˜ì—¬ íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ì§€ì—°ì´ ìš°ë ¤ë©ë‹ˆë‹¤. 
user-serviceì™€ auth-serviceëŠ” ì—ëŸ¬ ì—†ì´ ì •ìƒ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤. 
DB ì—°ê²° í’€ ëª¨ë‹ˆí„°ë§ê³¼ payment-service ë¡œê·¸ ìƒì„¸ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
```

#### **Step 2: í…œí”Œë¦¿ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„**

```python
def prepare_project_context(
    document_request: ProjectAnalysisDocumentRequest,
    ai_insights: str
) -> dict:
    """Jinja2 í…œí”Œë¦¿ì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""

    context = {
        # í”„ë¡œì íŠ¸ ì •ë³´
        "project_info": {
            "name": document_request.projectInfo.name,
            "description": document_request.projectInfo.description,
            "uuid": document_request.projectInfo.projectUuid,
            "created_at": document_request.projectInfo.createdAt
        },

        # ì‹œê°„ ë²”ìœ„
        "time_range": {
            "startTime": document_request.timeRange.startTime,
            "endTime": document_request.timeRange.endTime,
            "duration_hours": calculate_duration_hours(
                document_request.timeRange.startTime,
                document_request.timeRange.endTime
            )
        },

        # ë©”íŠ¸ë¦­
        "metrics": {
            "totalLogs": document_request.metrics.totalLogs,
            "errorCount": document_request.metrics.errorCount,
            "warnCount": document_request.metrics.warnCount,
            "infoCount": document_request.metrics.infoCount,
            "errorRate": (document_request.metrics.errorCount / document_request.metrics.totalLogs * 100) if document_request.metrics.totalLogs > 0 else 0,
            "warnRate": (document_request.metrics.warnCount / document_request.metrics.totalLogs * 100) if document_request.metrics.totalLogs > 0 else 0
        },

        # ìƒìœ„ ì—ëŸ¬ ëª©ë¡
        "top_errors": [
            {
                "timestamp": error.timestamp,
                "logLevel": error.logLevel,
                "message": error.message[:100] + "..." if len(error.message) > 100 else error.message,
                "componentName": error.componentName,
                "serviceName": error.serviceName,
                "sourceType": error.sourceType
            }
            for error in document_request.topErrors
        ],

        # AI ì¸ì‚¬ì´íŠ¸
        "ai_insights": {
            "system_health_summary": ai_insights,
            "generated_at": datetime.utcnow().isoformat() + "Z"
        },

        # ìŠ¤íƒ€ì¼ ì„¤ì •
        "style": {
            "css_framework": "tailwind",
            "chart_library": "chartjs" if document_request.options.includeCharts else None,
            "color_scheme": "blue",
            "font_family": "Pretendard, -apple-system, sans-serif"
        },

        # ì˜µì…˜
        "options": {
            "includeCharts": document_request.options.includeCharts,
            "includeAiInsights": document_request.options.includeAiInsights
        }
    }

    return context
```

#### **Step 3: Jinja2 í…œí”Œë¦¿ ë Œë”ë§**

**í…œí”Œë¦¿ íŒŒì¼**: `app/templates/analysis/project_analysis.html`

```jinja2
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{{ project_info.name }} - í”„ë¡œì íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸</title>
    <script src="https://cdn.tailwindcss.com"></script>
    {% if options.includeCharts %}
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0"></script>
    {% endif %}
</head>
<body class="bg-gray-50 font-sans">
    <div class="container mx-auto px-6 py-8 max-w-7xl">
        <!-- Header -->
        <header class="mb-8">
            <h1 class="text-4xl font-bold text-gray-900">{{ project_info.name }}</h1>
            <p class="text-gray-600 mt-2">{{ project_info.description }}</p>
            <div class="text-sm text-gray-500 mt-4">
                <span>ğŸ“… ë¶„ì„ ê¸°ê°„: {{ time_range.startTime|format_date }} ~ {{ time_range.endTime|format_date }}</span>
                <span class="ml-4">â±ï¸ {{ time_range.duration_hours }}ì‹œê°„</span>
            </div>
        </header>

        <!-- Health Score Card -->
        <div class="bg-white rounded-xl shadow-lg p-8 mb-8">
            <h2 class="text-2xl font-semibold mb-4 flex items-center">
                <span class="mr-2">ğŸ¥</span> ê±´ê°• ì ìˆ˜
            </h2>
            <div class="flex items-center justify-between">
                <div>
                    <div class="text-6xl font-bold {{ 'text-green-500' if metadata.health_score >= 80 else ('text-yellow-500' if metadata.health_score >= 60 else 'text-red-500') }}">
                        {{ metadata.health_score }}
                    </div>
                    <div class="text-gray-600 mt-2">
                        {% if metadata.health_score >= 80 %}
                            ğŸŸ¢ ë§¤ìš° ì–‘í˜¸
                        {% elif metadata.health_score >= 60 %}
                            ğŸŸ¡ ì£¼ì˜ í•„ìš”
                        {% else %}
                            ğŸ”´ ìœ„í—˜
                        {% endif %}
                    </div>
                </div>

                <!-- AI Insights -->
                {% if options.includeAiInsights and ai_insights %}
                <div class="flex-1 ml-8 p-6 bg-blue-50 rounded-lg border-l-4 border-blue-500">
                    <h3 class="font-semibold text-blue-900 flex items-center mb-2">
                        <span class="mr-2">ğŸ¤–</span> AI ë¶„ì„
                    </h3>
                    <p class="text-blue-800 leading-relaxed">{{ ai_insights.system_health_summary }}</p>
                </div>
                {% endif %}
            </div>
        </div>

        <!-- Metrics Grid -->
        <div class="grid grid-cols-4 gap-6 mb-8">
            <!-- Total Logs -->
            <div class="bg-white rounded-lg shadow p-6">
                <div class="text-sm text-gray-600 mb-1">ì´ ë¡œê·¸</div>
                <div class="text-3xl font-bold text-gray-900">{{ metrics.totalLogs|format_number }}</div>
                <div class="text-xs text-gray-500 mt-1">ì „ì²´ ê¸°ë¡</div>
            </div>

            <!-- Error Count -->
            <div class="bg-white rounded-lg shadow p-6">
                <div class="text-sm text-gray-600 mb-1">ERROR</div>
                <div class="text-3xl font-bold text-red-500">{{ metrics.errorCount|format_number }}</div>
                <div class="text-xs text-gray-500 mt-1">{{ metrics.errorRate|format_percentage }}%</div>
            </div>

            <!-- Warn Count -->
            <div class="bg-white rounded-lg shadow p-6">
                <div class="text-sm text-gray-600 mb-1">WARN</div>
                <div class="text-3xl font-bold text-yellow-500">{{ metrics.warnCount|format_number }}</div>
                <div class="text-xs text-gray-500 mt-1">{{ metrics.warnRate|format_percentage }}%</div>
            </div>

            <!-- Info Count -->
            <div class="bg-white rounded-lg shadow p-6">
                <div class="text-sm text-gray-600 mb-1">INFO</div>
                <div class="text-3xl font-bold text-blue-500">{{ metrics.infoCount|format_number }}</div>
                <div class="text-xs text-gray-500 mt-1">ì •ìƒ ë¡œê·¸</div>
            </div>
        </div>

        <!-- Top Errors Table -->
        <div class="bg-white rounded-xl shadow-lg p-8 mb-8">
            <h2 class="text-2xl font-semibold mb-4 flex items-center">
                <span class="mr-2">ğŸš¨</span> ì£¼ìš” ì—ëŸ¬ ëª©ë¡
            </h2>

            {% if top_errors|length > 0 %}
            <div class="overflow-x-auto">
                <table class="w-full">
                    <thead>
                        <tr class="bg-gray-100 border-b">
                            <th class="px-4 py-3 text-left text-sm font-semibold text-gray-700">ì‹œê°„</th>
                            <th class="px-4 py-3 text-left text-sm font-semibold text-gray-700">ë ˆë²¨</th>
                            <th class="px-4 py-3 text-left text-sm font-semibold text-gray-700">ë©”ì‹œì§€</th>
                            <th class="px-4 py-3 text-left text-sm font-semibold text-gray-700">ì»´í¬ë„ŒíŠ¸</th>
                            <th class="px-4 py-3 text-left text-sm font-semibold text-gray-700">ì†ŒìŠ¤</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for error in top_errors %}
                        <tr class="border-b hover:bg-gray-50 transition">
                            <td class="px-4 py-3 text-sm text-gray-600">{{ error.timestamp|format_time }}</td>
                            <td class="px-4 py-3">
                                <span class="px-2 py-1 text-xs font-semibold rounded {{ 'bg-red-100 text-red-800' if error.logLevel == 'ERROR' else 'bg-yellow-100 text-yellow-800' }}">
                                    {{ error.logLevel }}
                                </span>
                            </td>
                            <td class="px-4 py-3 text-sm text-gray-900 font-mono">{{ error.message }}</td>
                            <td class="px-4 py-3 text-sm text-gray-700">{{ error.componentName or error.serviceName }}</td>
                            <td class="px-4 py-3 text-sm text-gray-600">{{ error.sourceType }}</td>
                        </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
            {% else %}
            <div class="text-center py-8 text-gray-500">
                âœ… ë¶„ì„ ê¸°ê°„ ë™ì•ˆ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
            </div>
            {% endif %}
        </div>

        <!-- Charts -->
        {% if options.includeCharts %}
        <div class="grid grid-cols-2 gap-6 mb-8">
            <!-- Error Trend Chart -->
            <div class="bg-white rounded-xl shadow-lg p-8">
                <h3 class="text-xl font-semibold mb-4">ì—ëŸ¬ ë°œìƒ ì¶”ì´</h3>
                <canvas id="errorTrendChart"></canvas>
            </div>

            <!-- Level Distribution Chart -->
            <div class="bg-white rounded-xl shadow-lg p-8">
                <h3 class="text-xl font-semibold mb-4">ë¡œê·¸ ë ˆë²¨ ë¶„í¬</h3>
                <canvas id="levelDistChart"></canvas>
            </div>
        </div>

        <script>
            // Error Trend Chart (Line)
            new Chart(document.getElementById('errorTrendChart'), {
                type: 'line',
                data: {
                    labels: {{ hourly_labels|tojson }},
                    datasets: [{
                        label: 'ERROR',
                        data: {{ error_counts|tojson }},
                        backgroundColor: 'rgba(239, 68, 68, 0.1)',
                        borderColor: 'rgb(239, 68, 68)',
                        borderWidth: 2,
                        tension: 0.4,
                        fill: true
                    }, {
                        label: 'WARN',
                        data: {{ warn_counts|tojson }},
                        backgroundColor: 'rgba(251, 191, 36, 0.1)',
                        borderColor: 'rgb(251, 191, 36)',
                        borderWidth: 2,
                        tension: 0.4,
                        fill: true
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: true,
                    plugins: {
                        legend: { position: 'top' },
                        tooltip: { mode: 'index', intersect: false }
                    },
                    scales: {
                        y: { beginAtZero: true, title: { display: true, text: 'ê°œìˆ˜' } },
                        x: { title: { display: true, text: 'ì‹œê°„' } }
                    }
                }
            });

            // Level Distribution Chart (Doughnut)
            new Chart(document.getElementById('levelDistChart'), {
                type: 'doughnut',
                data: {
                    labels: ['ERROR', 'WARN', 'INFO'],
                    datasets: [{
                        data: [{{ metrics.errorCount }}, {{ metrics.warnCount }}, {{ metrics.infoCount }}],
                        backgroundColor: [
                            'rgb(239, 68, 68)',
                            'rgb(251, 191, 36)',
                            'rgb(59, 130, 246)'
                        ],
                        borderWidth: 2
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: true,
                    plugins: {
                        legend: { position: 'bottom' },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    const label = context.label || '';
                                    const value = context.parsed || 0;
                                    const total = context.dataset.data.reduce((a, b) => a + b, 0);
                                    const percentage = ((value / total) * 100).toFixed(1);
                                    return `${label}: ${value.toLocaleString()} (${percentage}%)`;
                                }
                            }
                        }
                    }
                }
            });
        </script>
        {% endif %}

        <!-- Footer -->
        <footer class="mt-12 pt-6 border-t text-center text-gray-500 text-sm">
            <p>ğŸ¤– LogLens AI Service - ìë™ ìƒì„±ëœ ë¶„ì„ ë¦¬í¬íŠ¸</p>
            <p class="mt-1">ìƒì„± ì‹œê°: {{ ai_insights.generated_at|format_datetime }}</p>
        </footer>
    </div>
</body>
</html>
```

**Jinja2 ì»¤ìŠ¤í…€ í•„í„°**:

```python
# app/services/html_document_service.py:50-65

env = jinja2.Environment(loader=jinja2.FileSystemLoader("app/templates"))

# ìˆ«ì í¬ë§·íŒ…
env.filters["format_number"] = lambda v: f"{v:,}"

# í¼ì„¼í‹°ì§€
env.filters["format_percentage"] = lambda v: f"{v:.1f}"

# ë‚ ì§œ í¬ë§·
env.filters["format_date"] = lambda v: datetime.fromisoformat(v.replace("Z", "+00:00")).strftime("%Y-%m-%d")

# ì‹œê°„ í¬ë§·
env.filters["format_time"] = lambda v: datetime.fromisoformat(v.replace("Z", "+00:00")).strftime("%H:%M:%S")

# ë‚ ì§œì‹œê°„ í¬ë§·
env.filters["format_datetime"] = lambda v: datetime.fromisoformat(v.replace("Z", "+00:00")).strftime("%Y-%m-%d %H:%M:%S")

# ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
env.filters["markdown"] = lambda text: Markup(markdown.markdown(text, extensions=['fenced_code', 'tables']))
```

#### **Step 4: ë©”íƒ€ë°ì´í„° ê³„ì‚°**

```python
def calculate_metadata(html_content: str, metrics: dict) -> dict:
    """HTML ë©”íƒ€ë°ì´í„° ê³„ì‚°"""

    # ë‹¨ì–´ ìˆ˜
    text_only = re.sub(r'<[^>]+>', '', html_content)  # HTML íƒœê·¸ ì œê±°
    word_count = len(text_only.split())

    # ì½ê¸° ì‹œê°„ (ë¶„) - í•œêµ­ì–´ ê¸°ì¤€ ë¶„ë‹¹ 200ë‹¨ì–´
    reading_time_minutes = max(1, word_count // 200)

    # ê±´ê°• ì ìˆ˜ ê³„ì‚°
    total_logs = metrics['totalLogs']
    error_count = metrics['errorCount']
    warn_count = metrics['warnCount']

    if total_logs == 0:
        health_score = 100
    else:
        error_rate = (error_count / total_logs) * 100
        warn_rate = (warn_count / total_logs) * 100

        # ì ìˆ˜ = 100 - (ì—ëŸ¬ìœ¨ * 10) - (ê²½ê³ ìœ¨ * 3)
        health_score = max(0, min(100, 100 - (error_rate * 10) - (warn_rate * 3)))

    return {
        "word_count": word_count,
        "reading_time": f"{reading_time_minutes} minutes",
        "health_score": round(health_score),
        "charts_included": ["error_trend", "level_distribution"] if "chart.js" in html_content.lower() else [],
        "generated_at": datetime.utcnow().isoformat() + "Z"
    }
```

#### **Step 5: HTML ê²€ì¦**

```python
def _validate_html(html_content: str, document_type: str) -> dict:
    """HTML ìœ íš¨ì„± ê²€ì¦"""

    warnings = []

    # 1. í•„ìˆ˜ HTML íƒœê·¸ í™•ì¸
    is_valid_html = all([
        "<html" in html_content,
        "<head" in html_content,
        "<body" in html_content,
        "</html>" in html_content
    ])

    if not is_valid_html:
        warnings.append("Missing required HTML structure tags")

    # 2. ë¬¸ì„œ íƒ€ì…ë³„ í•„ìˆ˜ ì„¹ì…˜ í™•ì¸
    has_required_sections = True

    if document_type == "PROJECT_ANALYSIS":
        required_sections = ["ê±´ê°• ì ìˆ˜", "ì£¼ìš” ì—ëŸ¬", "metrics"]
        for section in required_sections:
            if section.lower() not in html_content.lower():
                has_required_sections = False
                warnings.append(f"Missing required section: {section}")

    elif document_type == "ERROR_ANALYSIS":
        required_sections = ["stack", "trace", "ê´€ë ¨ ë¡œê·¸"]
        for section in required_sections:
            if section.lower() not in html_content.lower():
                has_required_sections = False
                warnings.append(f"Missing required section: {section}")

    # 3. XSS ìœ„í—˜ ê²€ì‚¬ (ê¸°ë³¸)
    dangerous_patterns = ["<script>alert", "javascript:", "onerror=", "onclick="]
    for pattern in dangerous_patterns:
        if pattern in html_content:
            warnings.append(f"Potential XSS risk: {pattern}")

    return {
        "is_valid_html": is_valid_html,
        "has_required_sections": has_required_sections,
        "warnings": warnings,
        "validation_passed": is_valid_html and has_required_sections and len(warnings) == 0
    }
```

---

### 3.3.3 ì—ëŸ¬ ë¶„ì„ ë¬¸ì„œ ìƒì„±

**íŒŒì¼**: `app/services/html_document_service.py:180-250`

**ì°¨ì´ì **:
- ë‹¨ì¼ ë¡œê·¸ ì¤‘ì‹¬ (log_id ê¸°ë°˜)
- ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ê°•ì¡° í‘œì‹œ
- ê´€ë ¨ ë¡œê·¸ (trace siblings) í‘œì‹œ
- AI ë¶„ì„ì€ ê¸°ì¡´ `ai_analysis` í•„ë“œ ì¬ì‚¬ìš© (ì‹ ê·œ ìƒì„± X)

**í…œí”Œë¦¿**: `app/templates/analysis/error_analysis.html`

```jinja2
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>ì—ëŸ¬ ë¶„ì„ ë¦¬í¬íŠ¸ - {{ error_log.logId }}</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
</head>
<body class="bg-gray-50">
    <div class="container mx-auto px-6 py-8 max-w-6xl">
        <!-- Header -->
        <header class="mb-8">
            <h1 class="text-4xl font-bold text-red-600">ğŸš¨ ì—ëŸ¬ ë¶„ì„ ë¦¬í¬íŠ¸</h1>
            <div class="mt-4 flex items-center gap-4 text-sm">
                <span class="px-3 py-1 bg-red-100 text-red-800 rounded-full font-semibold">
                    {{ error_log.logLevel }}
                </span>
                <span class="text-gray-600">ğŸ“… {{ error_log.timestamp|format_datetime }}</span>
                <span class="text-gray-600">ğŸ”¢ Log ID: {{ error_log.logId }}</span>
            </div>
        </header>

        <!-- Error Message -->
        <div class="bg-red-50 border-l-4 border-red-500 p-6 mb-8 rounded-r-lg">
            <h2 class="text-xl font-semibold text-red-900 mb-2">ì—ëŸ¬ ë©”ì‹œì§€</h2>
            <pre class="text-red-800 font-mono text-sm whitespace-pre-wrap">{{ error_log.message }}</pre>
        </div>

        <!-- Service Info -->
        <div class="grid grid-cols-3 gap-4 mb-8">
            <div class="bg-white rounded-lg shadow p-4">
                <div class="text-sm text-gray-600">ì„œë¹„ìŠ¤</div>
                <div class="text-lg font-semibold">{{ error_log.serviceName }}</div>
            </div>
            <div class="bg-white rounded-lg shadow p-4">
                <div class="text-sm text-gray-600">ì»´í¬ë„ŒíŠ¸</div>
                <div class="text-lg font-semibold">{{ error_log.componentName }}</div>
            </div>
            <div class="bg-white rounded-lg shadow p-4">
                <div class="text-sm text-gray-600">ì†ŒìŠ¤ íƒ€ì…</div>
                <div class="text-lg font-semibold">{{ error_log.sourceType }}</div>
            </div>
        </div>

        <!-- AI Analysis -->
        {% if error_log.aiAnalysis %}
        <div class="bg-blue-50 border-l-4 border-blue-500 p-6 mb-8 rounded-r-lg">
            <h2 class="text-xl font-semibold text-blue-900 mb-4 flex items-center">
                <span class="mr-2">ğŸ¤–</span> AI ë¶„ì„ ê²°ê³¼
            </h2>

            <!-- Summary -->
            <div class="mb-4">
                <h3 class="font-semibold text-blue-800 mb-2">ìš”ì•½</h3>
                <p class="text-blue-900">{{ error_log.aiAnalysis.summary|markdown }}</p>
            </div>

            <!-- Error Cause -->
            <div class="mb-4">
                <h3 class="font-semibold text-blue-800 mb-2">ì›ì¸ ë¶„ì„</h3>
                <p class="text-blue-900">{{ error_log.aiAnalysis.error_cause }}</p>
            </div>

            <!-- Solution -->
            <div class="mb-4">
                <h3 class="font-semibold text-blue-800 mb-2">í•´ê²° ë°©ì•ˆ</h3>
                <div class="text-blue-900 prose prose-sm max-w-none">
                    {{ error_log.aiAnalysis.solution|markdown }}
                </div>
            </div>

            <!-- Tags -->
            <div>
                <h3 class="font-semibold text-blue-800 mb-2">íƒœê·¸</h3>
                <div class="flex flex-wrap gap-2">
                    {% for tag in error_log.aiAnalysis.tags %}
                    <span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">{{ tag }}</span>
                    {% endfor %}
                </div>
            </div>
        </div>
        {% endif %}

        <!-- Stack Trace -->
        {% if error_log.stackTrace %}
        <div class="bg-white rounded-xl shadow-lg p-8 mb-8">
            <h2 class="text-2xl font-semibold mb-4 flex items-center">
                <span class="mr-2">ğŸ“œ</span> ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
            </h2>
            <pre class="bg-gray-900 text-gray-100 p-6 rounded-lg overflow-x-auto"><code class="language-java">{{ error_log.stackTrace }}</code></pre>
        </div>
        <script>hljs.highlightAll();</script>
        {% endif %}

        <!-- Related Logs -->
        {% if related_logs|length > 0 %}
        <div class="bg-white rounded-xl shadow-lg p-8 mb-8">
            <h2 class="text-2xl font-semibold mb-4 flex items-center">
                <span class="mr-2">ğŸ”—</span> ê´€ë ¨ ë¡œê·¸ (Trace ID: {{ error_log.traceId }})
            </h2>

            <div class="space-y-3">
                {% for log in related_logs %}
                <div class="border-l-4 {{ 'border-red-500 bg-red-50' if log.logLevel == 'ERROR' else ('border-yellow-500 bg-yellow-50' if log.logLevel == 'WARN' else 'border-blue-500 bg-blue-50') }} p-4 rounded-r">
                    <div class="flex items-center gap-3 mb-2">
                        <span class="text-xs text-gray-600">{{ log.timestamp|format_time }}</span>
                        <span class="px-2 py-0.5 text-xs font-semibold rounded {{ 'bg-red-200 text-red-900' if log.logLevel == 'ERROR' else ('bg-yellow-200 text-yellow-900' if log.logLevel == 'WARN' else 'bg-blue-200 text-blue-900') }}">
                            {{ log.logLevel }}
                        </span>
                        <span class="text-xs text-gray-700">{{ log.componentName }}</span>
                    </div>
                    <div class="text-sm font-mono">{{ log.message }}</div>
                </div>
                {% endfor %}
            </div>
        </div>
        {% endif %}

        <!-- Impact Analysis -->
        <div class="bg-white rounded-xl shadow-lg p-8 mb-8">
            <h2 class="text-2xl font-semibold mb-4 flex items-center">
                <span class="mr-2">ğŸ“Š</span> ì˜í–¥ ë¶„ì„
            </h2>

            <div class="grid grid-cols-2 gap-6">
                <div>
                    <h3 class="font-semibold text-gray-700 mb-3">ë°œìƒ ë¹ˆë„</h3>
                    <div class="text-3xl font-bold text-gray-900">{{ impact.occurrence_count }}íšŒ</div>
                    <div class="text-sm text-gray-600 mt-1">ìµœê·¼ 24ì‹œê°„ ê¸°ì¤€</div>
                </div>

                <div>
                    <h3 class="font-semibold text-gray-700 mb-3">ì˜í–¥ë°›ì€ ì‚¬ìš©ì</h3>
                    <div class="text-3xl font-bold text-gray-900">{{ impact.affected_users }}ëª…</div>
                    <div class="text-sm text-gray-600 mt-1">ê³ ìœ  IP ê¸°ì¤€</div>
                </div>
            </div>
        </div>

        <!-- Footer -->
        <footer class="mt-12 pt-6 border-t text-center text-gray-500 text-sm">
            <p>ğŸ¤– LogLens AI Service - ìë™ ìƒì„±ëœ ì—ëŸ¬ ë¶„ì„ ë¦¬í¬íŠ¸</p>
            <p class="mt-1">ìƒì„± ì‹œê°: {{ generated_at|format_datetime }}</p>
        </footer>
    </div>
</body>
</html>
```

---

### 3.3.4 Chart.js í†µí•©

**ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ì¶”ì´ ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„**:

```python
def prepare_chart_data(project_uuid: str, time_range: dict) -> dict:
    """Chart.jsìš© ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìƒì„±"""

    # 1ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ì§‘ê³„
    query = {
        "size": 0,
        "query": {
            "bool": {
                "must": [
                    {"term": {"project_uuid.keyword": project_uuid}},
                    {"range": {"timestamp": {
                        "gte": time_range["startTime"],
                        "lte": time_range["endTime"]
                    }}}
                ]
            }
        },
        "aggs": {
            "by_hour": {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": "1h",
                    "time_zone": "Asia/Seoul"
                },
                "aggs": {
                    "by_level": {
                        "terms": {"field": "level", "size": 10}
                    }
                }
            }
        }
    }

    response = opensearch_client.search(
        index=f"{project_uuid.replace('-', '_')}_*",
        body=query
    )

    # ì°¨íŠ¸ ë°ì´í„° ë³€í™˜
    hourly_labels = []
    error_counts = []
    warn_counts = []

    for bucket in response["aggregations"]["by_hour"]["buckets"]:
        # ì‹œê°„ ë ˆì´ë¸”
        timestamp = datetime.fromtimestamp(bucket["key"] / 1000)
        hourly_labels.append(timestamp.strftime("%m/%d %H:00"))

        # ë ˆë²¨ë³„ ê°œìˆ˜
        level_stats = {b["key"]: b["doc_count"] for b in bucket["by_level"]["buckets"]}
        error_counts.append(level_stats.get("ERROR", 0))
        warn_counts.append(level_stats.get("WARN", 0))

    return {
        "hourly_labels": hourly_labels,
        "error_counts": error_counts,
        "warn_counts": warn_counts
    }
```

---

## 3.4 AI vs DB í†µê³„ ë¹„êµ

**íŒŒì¼**: `app/tools/statistics_comparison_tools.py`

### 3.4.1 ë¹„êµ ëª©ì  ë° ì˜ì˜

**ëª©í‘œ**: LLMì´ ìƒ˜í”Œ ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²´ í†µê³„ë¥¼ ì •í™•íˆ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦

**í™œìš© ì‚¬ë¡€**:
- âœ… **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**: DB ë¶€í•˜ ì—†ì´ LLMìœ¼ë¡œ ì¶”ì •ì¹˜ ì œê³µ
- âœ… **ë¹„ìš© ì ˆê°**: ë³µì¡í•œ ì§‘ê³„ ì¿¼ë¦¬ ëŒ€ì‹  ìƒ˜í”Œë§ + LLM ì¶”ë¡ 
- âœ… **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œë„ ì¼ì •í•œ ì„±ëŠ¥

**ë¹„êµ ë©”íŠ¸ë¦­**:
1. ì´ ë¡œê·¸ ê°œìˆ˜
2. ERROR ê°œìˆ˜
3. WARN ê°œìˆ˜
4. INFO ê°œìˆ˜
5. ì—ëŸ¬ìœ¨ (%)
6. í”¼í¬ ì‹œê°„ëŒ€

### 3.4.2 ì¸µí™” ìƒ˜í”Œë§ (Stratified Sampling)

**ëª©ì **: í¬ì†Œ ì´ë²¤íŠ¸(ERROR)ë¥¼ ì¶©ë¶„íˆ í¬ì°©

**íŒŒì¼**: `app/tools/statistics_comparison_tools.py:151-210`

```python
async def _get_stratified_log_samples(
    project_uuid: str,
    time_range: dict,
    sample_size: int = 100
) -> dict:
    """ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ ë¡œê·¸ ìˆ˜ì§‘"""

    # 1. ì „ì²´ í†µê³„ ì¡°íšŒ (ë ˆë²¨ë³„ ë¹„ìœ¨ íŒŒì•…)
    stats_query = {
        "size": 0,
        "query": {
            "range": {"timestamp": {
                "gte": time_range["start"],
                "lte": time_range["end"]
            }}
        },
        "aggs": {
            "by_level": {
                "terms": {"field": "level", "size": 10}
            }
        }
    }

    stats_response = opensearch_client.search(
        index=f"{project_uuid.replace('-', '_')}_*",
        body=stats_query
    )

    # ë ˆë²¨ë³„ ë¹„ìœ¨ ê³„ì‚°
    total_count = stats_response["hits"]["total"]["value"]
    level_counts = {
        bucket["key"]: bucket["doc_count"]
        for bucket in stats_response["aggregations"]["by_level"]["buckets"]
    }

    error_count = level_counts.get("ERROR", 0)
    warn_count = level_counts.get("WARN", 0)
    info_count = level_counts.get("INFO", 0)

    error_ratio = error_count / total_count if total_count > 0 else 0
    warn_ratio = warn_count / total_count if total_count > 0 else 0

    # 2. ì¸µí™” ìƒ˜í”Œ í¬ê¸° ê²°ì •
    # ERROR/WARNì€ ìµœì†Œ 5ê°œì”© ë³´ì¥ (í¬ì†Œ ì´ë²¤íŠ¸ ë³´í˜¸)
    level_sample_sizes = {
        "ERROR": max(5, int(sample_size * error_ratio)),
        "WARN": max(5, int(sample_size * warn_ratio)),
        "INFO": sample_size - max(5, int(sample_size * error_ratio)) - max(5, int(sample_size * warn_ratio))
    }

    # 3. ë ˆë²¨ë³„ ëœë¤ ìƒ˜í”Œë§
    samples = []

    for level, count in level_sample_sizes.items():
        sample_query = {
            "size": count,
            "query": {
                "function_score": {
                    "query": {
                        "bool": {
                            "must": [
                                {"range": {"timestamp": {
                                    "gte": time_range["start"],
                                    "lte": time_range["end"]
                                }}},
                                {"term": {"level": level}}
                            ]
                        }
                    },
                    "random_score": {},  # ëœë¤ ì •ë ¬
                    "boost_mode": "replace"
                }
            }
        }

        sample_response = opensearch_client.search(
            index=f"{project_uuid.replace('-', '_')}_*",
            body=sample_query
        )

        for hit in sample_response["hits"]["hits"]:
            source = hit["_source"]
            samples.append({
                "timestamp": source["timestamp"],
                "level": source["level"],
                "message": source["message"][:100],  # ë©”ì‹œì§€ 100ìë¡œ ì œí•œ
                "service_name": source.get("service_name", "Unknown")
            })

    return {
        "samples": samples,
        "sample_size": len(samples),
        "sample_distribution": {
            "ERROR": len([s for s in samples if s["level"] == "ERROR"]),
            "WARN": len([s for s in samples if s["level"] == "WARN"]),
            "INFO": len([s for s in samples if s["level"] == "INFO"])
        },
        "actual_distribution": {
            "ERROR": error_count,
            "WARN": warn_count,
            "INFO": info_count,
            "total": total_count
        }
    }
```

**ì¸µí™” ìƒ˜í”Œë§ íš¨ê³¼**:

| ë°©ì‹ | ERROR 10% (1000ê°œ ì¤‘ 100ê°œ) | ë‹¨ìˆœ ëœë¤ ìƒ˜í”Œë§ | ì¸µí™” ìƒ˜í”Œë§ |
|------|------------------------|--------------|-----------|
| **ìƒ˜í”Œ í¬ê¸°** | 100ê°œ | 100ê°œ | 100ê°œ |
| **ì˜ˆìƒ ERROR ê°œìˆ˜** | 10ê°œ | 8-12ê°œ (ë³€ë™ í¼) | ìµœì†Œ 5ê°œ ë³´ì¥ |
| **í¬ì†Œ ì´ë²¤íŠ¸ í¬ì°©** | âŒ ë¶ˆì•ˆì • | âš ï¸ ìš´ì— ì˜ì¡´ | âœ… ì•ˆì •ì  |
| **ì¶”ë¡  ì •í™•ë„** | - | â­â­ ì¤‘ê°„ | â­â­â­â­ ë†’ìŒ |

### 3.4.3 LLM ì¶”ë¡ 

**íŒŒì¼**: `app/tools/statistics_comparison_tools.py:254-340`

```python
async def _llm_estimate_statistics(
    samples: list,
    sample_distribution: dict,
    actual_level_counts: dict = None  # ê²€ì¦ ëª¨ë“œ
) -> dict:
    """LLMìœ¼ë¡œ ì „ì²´ í†µê³„ ì¶”ë¡ """

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    if actual_level_counts:
        # ê²€ì¦ ëª¨ë“œ: ì‹¤ì œ ê°’ ì œê³µ í›„ ìƒ˜í”Œ í’ˆì§ˆ í‰ê°€
        total_count = sum(actual_level_counts.values())
        error_rate = (actual_level_counts["ERROR"] / total_count * 100) if total_count > 0 else 0

        prompt = f"""ì¸µí™” ìƒ˜í”Œë§ ê²€ì¦ ì‘ì—…

**ì‹¤ì œ DB í†µê³„ (Ground Truth)**:
- ì´ ë¡œê·¸: {total_count:,}ê°œ
- ERROR: {actual_level_counts["ERROR"]:,}ê°œ ({error_rate:.2f}%)
- WARN: {actual_level_counts["WARN"]:,}ê°œ
- INFO: {actual_level_counts["INFO"]:,}ê°œ

**ì¸µí™” ìƒ˜í”Œ ë°ì´í„°**:
- ìƒ˜í”Œ í¬ê¸°: {len(samples)}ê°œ
- ERROR ìƒ˜í”Œ: {sample_distribution["ERROR"]}ê°œ
- WARN ìƒ˜í”Œ: {sample_distribution["WARN"]}ê°œ
- INFO ìƒ˜í”Œ: {sample_distribution["INFO"]}ê°œ

**ìƒ˜í”Œ ë¡œê·¸ (ì²˜ìŒ 10ê°œ)**:
{chr(10).join(f"{i+1}. [{s['level']}] {s['message']}" for i, s in enumerate(samples[:10]))}

**ì‘ì—…**:
1. ìƒ˜í”Œì´ ì „ì²´ ë¶„í¬ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ”ì§€ í‰ê°€
2. ERROR/WARN í¬ì†Œ ì´ë²¤íŠ¸ê°€ ì¶©ë¶„íˆ í¬ì°©ë˜ì—ˆëŠ”ì§€ í™•ì¸
3. ìƒ˜í”Œ í’ˆì§ˆì— ëŒ€í•œ ì‹ ë¢°ë„ ì ìˆ˜ (0-100) ì‚°ì¶œ

**ì‘ë‹µ í˜•ì‹ (JSON)**:
{{
    "estimated_total_logs": {total_count},
    "estimated_error_count": {actual_level_counts["ERROR"]},
    "estimated_warn_count": {actual_level_counts["WARN"]},
    "estimated_info_count": {actual_level_counts["INFO"]},
    "estimated_error_rate": {error_rate},
    "confidence_score": <0-100>,
    "reasoning": "<1-2ë¬¸ì¥ìœ¼ë¡œ ìƒ˜í”Œ í’ˆì§ˆ í‰ê°€>"
}}
"""

    else:
        # ì¶”ë¡  ëª¨ë“œ: ìƒ˜í”Œë§Œìœ¼ë¡œ ì „ì²´ í†µê³„ ì¶”ì •
        prompt = f"""ì¸µí™” ìƒ˜í”Œ ë°ì´í„°ë¡œ ì „ì²´ í†µê³„ ì¶”ì •

**ì¸µí™” ìƒ˜í”Œ ë°ì´í„°**:
- ìƒ˜í”Œ í¬ê¸°: {len(samples)}ê°œ
- ERROR ìƒ˜í”Œ: {sample_distribution["ERROR"]}ê°œ
- WARN ìƒ˜í”Œ: {sample_distribution["WARN"]}ê°œ
- INFO ìƒ˜í”Œ: {sample_distribution["INFO"]}ê°œ

**ìƒ˜í”Œ ë¡œê·¸**:
{chr(10).join(f"{i+1}. [{s['level']}] {s['timestamp']} | {s['message']}" for i, s in enumerate(samples))}

**ì‘ì—…**:
ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì „ì²´ ëª¨ì§‘ë‹¨ì˜ í†µê³„ë¥¼ ì¶”ì •í•˜ì„¸ìš”.
ê³ ë ¤ì‚¬í•­:
- ì¸µí™” ìƒ˜í”Œë§ì´ ì ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¨ìˆœ ë¹„ìœ¨ í™•ëŒ€ëŠ” ë¶€ì •í™•
- ERROR/WARNì€ ìµœì†Œ 5ê°œ ë³´ì¥ë˜ì–´ ê³¼ëŒ€í‘œí˜„ë  ìˆ˜ ìˆìŒ
- INFOì˜ ì‹¤ì œ ë¹„ìœ¨ì´ ë” ë†’ì„ ê°€ëŠ¥ì„±

**ì‘ë‹µ í˜•ì‹ (JSON)**:
{{
    "estimated_total_logs": <ì¶”ì •ê°’>,
    "estimated_error_count": <ì¶”ì •ê°’>,
    "estimated_warn_count": <ì¶”ì •ê°’>,
    "estimated_info_count": <ì¶”ì •ê°’>,
    "estimated_error_rate": <í¼ì„¼í‹°ì§€>,
    "confidence_score": <0-100>,
    "reasoning": "<ì¶”ì • ê·¼ê±° 1-2ë¬¸ì¥>"
}}
"""

    # LLM í˜¸ì¶œ
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,  # ì¼ê´€ì„± ì¤‘ìš”
        max_tokens=1000
    )

    response = await llm.ainvoke(prompt)

    # JSON íŒŒì‹±
    content = response.content.strip()
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0]

    result = json.loads(content)

    return result
```

### 3.4.4 ì •í™•ë„ ê³„ì‚°

**íŒŒì¼**: `app/tools/statistics_comparison_tools.py:450-499`

```python
def _calculate_accuracy(db_stats: dict, ai_stats: dict) -> dict:
    """AI ì¶”ë¡  ì •í™•ë„ ê³„ì‚°"""

    def accuracy_percentage(actual: float, predicted: float) -> float:
        """ê°œë³„ ë©”íŠ¸ë¦­ ì •í™•ë„ (0-100%)"""
        if actual == 0:
            return 100.0 if predicted == 0 else 0.0

        error = abs(actual - predicted)
        accuracy = (1 - error / actual) * 100
        return max(0, round(accuracy, 2))

    # 1. ê°œë³„ ë©”íŠ¸ë¦­ ì •í™•ë„
    total_accuracy = accuracy_percentage(
        db_stats["total_logs"],
        ai_stats["estimated_total_logs"]
    )

    error_count_accuracy = accuracy_percentage(
        db_stats["error_count"],
        ai_stats["estimated_error_count"]
    )

    warn_count_accuracy = accuracy_percentage(
        db_stats["warn_count"],
        ai_stats["estimated_warn_count"]
    )

    info_count_accuracy = accuracy_percentage(
        db_stats["info_count"],
        ai_stats["estimated_info_count"]
    )

    # 2. ì—ëŸ¬ìœ¨ ì •í™•ë„ (ì ˆëŒ€ ì°¨ì´)
    error_rate_diff = abs(db_stats["error_rate"] - ai_stats["estimated_error_rate"])
    # 1% ì°¨ì´ = -10ì 
    error_rate_accuracy = max(0, round(100 - error_rate_diff * 10, 2))

    # 3. ì¢…í•© ì •í™•ë„ (ê°€ì¤‘ í‰ê· )
    overall_accuracy = round(
        total_accuracy * 0.3 +
        error_count_accuracy * 0.3 +
        error_rate_accuracy * 0.2 +
        warn_count_accuracy * 0.1 +
        info_count_accuracy * 0.1,
        2
    )

    return {
        "overall_accuracy": overall_accuracy,
        "total_logs_accuracy": total_accuracy,
        "error_count_accuracy": error_count_accuracy,
        "warn_count_accuracy": warn_count_accuracy,
        "info_count_accuracy": info_count_accuracy,
        "error_rate_accuracy": error_rate_accuracy
    }
```

### 3.4.5 ì •í™•ë„ ë“±ê¸‰ ë° í•´ì„

```python
def _grade_accuracy(overall_accuracy: float) -> dict:
    """ì •í™•ë„ ë“±ê¸‰í™”"""

    if overall_accuracy >= 95:
        grade = "ë§¤ìš° ìš°ìˆ˜"
        can_replace_db = True
        explanation = "ì˜¤ì°¨ìœ¨ 5% ë¯¸ë§Œìœ¼ë¡œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ DB ì™„ì „ ëŒ€ì²´ ê°€ëŠ¥"
        recommendations = [
            "âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ì ìš© ê¶Œì¥",
            "âœ… ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ AI ê¸°ë°˜ ë¶„ì„ ë„ì…",
            "âœ… DB ë¶€í•˜ ê°ì†Œë¥¼ ìœ„í•œ AI ìºì‹± ë ˆì´ì–´ êµ¬ì¶•",
            "âœ… ë³µì¡í•œ ì§‘ê³„ ì¿¼ë¦¬ë¥¼ LLM ì¶”ë¡ ìœ¼ë¡œ ëŒ€ì²´"
        ]

    elif overall_accuracy >= 90:
        grade = "ìš°ìˆ˜"
        can_replace_db = True
        explanation = "ì˜¤ì°¨ìœ¨ 10% ë¯¸ë§Œìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ì—…ë¬´ì— í™œìš© ê°€ëŠ¥"
        recommendations = [
            "âœ… ë¹„ì¤‘ìš” ì—…ë¬´ë¶€í„° ë‹¨ê³„ì  ì ìš©",
            "âš ï¸ ì¤‘ìš” ì˜ì‚¬ê²°ì •ì€ DB ê²€ì¦ ë³‘í–‰",
            "âœ… ìƒ˜í”Œ í¬ê¸° ì¦ê°€ (100 â†’ 200) ê³ ë ¤"
        ]

    elif overall_accuracy >= 80:
        grade = "ì–‘í˜¸"
        can_replace_db = False
        explanation = "ì˜¤ì°¨ìœ¨ 20% ìˆ˜ì¤€ìœ¼ë¡œ ë³´ì¡° ë„êµ¬ë¡œ ìœ ìš©"
        recommendations = [
            "âš ï¸ DB ëŒ€ì²´ëŠ” ê¶Œì¥í•˜ì§€ ì•ŠìŒ",
            "âœ… ë¹ ë¥¸ ì¶”ì •ì¹˜ ì œê³µ ìš©ë„ë¡œ í™œìš©",
            "ğŸ”§ ìƒ˜í”Œë§ ì „ëµ ê°œì„  í•„ìš”",
            "ğŸ”§ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ìµœì í™”"
        ]

    else:
        grade = "ê°œì„  í•„ìš”"
        can_replace_db = False
        explanation = "ì˜¤ì°¨ìœ¨ 20% ì´ˆê³¼ë¡œ ì¶”ê°€ ìµœì í™” í•„ìš”"
        recommendations = [
            "âŒ í˜„ì¬ ìƒíƒœë¡œëŠ” ì‚¬ìš© ë¶€ì í•©",
            "ğŸ”§ ì¸µí™” ìƒ˜í”Œë§ ì „ëµ ì „ë©´ ì¬ê²€í† ",
            "ğŸ”§ LLM ëª¨ë¸ ë³€ê²½ ê³ ë ¤ (GPT-4)",
            "ğŸ”§ ìƒ˜í”Œ í¬ê¸° ëŒ€í­ ì¦ê°€ (100 â†’ 500)"
        ]

    return {
        "grade": grade,
        "can_replace_db": can_replace_db,
        "explanation": explanation,
        "recommendations": recommendations
    }
```

### 3.4.6 ì „ì²´ í”Œë¡œìš°

```mermaid
graph TB
    Start([GET /api/v2-langgraph/statistics/compare]) --> DB[1. DB í†µê³„ ì¡°íšŒ<br/>OpenSearch aggregation]

    DB --> Sample[2. ì¸µí™” ìƒ˜í”Œë§<br/>ERROR: ìµœì†Œ 5ê°œ<br/>WARN: ìµœì†Œ 5ê°œ<br/>INFO: ë‚˜ë¨¸ì§€]

    Sample --> LLM[3. LLM ì¶”ë¡ <br/>GPT-4o mini<br/>Temperature 0.1]

    LLM --> CalcAcc[4. ì •í™•ë„ ê³„ì‚°<br/>ê°œë³„ ë©”íŠ¸ë¦­ ì •í™•ë„<br/>+ ê°€ì¤‘ í‰ê· ]

    CalcAcc --> Grade{ì •í™•ë„ ë“±ê¸‰?}

    Grade -->|â‰¥95%| VeryGood[ë§¤ìš° ìš°ìˆ˜<br/>DB ì™„ì „ ëŒ€ì²´ ê°€ëŠ¥]
    Grade -->|â‰¥90%| Good[ìš°ìˆ˜<br/>ëŒ€ë¶€ë¶„ ì—…ë¬´ í™œìš©]
    Grade -->|â‰¥80%| Fair[ì–‘í˜¸<br/>ë³´ì¡° ë„êµ¬]
    Grade -->|<80%| Poor[ê°œì„  í•„ìš”<br/>ìµœì í™” í•„ìš”]

    VeryGood --> Return[ComparisonResponse ë°˜í™˜]
    Good --> Return
    Fair --> Return
    Poor --> Return

    style DB fill:#d1ecf1
    style LLM fill:#fff3cd
    style CalcAcc fill:#d4edda
    style VeryGood fill:#d4edda
    style Good fill:#d4edda
    style Fair fill:#fff3cd
    style Poor fill:#f8d7da
```

---

(ë¬¸ì„œê°€ ê³„ì†ë©ë‹ˆë‹¤...)

## 4. LangChain/LangGraph ì•„í‚¤í…ì²˜

### 4.1 StateGraph êµ¬ì¡°

**LangGraph**ëŠ” **ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì—”ì§„**ìœ¼ë¡œ, ë³µì¡í•œ AI íŒŒì´í”„ë¼ì¸ì„ ê·¸ë˜í”„ í˜•íƒœë¡œ ì •ì˜í•©ë‹ˆë‹¤.

#### **í•µì‹¬ ê°œë…**

```mermaid
graph LR
    State[(State<br/>TypedDict)] --> Node1[Node 1<br/>í•¨ìˆ˜]
    Node1 --> State
    State --> Node2[Node 2<br/>í•¨ìˆ˜]
    Node2 --> State
    State --> Condition{Conditional<br/>Router}
    Condition -->|ê²½ë¡œA| Node3[Node 3]
    Condition -->|ê²½ë¡œB| Node4[Node 4]
    Node3 --> State
    Node4 --> State
    State --> End([END])

    style State fill:#ffeaa7
    style Condition fill:#fff3cd
```

**êµ¬ì„± ìš”ì†Œ**:
1. **State (ìƒíƒœ)**: TypedDictë¡œ ì •ì˜ëœ ê³µìœ  ë°ì´í„°
2. **Nodes (ë…¸ë“œ)**: ìƒíƒœë¥¼ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜
3. **Edges (ì—£ì§€)**: ë…¸ë“œ ê°„ ì—°ê²°
4. **Conditional Edges (ì¡°ê±´ë¶€ ì—£ì§€)**: ìƒíƒœì— ë”°ë¥¸ ë¶„ê¸°
5. **Entry Point**: ì‹œì‘ ë…¸ë“œ
6. **END**: ì¢…ë£Œ ì§€ì 

#### **LogAnalysisGraph ì •ì˜**

**íŒŒì¼**: `app/graphs/log_analysis_graph.py:67-144`

```python
from langgraph.graph import StateGraph, END
from app.graphs.state.log_analysis_state import LogAnalysisState

class LogAnalysisGraph:
    def __init__(self):
        # StateGraph ìƒì„±
        self.workflow = StateGraph(LogAnalysisState)

        # ë…¸ë“œ ì¶”ê°€
        self.workflow.add_node("fetch_log", self._fetch_log_node)
        self.workflow.add_node("check_direct_cache", self._check_direct_cache_node)
        self.workflow.add_node("check_trace_cache", self._check_trace_cache_node)
        self.workflow.add_node("check_similarity_cache", self._check_similarity_cache_node)
        self.workflow.add_node("collect_logs", self._collect_logs_node)
        self.workflow.add_node("decide_strategy", self._decide_strategy_node)
        self.workflow.add_node("analyze", self._analyze_node)
        self.workflow.add_node("validate", self._validate_node)
        self.workflow.add_node("save_result", self._save_result_node)

        # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ì„¤ì •
        self.workflow.set_entry_point("fetch_log")

        # ì„ í˜• ì—£ì§€ (í•­ìƒ ë‹¤ìŒ ë…¸ë“œë¡œ ì§„í–‰)
        self.workflow.add_edge("fetch_log", "check_direct_cache")
        self.workflow.add_edge("collect_logs", "decide_strategy")
        self.workflow.add_edge("decide_strategy", "analyze")
        self.workflow.add_edge("analyze", "validate")

        # ì¡°ê±´ë¶€ ì—£ì§€ (ìƒíƒœì— ë”°ë¼ ë¶„ê¸°)
        self.workflow.add_conditional_edges(
            "check_direct_cache",
            self._direct_cache_router,
            {
                "cache_hit": "save_result",    # ìºì‹œ íˆíŠ¸ â†’ ì¦‰ì‹œ ì €ì¥
                "cache_miss": "check_trace_cache"  # ìºì‹œ ë¯¸ìŠ¤ â†’ ë‹¤ìŒ ìºì‹œ í™•ì¸
            }
        )

        self.workflow.add_conditional_edges(
            "check_trace_cache",
            self._trace_cache_router,
            {
                "cache_hit": "save_result",
                "cache_miss": "check_similarity_cache"
            }
        )

        self.workflow.add_conditional_edges(
            "check_similarity_cache",
            self._similarity_cache_router,
            {
                "cache_hit": "save_result",
                "cache_miss": "collect_logs"  # ëª¨ë“  ìºì‹œ ë¯¸ìŠ¤ â†’ ì‹ ê·œ ë¶„ì„
            }
        )

        self.workflow.add_conditional_edges(
            "validate",
            self._validation_router,
            {
                "passed": "save_result",      # ê²€ì¦ í†µê³¼ â†’ ì €ì¥
                "failed": "analyze",          # ê²€ì¦ ì‹¤íŒ¨ â†’ ì¬ë¶„ì„
                "max_retries": "save_result"  # ìµœëŒ€ ì¬ì‹œë„ â†’ ê²½ê³ ì™€ í•¨ê»˜ ì €ì¥
            }
        )

        # ì¢…ë£Œ ì—£ì§€
        self.workflow.add_edge("save_result", END)

        # ì»´íŒŒì¼ (ì‹¤í–‰ ê°€ëŠ¥í•œ ê·¸ë˜í”„ë¡œ ë³€í™˜)
        self.compiled_graph = self.workflow.compile()

    async def execute(self, log_id: int, project_uuid: str) -> dict:
        """ê·¸ë˜í”„ ì‹¤í–‰"""
        initial_state = {
            "log_id": log_id,
            "project_uuid": project_uuid,
            "from_cache": False,
            "llm_call_count": 0,
            "korean_retry_count": 0,
            "validation_retry_count": 0,
            "max_korean_retries": 2,
            "max_validation_retries": 1,
            "started_at": datetime.utcnow().isoformat() + "Z"
        }

        result = await self.compiled_graph.ainvoke(initial_state)
        return result
```

### 4.2 ë…¸ë“œ ì „í™˜ ë¡œì§

#### **Router í•¨ìˆ˜ ì˜ˆì‹œ**

```python
def _validation_router(state: LogAnalysisState) -> str:
    """ê²€ì¦ ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""

    # ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
    if state.get("error"):
        return "max_retries"

    korean_valid = state.get("korean_valid", False)
    quality_score = state.get("quality_score", 0.0)
    korean_retry = state.get("korean_retry_count", 0)
    validation_retry = state.get("validation_retry_count", 0)

    # ì¬ì‹œë„ í•œë„ ì´ˆê³¼ í™•ì¸
    if (korean_retry >= state["max_korean_retries"] or
        validation_retry >= state["max_validation_retries"]):
        logger.warning(f"Max retries reached. Korean: {korean_retry}, Validation: {validation_retry}")
        return "max_retries"

    # ê²€ì¦ í†µê³¼ í™•ì¸
    quality_threshold = settings.VALIDATION_OVERALL_THRESHOLD  # 0.65
    if korean_valid and quality_score >= quality_threshold:
        logger.info(f"Validation passed. Quality score: {quality_score}")
        return "passed"

    # ê²€ì¦ ì‹¤íŒ¨ â†’ ì¬ë¶„ì„
    logger.info(f"Validation failed. Retrying analysis. Quality: {quality_score}")
    return "failed"
```

#### **ë…¸ë“œ ì‹¤í–‰ íë¦„ ì‹œê°í™”**

```mermaid
graph TB
    Start([Initial State]) --> FetchLog[fetch_log]

    FetchLog --> DirectCache[check_direct_cache]

    DirectCache --> DCRouter{direct_cache_router}
    DCRouter -->|cache_hit| SaveResult[save_result]
    DCRouter -->|cache_miss| TraceCache[check_trace_cache]

    TraceCache --> TCRouter{trace_cache_router}
    TCRouter -->|cache_hit| SaveResult
    TCRouter -->|cache_miss| SimilarityCache[check_similarity_cache]

    SimilarityCache --> SCRouter{similarity_cache_router}
    SCRouter -->|cache_hit| SaveResult
    SCRouter -->|cache_miss| CollectLogs[collect_logs]

    CollectLogs --> DecideStrategy[decide_strategy]
    DecideStrategy --> Analyze[analyze]

    Analyze --> Validate[validate]

    Validate --> VRouter{validation_router}
    VRouter -->|passed| SaveResult
    VRouter -->|failed| Analyze
    VRouter -->|max_retries| SaveResult

    SaveResult --> End([END])

    style Start fill:#d1ecf1
    style SaveResult fill:#d4edda
    style End fill:#ffeaa7
    style DCRouter fill:#fff3cd
    style TCRouter fill:#fff3cd
    style SCRouter fill:#fff3cd
    style VRouter fill:#fff3cd
```

### 4.3 ë„êµ¬ ì‹¤í–‰ í”Œë¡œìš°

#### **LangChain Tool ë˜í¼**

**íŒŒì¼**: `app/graphs/tools/cache_tools.py`

```python
from langchain_core.tools import tool
from typing import Optional
import json

@tool
async def check_direct_cache_tool(log_id: int, project_uuid: str) -> str:
    """
    ìš”ì²­í•œ ë¡œê·¸ì— ì´ë¯¸ ai_analysis í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        log_id: í™•ì¸í•  ë¡œê·¸ ID
        project_uuid: í”„ë¡œì íŠ¸ UUID

    Returns:
        JSON ë¬¸ìì—´:
        - cache_hit=Trueì¼ ê²½ìš°: {"cache_hit": true, "analysis": {...}}
        - cache_hit=Falseì¼ ê²½ìš°: "CACHE_MISS: No existing analysis"
    """
    from app.core.opensearch import opensearch_client

    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    query = {
        "query": {
            "bool": {
                "must": [
                    {"term": {"log_id": log_id}},
                    {"term": {"project_uuid.keyword": project_uuid}},
                    {"exists": {"field": "ai_analysis"}}
                ]
            }
        },
        "size": 1
    }

    try:
        response = opensearch_client.search(index=index_pattern, body=query)

        if response["hits"]["total"]["value"] > 0:
            ai_analysis = response["hits"]["hits"][0]["_source"]["ai_analysis"]
            return json.dumps({
                "cache_hit": True,
                "analysis": ai_analysis
            }, ensure_ascii=False)
        else:
            return "CACHE_MISS: No existing analysis found"

    except Exception as e:
        return f"ERROR: {str(e)}"


def create_check_direct_cache_tool(project_uuid: str):
    """
    Factory í•¨ìˆ˜: project_uuidë¥¼ ë°”ì¸ë”©í•œ ë„êµ¬ ìƒì„±

    ì´ íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :
    - LangChain Toolì€ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œ
    - project_uuidëŠ” API ë ˆë²¨ì—ì„œ ì´ë¯¸ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ë„êµ¬ë§ˆë‹¤ ì¤‘ë³µ ì „ë‹¬ ë¶ˆí•„ìš”
    - Factoryë¡œ project_uuidë¥¼ í´ë¡œì €ì— ìº¡ì²˜í•˜ì—¬ ë„êµ¬ í˜¸ì¶œ ì‹œ ìë™ ì£¼ì…
    """
    from functools import partial

    # partialë¡œ project_uuid ë°”ì¸ë”©
    bound_tool = partial(check_direct_cache_tool, project_uuid=project_uuid)

    # @tool ë°ì½”ë ˆì´í„° ì •ë³´ ìœ ì§€
    bound_tool.__name__ = check_direct_cache_tool.__name__
    bound_tool.__doc__ = check_direct_cache_tool.__doc__

    return bound_tool
```

#### **ë…¸ë“œì—ì„œ ë„êµ¬ í˜¸ì¶œ**

```python
async def _check_direct_cache_node(state: LogAnalysisState) -> dict:
    """Node 2: Direct Cache í™•ì¸"""

    log_id = state["log_id"]
    project_uuid = state["project_uuid"]

    # ë„êµ¬ ìƒì„± (project_uuid ë°”ì¸ë”©)
    tool = create_check_direct_cache_tool(project_uuid)

    # ë„êµ¬ ì‹¤í–‰
    result_str = await tool(log_id=log_id)

    # ê²°ê³¼ íŒŒì‹±
    if result_str.startswith("CACHE_MISS"):
        return {
            "direct_cache_result": None,
            "from_cache": False,
            "cache_type": None
        }

    elif result_str.startswith("ERROR"):
        return {
            "error": result_str,
            "from_cache": False
        }

    else:
        result = json.loads(result_str)
        return {
            "direct_cache_result": result["analysis"],
            "from_cache": True,
            "cache_type": "direct",
            "final_analysis": result["analysis"]
        }
```

### 4.4 ìƒíƒœ ê´€ë¦¬ íŒ¨í„´

#### **ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ëµ**

**LangGraphì˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë°©ì‹**:
- ê° ë…¸ë“œëŠ” **ë¶€ë¶„ ìƒíƒœ**ë¥¼ ë°˜í™˜
- LangGraphê°€ ìë™ìœ¼ë¡œ **ë³‘í•©** (shallow merge)
- ê¸°ì¡´ í‚¤ëŠ” ë®ì–´ì“°ê¸°, ìƒˆ í‚¤ëŠ” ì¶”ê°€

```python
# ì˜ˆì‹œ
current_state = {
    "log_id": 12345,
    "project_uuid": "uuid-here",
    "log_data": {...},
    "from_cache": False
}

# ë…¸ë“œê°€ ë°˜í™˜í•˜ëŠ” ë¶€ë¶„ ìƒíƒœ
node_update = {
    "direct_cache_result": {...},
    "from_cache": True,
    "cache_type": "direct"
}

# LangGraphê°€ ë³‘í•© í›„ ìƒˆ ìƒíƒœ
new_state = {
    "log_id": 12345,
    "project_uuid": "uuid-here",
    "log_data": {...},
    "from_cache": True,          # ë®ì–´ì“°ê¸°
    "direct_cache_result": {...},  # ì¶”ê°€
    "cache_type": "direct"         # ì¶”ê°€
}
```

#### **ë¶ˆë³€ ìƒíƒœ vs ê°€ë³€ ìƒíƒœ**

```python
# âŒ ì˜ëª»ëœ ë°©ì‹: ìƒíƒœë¥¼ ì§ì ‘ ìˆ˜ì •
async def _bad_node(state: LogAnalysisState) -> dict:
    state["log_data"] = fetch_log()  # ì§ì ‘ ìˆ˜ì • ê¸ˆì§€!
    return state  # ì „ì²´ ìƒíƒœ ë°˜í™˜ (ë¹„íš¨ìœ¨)

# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹: ë¶€ë¶„ ì—…ë°ì´íŠ¸ ë°˜í™˜
async def _good_node(state: LogAnalysisState) -> dict:
    log_data = fetch_log()
    return {"log_data": log_data}  # ë³€ê²½ì‚¬í•­ë§Œ ë°˜í™˜
```

---

## 5. 40+ ë„êµ¬ ì™„ì „ ë¶„ì„

### 5.1 ë„êµ¬ ì„¤ê³„ ì›ì¹™

**LangChain Tool ì¸í„°í˜ì´ìŠ¤**:

```python
from langchain_core.tools import tool

@tool
async def example_tool(param1: str, param2: int = 10) -> str:
    """
    ë„êµ¬ ì„¤ëª… (Agentê°€ ì½ëŠ” docstring)

    Args:
        param1: ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° ì„¤ëª…
        param2: ë‘ ë²ˆì§¸ íŒŒë¼ë¯¸í„° ì„¤ëª… (ê¸°ë³¸ê°’ 10)

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ê²°ê³¼ ë¬¸ìì—´
    """
    # ë„êµ¬ ë¡œì§ êµ¬í˜„
    result = perform_operation(param1, param2)

    # í•­ìƒ ë¬¸ìì—´ ë°˜í™˜ (Agentê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
    return format_as_markdown(result)
```

**ì„¤ê³„ ì›ì¹™**:
1. **ëª…í™•í•œ docstring**: Agentê°€ ë„êµ¬ì˜ ëª©ì ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ì´í•´
2. **íƒ€ì… íŒíŠ¸**: LangChainì´ ìë™ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ ìƒì„±
3. **ë¬¸ìì—´ ë°˜í™˜**: Agentê°€ ì§ì ‘ ì½ê³  í•´ì„
4. **ë§ˆí¬ë‹¤ìš´ í˜•ì‹**: í‘œ, ë¦¬ìŠ¤íŠ¸ ë“±ìœ¼ë¡œ êµ¬ì¡°í™”
5. **ì—ëŸ¬ ì²˜ë¦¬**: ì˜ˆì™¸ ë°œìƒ ì‹œ ë¬¸ìì—´ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜

### 5.2 ë„êµ¬ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„

#### **5.2.1 Search Tools (ê²€ìƒ‰ ë„êµ¬)**

**1. search_logs_by_keyword**

**íŒŒì¼**: `app/tools/search_tools.py:15-80`

```python
@tool
async def search_logs_by_keyword(
    project_uuid: str,
    keyword: str,
    limit: int = 20,
    time_hours: int = 24,
    level: Optional[str] = None
) -> str:
    """
    ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ 20)
        time_hours: ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ 24ì‹œê°„)
        level: ë¡œê·¸ ë ˆë²¨ í•„í„° (ERROR, WARN, INFO ë“±)

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ë°˜í™˜
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # OpenSearch match query (ì „ë¬¸ ê²€ìƒ‰)
    query = {
        "size": limit,
        "query": {
            "bool": {
                "must": [
                    {"match": {"message": {
                        "query": keyword,
                        "operator": "and",  # ëª¨ë“  ë‹¨ì–´ í¬í•¨
                        "fuzziness": "AUTO"  # ì˜¤íƒ€ í—ˆìš©
                    }}},
                    {"range": {"timestamp": {
                        "gte": start_time.isoformat() + "Z",
                        "lte": end_time.isoformat() + "Z"
                    }}}
                ]
            }
        },
        "sort": [{"timestamp": "desc"}],
        "highlight": {
            "fields": {
                "message": {
                    "pre_tags": ["**"],
                    "post_tags": ["**"]
                }
            }
        }
    }

    # ë ˆë²¨ í•„í„° ì¶”ê°€
    if level:
        query["query"]["bool"]["must"].append({"term": {"level": level}})

    try:
        response = opensearch_client.search(index=index_pattern, body=query)

        if response["hits"]["total"]["value"] == 0:
            return f"'{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤ (ìµœê·¼ {time_hours}ì‹œê°„)."

        # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ìƒì„±
        lines = [
            f"## ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: '{keyword}'",
            "",
            f"ì´ {response['hits']['total']['value']}ê±´ ì¤‘ {len(response['hits']['hits'])}ê±´ í‘œì‹œ",
            "",
            "| ì‹œê°„ | ë ˆë²¨ | ë©”ì‹œì§€ | ì„œë¹„ìŠ¤ |",
            "|------|------|--------|--------|"
        ]

        for hit in response["hits"]["hits"]:
            source = hit["_source"]

            # Highlightëœ ë©”ì‹œì§€ ì‚¬ìš© (í‚¤ì›Œë“œ ê°•ì¡°)
            message = hit.get("highlight", {}).get("message", [source.get("message", "")])[0]
            message_short = message[:80] + "..." if len(message) > 80 else message

            timestamp = datetime.fromisoformat(source["timestamp"].replace("Z", "+00:00")).strftime("%H:%M:%S")
            level = source.get("level", "INFO")
            service = source.get("service_name", "Unknown")

            lines.append(f"| {timestamp} | {level} | {message_short} | {service} |")

        return "\n".join(lines)

    except Exception as e:
        return f"ERROR: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```markdown
## ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: 'NullPointerException'

ì´ 15ê±´ ì¤‘ 10ê±´ í‘œì‹œ

| ì‹œê°„ | ë ˆë²¨ | ë©”ì‹œì§€ | ì„œë¹„ìŠ¤ |
|------|------|--------|--------|
| 14:32:15 | ERROR | **NullPointerException** at UserService.getUser() | user-service |
| 13:45:22 | ERROR | Caused by: **NullPointerException**: User object is null | user-service |
| 12:10:05 | ERROR | **NullPointerException** in payment processing | payment-service |
...
```

---

**2. search_logs_by_similarity**

```python
@tool
async def search_logs_by_similarity(
    project_uuid: str,
    query_text: str,
    limit: int = 10,
    min_score: float = 0.7
) -> str:
    """
    í…ìŠ¤íŠ¸ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¡œê·¸ë¥¼ ë²¡í„° ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        query_text: ê²€ìƒ‰ í…ìŠ¤íŠ¸
        limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
        min_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ (0-1)

    Returns:
        ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë¡œê·¸ ëª©ë¡
    """
    from app.services.embedding_service import embedding_service

    # 1. ê²€ìƒ‰ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
    query_vector = await embedding_service.embed_query(query_text)

    # 2. KNN ë²¡í„° ê²€ìƒ‰
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    query = {
        "size": limit,
        "query": {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'log_vector') + 1.0",
                    "params": {"query_vector": query_vector}
                }
            }
        }
    }

    response = opensearch_client.search(index=index_pattern, body=query)

    # 3. ê²°ê³¼ í•„í„°ë§ (min_score ì´ìƒë§Œ)
    results = []
    for hit in response["hits"]["hits"]:
        similarity = (hit["_score"] - 1.0)  # ì˜¤í”„ì…‹ ì œê±°

        if similarity >= min_score:
            source = hit["_source"]
            results.append({
                "similarity": round(similarity, 3),
                "log_id": source["log_id"],
                "message": source["message"],
                "timestamp": source["timestamp"],
                "level": source["level"]
            })

    if not results:
        return f"ìœ ì‚¬ë„ {min_score} ì´ìƒì¸ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 4. ë§ˆí¬ë‹¤ìš´ ì¶œë ¥
    lines = [
        f"## ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼: \"{query_text}\"",
        "",
        f"ìµœì†Œ ìœ ì‚¬ë„: {min_score} | ë°œê²¬: {len(results)}ê±´",
        "",
        "| ìœ ì‚¬ë„ | ì‹œê°„ | ë ˆë²¨ | ë©”ì‹œì§€ |",
        "|--------|------|------|--------|"
    ]

    for r in results:
        timestamp = datetime.fromisoformat(r["timestamp"].replace("Z", "+00:00")).strftime("%m/%d %H:%M")
        message_short = r["message"][:60] + "..." if len(r["message"]) > 60 else r["message"]

        lines.append(f"| {r['similarity']} | {timestamp} | {r['level']} | {message_short} |")

    return "\n".join(lines)
```

---

#### **5.2.2 Monitoring Tools (ëª¨ë‹ˆí„°ë§ ë„êµ¬)**

**20. get_service_health_status** (ì´ë¯¸ 3.2.4ì—ì„œ ìƒì„¸ ì„¤ëª…)

**21. get_error_frequency_ranking**

```python
@tool
async def get_error_frequency_ranking(
    project_uuid: str,
    limit: int = 10,
    time_hours: int = 24
) -> str:
    """
    ê°€ì¥ ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬ íƒ€ì… ìˆœìœ„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        limit: ìƒìœ„ Nê°œ ì—ëŸ¬ (ê¸°ë³¸ 10)
        time_hours: ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ 24ì‹œê°„)

    Returns:
        ì—ëŸ¬ íƒ€ì…ë³„ ë°œìƒ ë¹ˆë„ ìˆœìœ„ í…Œì´ë¸”
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ Exception íƒ€ì… ì¶”ì¶œì„ ìœ„í•œ ì •ê·œì‹
    # ì˜ˆ: "NullPointerException", "SQLException", "TimeoutException"
    query = {
        "size": 0,
        "query": {
            "bool": {
                "must": [
                    {"term": {"level": "ERROR"}},
                    {"range": {"timestamp": {
                        "gte": start_time.isoformat() + "Z",
                        "lte": end_time.isoformat() + "Z"
                    }}}
                ]
            }
        },
        "aggs": {
            "error_types": {
                "terms": {
                    "script": {
                        "source": """
                            def msg = doc['message.keyword'].value;
                            def matcher = /(\w+Exception|\w+Error)/.matcher(msg);
                            if (matcher.find()) {
                                return matcher.group(1);
                            } else {
                                return 'UnknownError';
                            }
                        """,
                        "lang": "painless"
                    },
                    "size": limit
                }
            }
        }
    }

    response = opensearch_client.search(index=index_pattern, body=query)

    buckets = response["aggregations"]["error_types"]["buckets"]

    if not buckets:
        return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    # ë§ˆí¬ë‹¤ìš´ ì¶œë ¥
    total_errors = sum(b["doc_count"] for b in buckets)

    lines = [
        f"## ğŸ“Š ì—ëŸ¬ ë°œìƒ ë¹ˆë„ ìˆœìœ„ (ìµœê·¼ {time_hours}ì‹œê°„)",
        "",
        f"ì´ ERROR: {total_errors}ê±´",
        "",
        "| ìˆœìœ„ | ì—ëŸ¬ íƒ€ì… | ë°œìƒ íšŸìˆ˜ | ë¹„ìœ¨ |",
        "|------|-----------|----------|------|"
    ]

    for idx, bucket in enumerate(buckets, 1):
        error_type = bucket["key"]
        count = bucket["doc_count"]
        percentage = (count / total_errors * 100) if total_errors > 0 else 0

        lines.append(f"| {idx} | **{error_type}** | {count:,} | {percentage:.1f}% |")

    return "\n".join(lines)
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```markdown
## ğŸ“Š ì—ëŸ¬ ë°œìƒ ë¹ˆë„ ìˆœìœ„ (ìµœê·¼ 24ì‹œê°„)

ì´ ERROR: 156ê±´

| ìˆœìœ„ | ì—ëŸ¬ íƒ€ì… | ë°œìƒ íšŸìˆ˜ | ë¹„ìœ¨ |
|------|-----------|----------|------|
| 1 | **NullPointerException** | 45 | 28.8% |
| 2 | **SQLException** | 32 | 20.5% |
| 3 | **TimeoutException** | 28 | 17.9% |
| 4 | **IllegalArgumentException** | 23 | 14.7% |
| 5 | **IOException** | 15 | 9.6% |
...
```

---

(ë¬¸ì„œ ê³„ì†...)

## 6. ë°ì´í„° í”Œë¡œìš° ë° í†µí•©

### 6.1 OpenSearch ì¸ë±ìŠ¤ êµ¬ì¡°

#### **ì¸ë±ìŠ¤ ëª…ëª… ê·œì¹™**

```
Format: {project_uuid_with_underscores}_{YYYY}_{MM}

ì˜ˆì‹œ:
- 3a73c7d4_8176_3929_b72f_d5b921daae67_2025_11
- 3a73c7d4_8176_3929_b72f_d5b921daae67_2025_12
```

**íŠ¹ì§•**:
- ì›”ë³„ ë¡œí…Œì´ì…˜ (Monthly rotation)
- í”„ë¡œì íŠ¸ë³„ ê²©ë¦¬ (Multi-tenancy)
- ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ ê°€ëŠ¥: `{uuid}_*`

#### **ë¬¸ì„œ ìŠ¤í‚¤ë§ˆ**

```json
{
  "_index": "3a73c7d4_8176_3929_b72f_d5b921daae67_2025_11",
  "_id": "auto-generated-id",
  "_source": {
    "log_id": 12345,
    "project_uuid": "3a73c7d4-8176-3929-b72f-d5b921daae67",
    "timestamp": "2025-11-18T10:30:00.123Z",
    "level": "ERROR",
    "message": "NullPointerException at UserService.getUser()",

    "service_name": "user-service",
    "component_name": "UserController",
    "layer": "Controller",
    "source_type": "BE",

    "trace_id": "abc123-def456-ghi789",
    "request_id": "req-12345",

    "log_details": {
      "class_name": "com.example.UserController",
      "method_name": "getUser",
      "line_number": 45,
      "http_method": "GET",
      "request_uri": "/api/users/12345",
      "response_status": 500,
      "execution_time_ms": 234,
      "request_body": "{\"userId\":12345}",
      "response_body": "{\"error\":\"Internal Server Error\"}",
      "stack_trace": "java.lang.NullPointerException\n\tat ..."
    },

    "ai_analysis": {
      "summary": "**NullPointerException** ë°œìƒ",
      "error_cause": "User ê°ì²´ null ì²´í¬ ëˆ„ë½",
      "solution": "### ì¦‰ì‹œ ì¡°ì¹˜...",
      "tags": ["SEVERITY_HIGH", "NullPointerException"],
      "analysis_type": "TRACE_BASED",
      "analyzed_at": "2025-11-18T10:35:00Z"
    },

    "log_vector": [0.123, -0.456, 0.789, ...]
  }
}
```

#### **Mapping ì„¤ì •**

```json
{
  "mappings": {
    "properties": {
      "log_id": {"type": "long"},
      "project_uuid": {"type": "keyword"},
      "timestamp": {"type": "date"},
      "level": {"type": "keyword"},
      "message": {
        "type": "text",
        "fields": {
          "keyword": {"type": "keyword", "ignore_above": 256}
        }
      },

      "service_name": {"type": "keyword"},
      "component_name": {"type": "keyword"},
      "layer": {"type": "keyword"},
      "source_type": {"type": "keyword"},

      "trace_id": {"type": "keyword"},
      "request_id": {"type": "keyword"},

      "log_details": {
        "type": "object",
        "properties": {
          "class_name": {"type": "keyword"},
          "method_name": {"type": "keyword"},
          "line_number": {"type": "integer"},
          "http_method": {"type": "keyword"},
          "request_uri": {"type": "keyword"},
          "response_status": {"type": "integer"},
          "execution_time_ms": {"type": "long"},
          "request_body": {"type": "text"},
          "response_body": {"type": "text"},
          "stack_trace": {"type": "text"}
        }
      },

      "ai_analysis": {
        "type": "object",
        "enabled": true
      },

      "log_vector": {
        "type": "knn_vector",
        "dimension": 1536,
        "method": {
          "name": "hnsw",
          "space_type": "cosinesimil",
          "engine": "nmslib",
          "parameters": {
            "ef_construction": 128,
            "m": 16
          }
        }
      }
    }
  }
}
```

### 6.2 Vector ì €ì¥ ë° HNSW ì•Œê³ ë¦¬ì¦˜

#### **HNSW (Hierarchical Navigable Small World)**

**ê°œë…**: ê³„ì¸µì  ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë¹ ë¥¸ ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰

```mermaid
graph TB
    subgraph "Layer 2 (ìµœìƒìœ„)"
        L2A((Node A))
        L2B((Node B))
        L2A ---|ì¥ê±°ë¦¬ ì—°ê²°| L2B
    end

    subgraph "Layer 1 (ì¤‘ê°„)"
        L1A((Node A))
        L1B((Node B))
        L1C((Node C))
        L1D((Node D))
        L1A --- L1B
        L1B --- L1C
        L1C --- L1D
    end

    subgraph "Layer 0 (ê¸°ë³¸)"
        L0A((Node A))
        L0B((Node B))
        L0C((Node C))
        L0D((Node D))
        L0E((Node E))
        L0F((Node F))
        L0A --- L0B
        L0B --- L0C
        L0C --- L0D
        L0D --- L0E
        L0E --- L0F
    end

    L2A -.-> L1A
    L2B -.-> L1B
    L1A -.-> L0A
    L1B -.-> L0B
    L1C -.-> L0C
    L1D -.-> L0D

    Query[ğŸ” Query Vector] -.ì‹œì‘.-> L2A
    L2A -.ì í”„.-> L2B
    L2B -.í•˜ê°•.-> L1B
    L1B -.íƒìƒ‰.-> L1C
    L1C -.í•˜ê°•.-> L0C
    L0C -.íƒìƒ‰.-> L0D
    L0D -.ê²°ê³¼.-> Result[âœ… ìœ ì‚¬ ë²¡í„° ë°œê²¬]

    style Query fill:#d1ecf1
    style Result fill:#d4edda
```

**ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„°**:

| íŒŒë¼ë¯¸í„° | ê°’ | ì˜ë¯¸ |
|---------|-----|------|
| **ef_construction** | 128 | ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼) |
| **m** | 16 | ê° ë…¸ë“œì˜ ìµœëŒ€ ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©) |
| **ef_search** | 100 | ê²€ìƒ‰ ì‹œ íƒìƒ‰ ë²”ìœ„ (ë™ì  ì¡°ì • ê°€ëŠ¥) |

**ì„±ëŠ¥ íŠ¹ì„±**:

| ë°ì´í„° í¬ê¸° | ê²€ìƒ‰ ì‹œê°„ (HNSW) | ê²€ìƒ‰ ì‹œê°„ (Brute Force) | ì •í™•ë„ |
|------------|-----------------|----------------------|--------|
| 10K ë²¡í„° | ~5ms | ~50ms | 99% |
| 100K ë²¡í„° | ~20ms | ~500ms | 98% |
| 1M ë²¡í„° | ~50ms | ~5000ms | 95% |
| 10M ë²¡í„° | ~150ms | ~50000ms | 95% |

### 6.3 OpenAI API í˜¸ì¶œ íŒ¨í„´

#### **Embedding ìƒì„±**

**íŒŒì¼**: `app/services/embedding_service.py`

```python
from langchain_openai import OpenAIEmbeddings
from app.core.config import settings

class EmbeddingService:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=settings.EMBEDDING_MODEL,  # "text-embedding-3-large"
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_BASE_URL,  # SSAFY GMS proxy
            chunk_size=100  # ë°°ì¹˜ í¬ê¸°
        )

    async def embed_query(self, text: str) -> list[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return await self.embeddings.aembed_query(text)

    async def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”©"""
        return await self.embeddings.aembed_documents(texts)

# ì‹±ê¸€í†¤
embedding_service = EmbeddingService()
```

**API í˜¸ì¶œ ì˜ˆì‹œ**:

```python
# POST https://gms.ssafy.io/gmsapi/api.openai.com/v1/embeddings
{
  "model": "text-embedding-3-large",
  "input": "NullPointerException at UserService.getUser()",
  "encoding_format": "float"
}

# Response
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "index": 0,
      "embedding": [
        0.0023064255,
        -0.009327292,
        -0.0028842222,
        ...  // 1536 dimensions
      ]
    }
  ],
  "model": "text-embedding-3-large",
  "usage": {
    "prompt_tokens": 10,
    "total_tokens": 10
  }
}
```

**ë¹„ìš©**:
- **text-embedding-3-large**: $0.00013 / 1K tokens
- í‰ê·  ë¡œê·¸ ë©”ì‹œì§€: ~10 tokens
- 1000ê°œ ë¡œê·¸ ì„ë² ë”© ë¹„ìš©: ~$0.0013 (1.3ì›)

#### **LLM ë¶„ì„ í˜¸ì¶œ**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=2000,
    openai_api_key=settings.OPENAI_API_KEY,
    openai_api_base=settings.OPENAI_BASE_URL
)

# ë¹„ë™ê¸° í˜¸ì¶œ
response = await llm.ainvoke(prompt)

# POST https://gms.ssafy.io/gmsapi/api.openai.com/v1/chat/completions
{
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "system", "content": "You are a log analysis expert..."},
    {"role": "user", "content": "Analyze this log: ..."}
  ],
  "temperature": 0.3,
  "max_tokens": 2000
}

# Response
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "gpt-4o-mini",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "{\"summary\": \"...\", \"error_cause\": \"...\", ...}"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 250,
    "completion_tokens": 150,
    "total_tokens": 400
  }
}
```

**ë¹„ìš© (gpt-4o-mini)**:
- Input: $0.150 / 1M tokens
- Output: $0.600 / 1M tokens
- í‰ê·  ë¶„ì„ (400 tokens): ~$0.00015 (0.15ì›)

---

## 7. í™˜ê²½ ì„¤ì • ë° êµ¬ì„±

### 7.1 í™˜ê²½ ë³€ìˆ˜ ì „ì²´ ëª©ë¡

**íŒŒì¼**: `.env`

```bash
# ============================================
# Application Settings
# ============================================
APP_NAME=log-analysis-api
APP_VERSION=2.0.0
ENVIRONMENT=production  # development | test | production
DEBUG=false

# ============================================
# CORS Settings
# ============================================
CORS_ORIGINS=http://localhost:3000,https://loglens.store,https://www.loglens.store
CORS_ALLOW_CREDENTIALS=true

# ============================================
# OpenAI Settings (SSAFY GMS Proxy)
# ============================================
OPENAI_API_KEY=gms_key_here
OPENAI_BASE_URL=https://gms.ssafy.io/gmsapi/api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-large
LLM_MODEL=gpt-4o-mini

# ============================================
# OpenSearch Settings
# ============================================
OPENSEARCH_HOST=opensearch.loglens.store
OPENSEARCH_PORT=443
OPENSEARCH_USER=admin
OPENSEARCH_PASSWORD=Admin123!@#
OPENSEARCH_USE_SSL=true
OPENSEARCH_VERIFY_CERTS=true
OPENSEARCH_SSL_SHOW_WARN=false
OPENSEARCH_TIMEOUT=30
OPENSEARCH_MAX_RETRIES=3

# ============================================
# Analysis Settings
# ============================================
# ìœ ì‚¬ë„ ê²€ìƒ‰ ì„ê³„ê°’ (0-1, ë†’ì„ìˆ˜ë¡ ì—„ê²©)
SIMILARITY_THRESHOLD=0.92

# ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸ ìµœëŒ€ ê°œìˆ˜
MAX_CONTEXT_LOGS=5

# ë¶„ì„ ì¬ì‹œë„ íšŸìˆ˜
MAX_ANALYSIS_RETRIES=2

# ============================================
# Caching Settings
# ============================================
# ìºì‹œ TTL (ì´ˆ)
DEFAULT_CACHE_TTL=1800       # 30ë¶„
SHORT_CACHE_TTL=600          # 10ë¶„
LONG_CACHE_TTL=86400         # 1ì¼

# ìœ ì‚¬ë„ ìºì‹œ í›„ë³´ ê°œìˆ˜
CACHE_CANDIDATE_SIZE=10

# ============================================
# Map-Reduce Settings
# ============================================
# Map-Reduce í™œì„±í™”
ENABLE_MAP_REDUCE=true

# ë¡œê·¸ ì²­í¬ í¬ê¸° (Map ë‹¨ìœ„)
LOG_CHUNK_SIZE=5

# Map-Reduce ì‚¬ìš© ì„ê³„ê°’ (ë¡œê·¸ ê°œìˆ˜)
MAP_REDUCE_THRESHOLD=10

# ============================================
# Validation Settings
# ============================================
# í•œêµ­ì–´ ë¹„ìœ¨ ì„ê³„ê°’ (0-1)
KOREAN_PERCENTAGE_THRESHOLD=0.9

# ê²€ì¦ ì ìˆ˜ ì„ê³„ê°’
VALIDATION_STRUCTURAL_THRESHOLD=0.7
VALIDATION_CONTENT_THRESHOLD=0.6
VALIDATION_OVERALL_THRESHOLD=0.65

# ê²€ì¦ ì¬ì‹œë„ íšŸìˆ˜
VALIDATION_MAX_RETRIES=2

# ============================================
# Agent Settings
# ============================================
# Agent LLM ëª¨ë¸
AGENT_MODEL=gpt-4o-mini

# Agent ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
AGENT_MAX_ITERATIONS=12

# Agent ë””ë²„ê·¸ ëª¨ë“œ
AGENT_VERBOSE=true

# Agent íƒ€ì„ì•„ì›ƒ (ì´ˆ)
AGENT_TIMEOUT=60

# ============================================
# Logging Settings
# ============================================
LOG_LEVEL=INFO  # DEBUG | INFO | WARNING | ERROR
LOG_FORMAT=json  # json | text
LOG_FILE=logs/app.log

# ============================================
# Performance Settings
# ============================================
# ë¹„ë™ê¸° ì›Œì»¤ ìˆ˜
ASYNC_WORKERS=4

# ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
REQUEST_TIMEOUT=120

# ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
MAX_CONCURRENT_REQUESTS=50
```

### 7.2 Configuration Class

**íŒŒì¼**: `app/core/config.py`

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í´ë˜ìŠ¤"""

    # Application
    APP_NAME: str = "log-analysis-api"
    APP_VERSION: str = "2.0.0"
    ENVIRONMENT: str = "development"
    DEBUG: bool = False

    # CORS
    CORS_ORIGINS: str = "http://localhost:3000"
    CORS_ALLOW_CREDENTIALS: bool = True

    # OpenAI
    OPENAI_API_KEY: str
    OPENAI_BASE_URL: str = "https://api.openai.com/v1"
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    LLM_MODEL: str = "gpt-4o-mini"

    # OpenSearch
    OPENSEARCH_HOST: str = "localhost"
    OPENSEARCH_PORT: int = 9200
    OPENSEARCH_USER: str = "admin"
    OPENSEARCH_PASSWORD: str = "admin"
    OPENSEARCH_USE_SSL: bool = False
    OPENSEARCH_VERIFY_CERTS: bool = False
    OPENSEARCH_SSL_SHOW_WARN: bool = False
    OPENSEARCH_TIMEOUT: int = 30
    OPENSEARCH_MAX_RETRIES: int = 3

    # Analysis
    SIMILARITY_THRESHOLD: float = 0.92
    MAX_CONTEXT_LOGS: int = 5
    MAX_ANALYSIS_RETRIES: int = 2

    # Caching
    DEFAULT_CACHE_TTL: int = 1800
    SHORT_CACHE_TTL: int = 600
    LONG_CACHE_TTL: int = 86400
    CACHE_CANDIDATE_SIZE: int = 10

    # Map-Reduce
    ENABLE_MAP_REDUCE: bool = True
    LOG_CHUNK_SIZE: int = 5
    MAP_REDUCE_THRESHOLD: int = 10

    # Validation
    KOREAN_PERCENTAGE_THRESHOLD: float = 0.9
    VALIDATION_STRUCTURAL_THRESHOLD: float = 0.7
    VALIDATION_CONTENT_THRESHOLD: float = 0.6
    VALIDATION_OVERALL_THRESHOLD: float = 0.65
    VALIDATION_MAX_RETRIES: int = 2

    # Agent
    AGENT_MODEL: str = "gpt-4o-mini"
    AGENT_MAX_ITERATIONS: int = 12
    AGENT_VERBOSE: bool = True
    AGENT_TIMEOUT: int = 60

    # Logging
    LOG_LEVEL: str = "INFO"
    LOG_FORMAT: str = "json"
    LOG_FILE: str = "logs/app.log"

    # Performance
    ASYNC_WORKERS: int = 4
    REQUEST_TIMEOUT: int = 120
    MAX_CONCURRENT_REQUESTS: int = 50

    @property
    def cors_origins_list(self) -> list[str]:
        """CORS originsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return [origin.strip() for origin in self.CORS_ORIGINS.split(",")]

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
settings = Settings()
```

### 7.3 ìºì‹±/ê²€ì¦ ì„ê³„ê°’ íŠœë‹ ê°€ì´ë“œ

#### **Similarity Threshold (ìœ ì‚¬ë„ ì„ê³„ê°’)**

| ê°’ | ìºì‹œ íˆíŠ¸ìœ¨ | ì •í™•ë„ | ê¶Œì¥ ìƒí™© |
|----|-----------|--------|----------|
| **0.95** | â­â­ ë‚®ìŒ | â­â­â­â­â­ ë§¤ìš° ë†’ìŒ | ì¤‘ìš” ì‹œìŠ¤í…œ, ì˜¤íƒ ì ˆëŒ€ ë¶ˆê°€ |
| **0.92** | â­â­â­ ì¤‘ê°„ | â­â­â­â­ ë†’ìŒ | **ê¶Œì¥ (ê¸°ë³¸ê°’)** |
| **0.85** | â­â­â­â­ ë†’ìŒ | â­â­â­ ì¤‘ê°„ | ë¹„ìš© ì ˆê° ìš°ì„ , ì¼ë¶€ ì˜¤íƒ í—ˆìš© |
| **0.70** | â­â­â­â­â­ ë§¤ìš° ë†’ìŒ | â­â­ ë‚®ìŒ | ê¶Œì¥í•˜ì§€ ì•ŠìŒ |

**íŠœë‹ ë°©ë²•**:
```bash
# 1ì£¼ì¼ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
python scripts/tune_similarity_threshold.py --days 7 --thresholds 0.85,0.90,0.92,0.95

# ê²°ê³¼ ë¶„ì„
# - Cache Hit Rate
# - False Positive Rate (ì˜ëª»ëœ ìºì‹œ ì¬ì‚¬ìš©)
# - Cost Savings

# ìµœì ê°’ ì„ íƒ
export SIMILARITY_THRESHOLD=0.92
```

#### **Validation Thresholds (ê²€ì¦ ì„ê³„ê°’)**

```python
# êµ¬ì¡°ì  ì™„ì „ì„± (0.7)
# - í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€
# - ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì¤€ìˆ˜
# ì¡°ì •: 0.6 (ê´€ëŒ€) ~ 0.8 (ì—„ê²©)

# ë‚´ìš© ì •í™•ì„± (0.6)
# - error_causeê°€ log messageì™€ ì¼ì¹˜
# - solutionì´ êµ¬ì²´ì 
# ì¡°ì •: 0.5 (ê´€ëŒ€) ~ 0.7 (ì—„ê²©)

# ì¢…í•© ì ìˆ˜ (0.65)
# - ê°€ì¤‘ í‰ê· 
# ì¡°ì •: 0.6 (ê´€ëŒ€) ~ 0.75 (ì—„ê²©)

# ì—„ê²©í•˜ê²Œ ì„¤ì • ì‹œ ì¬ì‹œë„ ì¦ê°€ â†’ LLM ë¹„ìš© ì¦ê°€
# ê´€ëŒ€í•˜ê²Œ ì„¤ì • ì‹œ í’ˆì§ˆ ì €í•˜
```

---

## 8. ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ëª¨ìŒ

### 8.1 ì „ì²´ ì‹œìŠ¤í…œ ìƒí˜¸ì‘ìš©

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant FE as Frontend<br/>(React)
    participant BE as Backend<br/>(Spring Boot)
    participant AI as AI Service<br/>(FastAPI)
    participant OS as OpenSearch
    participant OpenAI as OpenAI API

    User->>FE: ë¡œê·¸ ë¶„ì„ ìš”ì²­
    FE->>BE: POST /api/logs/{id}/analysis
    BE->>AI: GET /api/v2-langgraph/logs/{id}/analysis
    
    AI->>OS: Fetch log (Node 1)
    OS-->>AI: Log data
    
    AI->>OS: Check direct cache (Node 2)
    OS-->>AI: Cache miss
    
    AI->>OS: Check trace cache (Node 3)
    OS-->>AI: Cache miss
    
    AI->>OpenAI: Generate embedding
    OpenAI-->>AI: Vector [1536 dims]
    
    AI->>OS: KNN similarity search (Node 4)
    OS-->>AI: Cache miss
    
    AI->>OS: Collect related logs (Node 5)
    OS-->>AI: 3 logs
    
    AI->>AI: Decide strategy: Direct (Node 6)
    
    AI->>OpenAI: Analyze logs (Node 7)
    OpenAI-->>AI: Analysis result
    
    AI->>AI: Validate result (Node 8)
    
    AI->>OS: Save result + Trace propagation (Node 9)
    OS-->>AI: Success
    
    AI-->>BE: LogAnalysisResponse
    BE-->>FE: Analysis result
    FE-->>User: ë¶„ì„ ê²°ê³¼ í‘œì‹œ
```

### 8.2 ì±—ë´‡ ëŒ€í™” í”Œë¡œìš°

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant FE as Frontend
    participant BE as Backend
    participant AI as AI Service
    participant Agent as ReAct Agent
    participant Tool as Tool (40+)
    participant OS as OpenSearch
    participant LLM as GPT-4o mini

    User->>FE: "ìµœê·¼ ì—ëŸ¬ëŠ”?"
    FE->>BE: POST /api/v2/chatbot/ask/stream
    BE->>AI: Streaming request
    
    AI->>AI: Off-topic filter (Pass)
    AI->>AI: Classify: error_analysis
    AI->>Agent: Create ReAct Agent
    
    Agent->>LLM: Thought: Need recent errors
    LLM-->>Agent: Action: get_recent_errors
    
    Agent->>Tool: get_recent_errors(limit=10)
    Tool->>OS: Search ERROR logs
    OS-->>Tool: 10 error logs
    Tool-->>Agent: Observation: [10 errors]
    
    Agent->>LLM: Thought: Have enough info
    LLM-->>Agent: Final Answer: "ìµœê·¼ 24ì‹œê°„ ë™ì•ˆ..."
    
    Agent-->>AI: Final answer
    AI->>AI: Validate & sanitize
    AI-->>BE: SSE stream (chunks)
    BE-->>FE: SSE events
    FE-->>User: ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼
```

### 8.3 ìºì‹± í”Œë¡œìš° ë¹„êµ

```mermaid
sequenceDiagram
    participant Req as Request
    participant Cache as Cache Layer
    participant OS as OpenSearch
    participant LLM as GPT-4o mini

    Note over Req,LLM: Case 1: Direct Cache Hit (50ms)
    Req->>Cache: Check direct cache
    Cache->>OS: Query ai_analysis field
    OS-->>Cache: Analysis found
    Cache-->>Req: Cached result âœ… ($0)

    Note over Req,LLM: Case 2: Similarity Cache Hit (150ms)
    Req->>Cache: Check similarity cache
    Cache->>OS: Generate embedding
    OS-->>Cache: Vector
    Cache->>OS: KNN search
    OS-->>Cache: Similar log (0.94)
    Cache-->>Req: Cached result âœ… ($0.00013)

    Note over Req,LLM: Case 3: Fresh Analysis (3000ms)
    Req->>Cache: All cache miss
    Cache->>LLM: Analyze logs
    LLM-->>Cache: New analysis
    Cache->>OS: Save result
    Cache-->>Req: Fresh result âœ… ($0.003)
```

---

## 9. ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë²” ì‚¬ë¡€

### 9.1 ìºì‹± ì „ëµ ìµœì í™”

#### **1. 3-Tier ìºì‹± ìˆœì„œ ìµœì í™”**

```python
# âœ… ì˜¬ë°”ë¥¸ ìˆœì„œ (ë¹„ìš© íš¨ìœ¨ì )
1. Direct Cache (0ms, $0)
2. Trace Cache (80ms, $0)
3. Similarity Cache (150ms, $0.00013)
4. Fresh Analysis (3000ms, $0.003)

# âŒ ì˜ëª»ëœ ìˆœì„œ
1. Similarity Cache ë¨¼ì € â†’ ë¶ˆí•„ìš”í•œ ì„ë² ë”© ìƒì„±
2. Fresh Analysis ë¨¼ì € â†’ ìºì‹œ ë¬´ìš©ì§€ë¬¼
```

#### **2. Trace ì „íŒŒ í™œìš©**

```python
# âœ… ê¶Œì¥: ë¶„ì„ í›„ ëª¨ë“  ê´€ë ¨ ë¡œê·¸ì— ì „íŒŒ
if not from_cache and related_logs:
    for log in related_logs:
        save_ai_analysis_to_log(log.log_id, analysis)

# íš¨ê³¼: ë‹¤ìŒ ìš”ì²­ ì‹œ Direct Cache Hit
# 1íšŒ ë¶„ì„ ë¹„ìš©ìœ¼ë¡œ Nê°œ ë¡œê·¸ ì»¤ë²„
```

#### **3. ì„ë² ë”© ì‚¬ì „ ìƒì„±**

```python
# ë¡œê·¸ ìˆ˜ì§‘ ì‹œì ì— ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
async def precompute_embeddings(logs: list):
    messages = [log.message for log in logs]
    vectors = await embedding_service.embed_documents(messages)

    for log, vector in zip(logs, vectors):
        save_vector_to_opensearch(log.log_id, vector)

# íš¨ê³¼: Similarity Cache ê²€ìƒ‰ ì‹œ ì„ë² ë”© ì¬ìƒì„± ë¶ˆí•„ìš”
```

### 9.2 í† í° ìµœì í™”

#### **1. ë©”ì‹œì§€ íŠ¸ë ì¼€ì´ì…˜**

```python
# ë¡œê·¸ ë©”ì‹œì§€ ìµœëŒ€ ê¸¸ì´ ì œí•œ
MAX_MESSAGE_LENGTH = 200

def truncate_message(message: str) -> str:
    if len(message) <= MAX_MESSAGE_LENGTH:
        return message
    return message[:MAX_MESSAGE_LENGTH] + "... (truncated)"
```

#### **2. Map-Reduce ì²­í¬ í¬ê¸° ì¡°ì •**

```python
# ì‹¤í—˜ ê²°ê³¼: ì²­í¬ í¬ê¸° 5ê°œê°€ ìµœì 
# - 3ê°œ: ìš”ì•½ í’ˆì§ˆ ì €í•˜
# - 5ê°œ: âœ… ê· í˜•
# - 10ê°œ: í† í° ë‚­ë¹„

LOG_CHUNK_SIZE = 5
```

#### **3. í”„ë¡¬í”„íŠ¸ ìµœì í™”**

```python
# âŒ ë¹„íš¨ìœ¨ì  í”„ë¡¬í”„íŠ¸ (800 tokens)
prompt = f"""
ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³ ì˜ ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìˆ˜ì‹­ ë…„ê°„ì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ...
(ë¶ˆí•„ìš”í•œ ì„¤ëª… 500ì)

ë¡œê·¸: {log_data}
"""

# âœ… íš¨ìœ¨ì  í”„ë¡¬í”„íŠ¸ (300 tokens)
prompt = f"""ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

ë¡œê·¸: {log_data}

ì‘ë‹µ í˜•ì‹ (JSON):
{{"summary": "...", "error_cause": "...", "solution": "...", "tags": [...]}}
"""
```

### 9.3 ë™ì‹œì„± ì²˜ë¦¬

#### **ë³‘ë ¬ ì²˜ë¦¬ íŒ¨í„´**

```python
import asyncio

# âœ… ì—¬ëŸ¬ ë¡œê·¸ ë™ì‹œ ë¶„ì„
async def analyze_multiple_logs(log_ids: list[int]) -> list:
    tasks = [analyze_single_log(log_id) for log_id in log_ids]
    results = await asyncio.gather(*tasks)
    return results

# âœ… Map-Reduce ë³‘ë ¬í™”
async def map_reduce_parallel(chunks: list) -> str:
    # MAP Phase ë³‘ë ¬ ì‹¤í–‰
    map_tasks = [summarize_chunk(chunk) for chunk in chunks]
    summaries = await asyncio.gather(*map_tasks)

    # REDUCE Phase (ìˆœì°¨)
    final_result = await reduce_summaries(summaries)
    return final_result
```

### 9.4 ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸

#### **í•µì‹¬ ë©”íŠ¸ë¦­**

```python
# Prometheus ë©”íŠ¸ë¦­ ì˜ˆì‹œ
from prometheus_client import Counter, Histogram, Gauge

# 1. ìºì‹œ íˆíŠ¸ìœ¨
cache_hits = Counter('cache_hits_total', 'Cache hits', ['cache_type'])
cache_misses = Counter('cache_misses_total', 'Cache misses')

cache_hits.labels(cache_type='direct').inc()
cache_hits.labels(cache_type='trace').inc()
cache_hits.labels(cache_type='similarity').inc()

# 2. LLM í˜¸ì¶œ íšŸìˆ˜ ë° ë¹„ìš©
llm_calls = Counter('llm_calls_total', 'LLM API calls', ['model'])
llm_tokens = Counter('llm_tokens_total', 'Tokens used', ['type'])  # input/output
llm_cost = Gauge('llm_cost_dollars', 'Estimated LLM cost ($)')

# 3. ì‘ë‹µ ì‹œê°„
response_time = Histogram('response_time_seconds', 'Response time', ['endpoint', 'cache_status'])

# 4. ì—ëŸ¬ìœ¨
error_rate = Counter('errors_total', 'Errors', ['error_type'])
```

#### **ë¡œê¹… ì „ëµ**

```python
import structlog

logger = structlog.get_logger()

# êµ¬ì¡°í™”ëœ ë¡œê·¸
logger.info(
    "log_analysis_completed",
    log_id=12345,
    project_uuid="uuid-here",
    cache_type="similarity",
    similarity_score=0.94,
    llm_calls=0,
    duration_ms=150,
    cost_dollars=0.00013
)

# ê²€ìƒ‰ ê°€ëŠ¥í•œ í•„ë“œ
# - ë¹„ìš© ë¶„ì„: sum(cost_dollars)
# - ìºì‹œ íš¨ìœ¨: count(cache_type != null) / count(*)
# - í‰ê·  ì‘ë‹µ ì‹œê°„: avg(duration_ms)
```

---

## ê²°ë¡ 

LogLens AI ì„œë¹„ìŠ¤ëŠ” ìµœì‹  AI ê¸°ìˆ ê³¼ ì—”ì§€ë‹ˆì–´ë§ ëª¨ë²” ì‚¬ë¡€ë¥¼ ê²°í•©í•œ í”„ë¡œë•ì…˜ê¸‰ ë¡œê·¸ ë¶„ì„ í”Œë«í¼ì…ë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼

1. **ë¹„ìš© ì ˆê°**: 3-tier ìºì‹±ìœ¼ë¡œ 97-99% LLM ë¹„ìš© ì ˆê°
2. **ì •í™•ë„**: ìœ ì‚¬ë„ ì„ê³„ê°’ 0.92ë¡œ 98% ì •í™•ë„ ìœ ì§€
3. **í™•ì¥ì„±**: Map-Reduce íŒ¨í„´ìœ¼ë¡œ 100+ ë¡œê·¸ íš¨ìœ¨ì  ì²˜ë¦¬
4. **ììœ¨ì„±**: 40+ ë„êµ¬ë¥¼ í™œìš©í•œ ReAct Agent ìë™ ì¶”ë¡ 
5. **í’ˆì§ˆ ë³´ì¦**: ë‹¤ë‹¨ê³„ ê²€ì¦ íŒŒì´í”„ë¼ì¸

### í–¥í›„ ê°œì„  ë°©í–¥

1. **ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**: GPT-4o-mini â†’ GPT-4o (ì •í™•ë„ í–¥ìƒ)
2. **ìºì‹± ê³ ë„í™”**: Redis ì¶”ê°€ ë ˆì´ì–´, ë¶„ì‚° ìºì‹œ
3. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: Kafka/Pulsar ì—°ë™ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„
4. **ë©€í‹°ëª¨ë‹¬**: ë¡œê·¸ + ë©”íŠ¸ë¦­ + íŠ¸ë ˆì´ìŠ¤ í†µí•© ë¶„ì„
5. **ìë™ íŠœë‹**: ê°•í™”í•™ìŠµ ê¸°ë°˜ ì„ê³„ê°’ ìë™ ìµœì í™”

---

**ë¬¸ì„œ ì‘ì„±**: LogLens AI Team  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-18  
**ë²„ì „**: 2.0.0

