"""
ë¶„ì„ ë„êµ¬ (Analysis Tools)
- ë¡œê·¸ í†µê³„, ìµœê·¼ ì—ëŸ¬ ë¶„ì„
"""

import re
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


def extract_exception_type(source: Dict[str, Any]) -> str:
    """
    ë¡œê·¸ ë°ì´í„°ì—ì„œ ì˜ˆì™¸ íƒ€ì…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„:
    1. log_details.exception_type (nested í•„ë“œ)
    2. ai_analysis.tags (AIê°€ ë¶„ì„í•œ íƒœê·¸)
    3. messageì—ì„œ ì •ê·œì‹ ì¶”ì¶œ
    4. "Unknown" ë°˜í™˜
    """
    # 1. log_details.exception_type ìš°ì„ 
    log_details = source.get("log_details", {})
    exc_type = log_details.get("exception_type")
    if exc_type and exc_type != "Unknown" and exc_type.strip():
        return exc_type

    # 2. AI ë¶„ì„ íƒœê·¸ì—ì„œ ì¶”ì¶œ
    ai_analysis = source.get("ai_analysis", {})
    ai_tags = ai_analysis.get("tags", [])
    if ai_tags:
        for tag in ai_tags:
            if "Exception" in tag or "Error" in tag or "Timeout" in tag:
                return tag

    # 3. messageì—ì„œ ì •ê·œì‹ ì¶”ì¶œ
    message = source.get("message", "")
    patterns = [
        r'([A-Z][a-zA-Z]*Exception)',
        r'([A-Z][a-zA-Z]*Error)',
        r'(DatabaseTimeout|ConnectionRefused|ConnectionPoolExhausted|PoolExhausted|Timeout)',
    ]
    for pattern in patterns:
        match = re.search(pattern, message)
        if match:
            return match.group(1)

    return "Unknown"


def assess_severity(source: Dict[str, Any]) -> int:
    """
    ì—ëŸ¬ ì‹¬ê°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

    Returns:
        1: CRITICAL (ìµœê³  ì‹¬ê°ë„) - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
        2: HIGH (ë†’ìŒ) - ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”
        3: MEDIUM (ì¤‘ê°„) - ìš°ì„  ì¡°ì¹˜ í•„ìš”
        4: LOW (ë‚®ìŒ) - ëª¨ë‹ˆí„°ë§ í•„ìš”
        5: MINIMAL (ìµœì†Œ) - ì •ë³´ì„±
    """
    exc_type = extract_exception_type(source)
    log_details = source.get("log_details", {})
    response_status = log_details.get("response_status", 0)

    # AI ë¶„ì„ì´ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
    ai_analysis = source.get("ai_analysis", {})
    analysis_type = ai_analysis.get("analysis_type", "").upper()
    if analysis_type == "CRITICAL":
        return 1
    elif analysis_type == "HIGH":
        return 2
    elif analysis_type == "MEDIUM":
        return 3

    # Database/Connection ì—ëŸ¬ = CRITICAL (ëª¨ë“  ì‚¬ìš©ì ì˜í–¥)
    critical_keywords = [
        "Database", "Connection", "Pool", "Timeout",
        "OutOfMemory", "StackOverflow", "Deadlock"
    ]
    if any(keyword in exc_type for keyword in critical_keywords):
        return 1

    # 5xx ì—ëŸ¬ = HIGH (ì„œë²„ ì˜¤ë¥˜)
    if 500 <= response_status < 600:
        return 2

    # Security/Auth ì—ëŸ¬ = HIGH (ë³´ì•ˆ ìœ„í—˜)
    security_keywords = ["Auth", "Security", "Unauthorized", "Token", "Permission"]
    if any(keyword in exc_type for keyword in security_keywords):
        return 2

    # NullPointer, Runtime = MEDIUM (íŠ¹ì • ê¸°ëŠ¥ ì˜í–¥)
    medium_keywords = ["NullPointer", "Runtime", "IllegalState", "IllegalArgument"]
    if any(keyword in exc_type for keyword in medium_keywords):
        return 3

    # 4xx ì—ëŸ¬ = LOW (í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜)
    if 400 <= response_status < 500:
        return 4

    # ê¸°íƒ€ = MINIMAL
    return 5


@tool
async def get_log_statistics(
    project_uuid: str,
    time_hours: int = 24,
    group_by: str = "level"
) -> str:
    """
    ë¡œê·¸ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸ (ì˜ˆ: "ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½í•´ì¤˜")
    - íŠ¹ì • ê¸°ê°„ì˜ ë¡œê·¸ ë¶„í¬ í™•ì¸ (ì˜ˆ: "ì˜¤ëŠ˜ ì—ëŸ¬ê°€ ëª‡ ê°œì•¼?")
    - ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ í˜„í™© íŒŒì•…

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: í†µê³„ ê¸°ê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)
        group_by: ì§‘ê³„ ê¸°ì¤€ (level, service_name, source_type ì¤‘ í•˜ë‚˜, ê¸°ë³¸ level)

    ì°¸ê³ :
    - project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ "ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤" ì‘ë‹µì€ ìœ íš¨í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ì‹œë„í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        í†µê³„ ì •ë³´ (ë ˆë²¨ë³„/ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ê°œìˆ˜, ì—ëŸ¬ìœ¨ ë“±)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # Aggregation ì¿¼ë¦¬
    query_body = {
        "query": {
            "range": {
                "timestamp": {
                    "gte": time_range["start"],
                    "lte": time_range["end"]
                }
            }
        },
        "size": 0,  # í†µê³„ë§Œ í•„ìš” (ë¬¸ì„œëŠ” ë¶ˆí•„ìš”)
        "aggs": {
            "by_level": {
                "terms": {
                    "field": "level",
                    "size": 10
                }
            },
            "by_service": {
                "terms": {
                    "field": "service_name",
                    "size": 10
                }
            },
            "by_source": {
                "terms": {
                    "field": "source_type",
                    "size": 10
                }
            },
            "timeline": {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": f"{max(1, time_hours // 24)}h"  # ì‹œê°„ëŒ€ë³„
                }
            }
        }
    }

    try:
        # OpenSearch Aggregation ì‹¤í–‰ (sync client)
        results = opensearch_client.search(
            index=index_pattern,
            body=query_body
        )

        # ì „ì²´ ë¡œê·¸ ê°œìˆ˜
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggs = results.get("aggregations", {})

        # ë ˆë²¨ë³„ í†µê³„
        level_buckets = aggs.get("by_level", {}).get("buckets", [])
        level_stats = {bucket["key"]: bucket["doc_count"] for bucket in level_buckets}

        # ì„œë¹„ìŠ¤ë³„ í†µê³„
        service_buckets = aggs.get("by_service", {}).get("buckets", [])
        service_stats = {bucket["key"]: bucket["doc_count"] for bucket in service_buckets}

        # ì†ŒìŠ¤ë³„ í†µê³„
        source_buckets = aggs.get("by_source", {}).get("buckets", [])
        source_stats = {bucket["key"]: bucket["doc_count"] for bucket in source_buckets}

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ë¡œê·¸ í†µê³„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì´ ë¡œê·¸ ê°œìˆ˜: {total_count}ê±´",
            ""
        ]

        # ë ˆë²¨ë³„
        if level_stats:
            summary_lines.append("ğŸ“Š ë ˆë²¨ë³„ ë¶„í¬:")
            for level, count in sorted(level_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {level}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì—ëŸ¬ìœ¨
        error_count = level_stats.get("ERROR", 0)
        if error_count > 0:
            error_rate = (error_count / total_count) * 100
            summary_lines.append(f"âš ï¸  ì—ëŸ¬ìœ¨: {error_rate:.2f}% ({error_count}ê±´)")
            summary_lines.append("")

        # ì„œë¹„ìŠ¤ë³„
        if service_stats and group_by in ["service_name", "all"]:
            summary_lines.append("ğŸ”§ ì„œë¹„ìŠ¤ë³„ ë¡œê·¸:")
            for service, count in sorted(service_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {service}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì†ŒìŠ¤ë³„
        if source_stats and group_by in ["source_type", "all"]:
            summary_lines.append("ğŸ“ ì†ŒìŠ¤ë³„ ë¡œê·¸:")
            for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
                summary_lines.append(f"  - {source}: {count}ê±´")
            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_recent_errors(
    project_uuid: str,
    limit: int = 10,
    service_name: Optional[str] = None,
    time_hours: int = 24
) -> str:
    """
    ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ìµœê·¼ ë°œìƒí•œ ì—ëŸ¬ í™•ì¸ (ì˜ˆ: "ìµœê·¼ ì—ëŸ¬ê°€ ë­ì•¼?")
    - íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ì—ëŸ¬ë§Œ ì¡°íšŒ (ì˜ˆ: "user-service ì—ëŸ¬ ë³´ì—¬ì¤˜")
    - ì—ëŸ¬ ë°œìƒ ë¹ˆë„ íŒŒì•…

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        limit: ì¡°íšŒí•  ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        service_name: íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¡°íšŒ (ì„ íƒ)
        time_hours: ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)

    ì°¸ê³ :
    - project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ "ERROR ë ˆë²¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤" ì‘ë‹µì€ ìœ íš¨í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ì‹œë„í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ ì „ì²´ ì—ëŸ¬ í˜„í™©ì„ íŒŒì•…í•˜ë ¤ë©´ service_name í•„í„° ì—†ì´ ë¨¼ì € ì¡°íšŒí•˜ì„¸ìš”. ì„œë¹„ìŠ¤ë³„ ìƒì„¸ ë¶„ì„ì€ ê·¸ ë‹¤ìŒì— í•„ìš”ì‹œì—ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”.

    Returns:
        ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (ì‹œê°„ ì—­ìˆœ)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    query = {
        "bool": {
            "must": must_clauses,
            "filter": [
                {
                    "range": {
                        "timestamp": {
                            "gte": time_range["start"],
                            "lte": time_range["end"]
                        }
                    }
                }
            ]
        }
    }

    try:
        # OpenSearch ê²€ìƒ‰ (sync client)
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "query": query,
                "size": limit,
                "sort": [{"timestamp": "desc"}],
                "_source": [
                    "message", "level", "service_name", "timestamp", "log_id",
                    "stacktrace",  # í•„ë“œëª… ìˆ˜ì • (stack_trace -> stacktrace)
                    "layer", "component_name",
                    # Nested fields (log_details)
                    "log_details.exception_type",
                    "log_details.class_name",
                    "log_details.method_name",
                    "log_details.http_method",
                    "log_details.request_uri",
                    "log_details.response_status",
                    "log_details.execution_time",
                    "log_details.stacktrace",
                    # AI analysis fields
                    "ai_analysis.summary",
                    "ai_analysis.error_cause",
                    "ai_analysis.solution",
                    "ai_analysis.tags",
                    "ai_analysis.analysis_type"
                ]
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            service_filter = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë ˆë²¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤{service_filter}."

        # ì—ëŸ¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
        error_types = {}
        errors_with_severity = []  # (hit, severity) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        for hit in hits:
            source = hit["_source"]
            exc_type = extract_exception_type(source)
            severity = assess_severity(source)
            error_types[exc_type] = error_types.get(exc_type, 0) + 1
            errors_with_severity.append((hit, severity))

        # ì‹¬ê°ë„ìˆœ ì •ë ¬ (ë‚®ì€ ìˆ«ì = ë†’ì€ ì‹¬ê°ë„)
        errors_with_severity.sort(key=lambda x: (x[1], x[0]["_source"].get("timestamp", "")), reverse=True)

        # ê²°ê³¼ í¬ë§·íŒ…
        service_filter_str = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
        summary_lines = [
            f"=== ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (ìµœê·¼ {time_hours}ì‹œê°„){service_filter_str} ===",
            f"ì´ {total_count}ê±´ì˜ ì—ëŸ¬ ë°œìƒ, ìƒìœ„ {len(hits)}ê±´ í‘œì‹œ",
            ""
        ]

        # ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬
        if error_types:
            summary_lines.append("ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬:")
            for exc_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                summary_lines.append(f"  - {exc_type}: {count}ê±´")
            summary_lines.append("")

        # ìƒìœ„ ì—ëŸ¬ ëª©ë¡ (ì‹¬ê°ë„ìˆœ)
        summary_lines.append("ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (ì‹¬ê°ë„ìˆœ):")
        severity_labels = {1: "CRITICAL", 2: "HIGH", 3: "MEDIUM", 4: "LOW", 5: "MINIMAL"}

        for i, (hit, severity) in enumerate(errors_with_severity, 1):
            source = hit["_source"]
            msg = source.get("message", "")[:400]
            timestamp_str = source.get("timestamp", "")[:19]
            service = source.get("service_name", "unknown")
            log_id = source.get("log_id", "")
            layer = source.get("layer", "")
            component = source.get("component_name", "")

            # ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ
            exc_type = extract_exception_type(source)

            # log_details ì ‘ê·¼
            log_details = source.get("log_details", {})
            class_name = log_details.get("class_name", "")
            method_name = log_details.get("method_name", "")
            http_method = log_details.get("http_method", "")
            request_uri = log_details.get("request_uri", "")
            response_status = log_details.get("response_status")
            execution_time = log_details.get("execution_time")

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€
            has_stack = bool(source.get("stacktrace") or log_details.get("stacktrace"))

            # AI ë¶„ì„ ê²°ê³¼
            ai_analysis = source.get("ai_analysis", {})
            ai_summary = ai_analysis.get("summary", "")
            ai_cause = ai_analysis.get("error_cause", "")
            ai_solution = ai_analysis.get("solution", "")

            # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            severity_label = severity_labels.get(severity, "UNKNOWN")
            summary_lines.append(f"{i}. [{exc_type}] {timestamp_str} | ì‹¬ê°ë„: {severity_label}")
            summary_lines.append(f"   ì„œë¹„ìŠ¤: {service}")

            # ë ˆì´ì–´/ì»´í¬ë„ŒíŠ¸
            if layer or component:
                loc_info = []
                if layer:
                    loc_info.append(f"Layer: {layer}")
                if component:
                    loc_info.append(f"Component: {component}")
                summary_lines.append(f"   ìœ„ì¹˜: {', '.join(loc_info)}")

            # í´ë˜ìŠ¤/ë©”ì„œë“œ
            if class_name and method_name:
                summary_lines.append(f"   ğŸ“ {class_name}.{method_name}")
            elif class_name:
                summary_lines.append(f"   ğŸ“ {class_name}")

            # HTTP ì •ë³´
            if http_method and request_uri:
                status_info = f" â†’ {response_status}" if response_status else ""
                summary_lines.append(f"   ğŸŒ {http_method} {request_uri}{status_info}")
            elif response_status:
                summary_lines.append(f"   ğŸ“Š HTTP {response_status}")

            # ì‹¤í–‰ ì‹œê°„
            if execution_time:
                summary_lines.append(f"   â±ï¸  {execution_time}ms")

            # ë©”ì‹œì§€
            summary_lines.append(f"   ğŸ’¬ {msg}...")

            # AI ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
            if ai_summary:
                summary_lines.append(f"   ğŸ¤– AI ë¶„ì„: {ai_summary[:200]}")
            if ai_cause:
                summary_lines.append(f"   ğŸ“Œ ì›ì¸: {ai_cause[:150]}")
            if ai_solution:
                summary_lines.append(f"   ğŸ’¡ í•´ê²°ì±…: {ai_solution[:150]}")

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì—¬ë¶€
            if has_stack:
                summary_lines.append(f"   ğŸ“š (ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìˆìŒ)")

            # log_id
            if log_id:
                summary_lines.append(f"   (log_id: {log_id})")

            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
