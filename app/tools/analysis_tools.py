"""
ë¶„ì„ ë„êµ¬ (Analysis Tools)
- ë¡œê·¸ í†µê³„, ìµœê·¼ ì—ëŸ¬ ë¶„ì„
"""

import re
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client
from app.tools.common_fields import ALL_FIELDS


def extract_exception_type(source: Dict[str, Any]) -> str:
    """
    ë¡œê·¸ ë°ì´í„°ì—ì„œ ì˜ˆì™¸ íƒ€ì…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„:
    1. log_details.exception_type (nested í•„ë“œ)
    2. ai_analysis.tags (AIê°€ ë¶„ì„í•œ íƒœê·¸)
    3. messageì—ì„œ ì •ê·œì‹ ì¶”ì¶œ
    4. "Unknown" ë°˜í™˜
    """
    # 1. log_details.exception_type ìš°ì„ 
    log_details = source.get("log_details", {})
    exc_type = log_details.get("exception_type")
    if exc_type and exc_type != "Unknown" and exc_type.strip():
        return exc_type

    # 2. AI ë¶„ì„ íƒœê·¸ì—ì„œ ì¶”ì¶œ
    ai_analysis = source.get("ai_analysis", {})
    ai_tags = ai_analysis.get("tags", [])
    if ai_tags:
        for tag in ai_tags:
            if "Exception" in tag or "Error" in tag or "Timeout" in tag:
                return tag

    # 3. messageì—ì„œ ì •ê·œì‹ ì¶”ì¶œ
    message = source.get("message", "")
    patterns = [
        r'([A-Z][a-zA-Z]*Exception)',
        r'([A-Z][a-zA-Z]*Error)',
        r'(DatabaseTimeout|ConnectionRefused|ConnectionPoolExhausted|PoolExhausted|Timeout)',
    ]
    for pattern in patterns:
        match = re.search(pattern, message)
        if match:
            return match.group(1)

    return "Unknown"


def assess_severity(source: Dict[str, Any]) -> int:
    """
    ì—ëŸ¬ ì‹¬ê°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

    Returns:
        1: CRITICAL (ìµœê³  ì‹¬ê°ë„) - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
        2: HIGH (ë†’ìŒ) - ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”
        3: MEDIUM (ì¤‘ê°„) - ìš°ì„  ì¡°ì¹˜ í•„ìš”
        4: LOW (ë‚®ìŒ) - ëª¨ë‹ˆí„°ë§ í•„ìš”
        5: MINIMAL (ìµœì†Œ) - ì •ë³´ì„±
    """
    exc_type = extract_exception_type(source)
    log_details = source.get("log_details", {})
    response_status = log_details.get("response_status", 0)

    # AI ë¶„ì„ì´ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
    ai_analysis = source.get("ai_analysis", {})
    analysis_type = ai_analysis.get("analysis_type", "").upper()
    if analysis_type == "CRITICAL":
        return 1
    elif analysis_type == "HIGH":
        return 2
    elif analysis_type == "MEDIUM":
        return 3

    # Database/Connection ì—ëŸ¬ = CRITICAL (ëª¨ë“  ì‚¬ìš©ì ì˜í–¥)
    critical_keywords = [
        "Database", "Connection", "Pool", "Timeout",
        "OutOfMemory", "StackOverflow", "Deadlock"
    ]
    if any(keyword in exc_type for keyword in critical_keywords):
        return 1

    # 5xx ì—ëŸ¬ = HIGH (ì„œë²„ ì˜¤ë¥˜)
    if 500 <= response_status < 600:
        return 2

    # Security/Auth ì—ëŸ¬ = HIGH (ë³´ì•ˆ ìœ„í—˜)
    security_keywords = ["Auth", "Security", "Unauthorized", "Token", "Permission"]
    if any(keyword in exc_type for keyword in security_keywords):
        return 2

    # NullPointer, Runtime = MEDIUM (íŠ¹ì • ê¸°ëŠ¥ ì˜í–¥)
    medium_keywords = ["NullPointer", "Runtime", "IllegalState", "IllegalArgument"]
    if any(keyword in exc_type for keyword in medium_keywords):
        return 3

    # 4xx ì—ëŸ¬ = LOW (í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜)
    if 400 <= response_status < 500:
        return 4

    # ê¸°íƒ€ = MINIMAL
    return 5


@tool
async def get_log_statistics(
    project_uuid: str,
    time_hours: int = 24,
    group_by: str = "level"
) -> str:
    """
    ë¡œê·¸ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸ (ì˜ˆ: "ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½í•´ì¤˜")
    - íŠ¹ì • ê¸°ê°„ì˜ ë¡œê·¸ ë¶„í¬ í™•ì¸ (ì˜ˆ: "ì˜¤ëŠ˜ ì—ëŸ¬ê°€ ëª‡ ê°œì•¼?")
    - ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ í˜„í™© íŒŒì•…

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: í†µê³„ ê¸°ê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)
        group_by: ì§‘ê³„ ê¸°ì¤€ (level, service_name, source_type ì¤‘ í•˜ë‚˜, ê¸°ë³¸ level)

    ì°¸ê³ :
    - project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ "ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤" ì‘ë‹µì€ ìœ íš¨í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ì‹œë„í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        í†µê³„ ì •ë³´ (ë ˆë²¨ë³„/ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ê°œìˆ˜, ì—ëŸ¬ìœ¨ ë“±)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # Aggregation ì¿¼ë¦¬
    query_body = {
        "query": {
            "range": {
                "timestamp": {
                    "gte": time_range["start"],
                    "lte": time_range["end"]
                }
            }
        },
        "size": 0,  # í†µê³„ë§Œ í•„ìš” (ë¬¸ì„œëŠ” ë¶ˆí•„ìš”)
        "aggs": {
            "by_level": {
                "terms": {
                    "field": "level",
                    "size": 10
                }
            },
            "by_service": {
                "terms": {
                    "field": "service_name",
                    "size": 10
                }
            },
            "by_source": {
                "terms": {
                    "field": "source_type",
                    "size": 10
                }
            },
            "timeline": {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": f"{max(1, time_hours // 24)}h"  # ì‹œê°„ëŒ€ë³„
                }
            }
        }
    }

    try:
        # OpenSearch Aggregation ì‹¤í–‰ (sync client)
        results = opensearch_client.search(
            index=index_pattern,
            body=query_body
        )

        # ì „ì²´ ë¡œê·¸ ê°œìˆ˜
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggs = results.get("aggregations", {})

        # ë ˆë²¨ë³„ í†µê³„
        level_buckets = aggs.get("by_level", {}).get("buckets", [])
        level_stats = {bucket["key"]: bucket["doc_count"] for bucket in level_buckets}

        # ì„œë¹„ìŠ¤ë³„ í†µê³„
        service_buckets = aggs.get("by_service", {}).get("buckets", [])
        service_stats = {bucket["key"]: bucket["doc_count"] for bucket in service_buckets}

        # ì†ŒìŠ¤ë³„ í†µê³„
        source_buckets = aggs.get("by_source", {}).get("buckets", [])
        source_stats = {bucket["key"]: bucket["doc_count"] for bucket in source_buckets}

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ë¡œê·¸ í†µê³„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì´ ë¡œê·¸ ê°œìˆ˜: {total_count}ê±´",
            ""
        ]

        # ë ˆë²¨ë³„
        if level_stats:
            summary_lines.append("ğŸ“Š ë ˆë²¨ë³„ ë¶„í¬:")
            for level, count in sorted(level_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {level}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì—ëŸ¬ìœ¨
        error_count = level_stats.get("ERROR", 0)
        if error_count > 0:
            error_rate = (error_count / total_count) * 100
            summary_lines.append(f"âš ï¸  ì—ëŸ¬ìœ¨: {error_rate:.2f}% ({error_count}ê±´)")
            summary_lines.append("")

        # ì„œë¹„ìŠ¤ë³„
        if service_stats and group_by in ["service_name", "all"]:
            summary_lines.append("ğŸ”§ ì„œë¹„ìŠ¤ë³„ ë¡œê·¸:")
            for service, count in sorted(service_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {service}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì†ŒìŠ¤ë³„
        if source_stats and group_by in ["source_type", "all"]:
            summary_lines.append("ğŸ“ ì†ŒìŠ¤ë³„ ë¡œê·¸:")
            for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
                summary_lines.append(f"  - {source}: {count}ê±´")
            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_recent_errors(
    project_uuid: str,
    limit: int = 10,
    service_name: Optional[str] = None,
    time_hours: int = 24
) -> str:
    """ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ë° ì‹¬ê°ë„ ë¶„ì„. "ìµœê·¼ ì—ëŸ¬?", "user-service ì—ëŸ¬", "ì‹¬ê°í•œ ì—ëŸ¬ ëª©ë¡" âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„, ìë™ ì‹¬ê°ë„ í‰ê°€ í¬í•¨"""
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    query = {
        "bool": {
            "must": must_clauses,
            "filter": [
                {
                    "range": {
                        "timestamp": {
                            "gte": time_range["start"],
                            "lte": time_range["end"]
                        }
                    }
                }
            ]
        }
    }

    try:
        # OpenSearch ê²€ìƒ‰ (sync client)
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "query": query,
                "size": limit,
                "sort": [{"timestamp": "desc"}],
                "_source": ALL_FIELDS  # ê³µí†µ í•„ë“œ ì‚¬ìš© (trace_id, request_id í¬í•¨)
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            service_filter = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë ˆë²¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤{service_filter}."

        # ì—ëŸ¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
        error_types = {}
        errors_with_severity = []  # (hit, severity) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        for hit in hits:
            source = hit["_source"]
            exc_type = extract_exception_type(source)
            severity = assess_severity(source)
            error_types[exc_type] = error_types.get(exc_type, 0) + 1
            errors_with_severity.append((hit, severity))

        # ì‹¬ê°ë„ìˆœ ì •ë ¬ (ë‚®ì€ ìˆ«ì = ë†’ì€ ì‹¬ê°ë„)
        errors_with_severity.sort(key=lambda x: (x[1], x[0]["_source"].get("timestamp", "")), reverse=True)

        # ê²°ê³¼ í¬ë§·íŒ…
        service_filter_str = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
        summary_lines = [
            f"=== ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (ìµœê·¼ {time_hours}ì‹œê°„){service_filter_str} ===",
            f"ìƒìœ„ {len(hits)}ê±´ í‘œì‹œ",  # "ì´ Xê±´" ì œê±° - LLM í˜¼ë€ ë°©ì§€
            ""
        ]

        # ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬
        if error_types:
            summary_lines.append("ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬:")
            for exc_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                summary_lines.append(f"  - {exc_type}: {count}ê±´")
            summary_lines.append("")

        # ìƒìœ„ ì—ëŸ¬ ëª©ë¡ (ì‹¬ê°ë„ìˆœ)
        summary_lines.append("ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (ì‹¬ê°ë„ìˆœ):")
        severity_labels = {1: "CRITICAL", 2: "HIGH", 3: "MEDIUM", 4: "LOW", 5: "MINIMAL"}

        for i, (hit, severity) in enumerate(errors_with_severity, 1):
            source = hit["_source"]
            msg = source.get("message", "")[:200]  # 400 â†’ 200ìë¡œ ì¶•ì†Œ (ì»¨í…ìŠ¤íŠ¸ ë¶€ë‹´ ê°ì†Œ)
            timestamp_str = source.get("timestamp", "")[:19]
            service = source.get("service_name", "unknown")
            log_id = source.get("log_id", "")
            layer = source.get("layer", "")
            component = source.get("component_name", "")

            # ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ
            exc_type = extract_exception_type(source)

            # log_details ì ‘ê·¼
            log_details = source.get("log_details", {})
            class_name = log_details.get("class_name", "")
            method_name = log_details.get("method_name", "")
            http_method = log_details.get("http_method", "")
            request_uri = log_details.get("request_uri", "")
            response_status = log_details.get("response_status")
            execution_time = log_details.get("execution_time")

            # ì¶”ì  ì •ë³´ (trace_id, request_id)
            trace_id = source.get("trace_id", "")
            request_id = source.get("request_id", "")

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€
            has_stack = bool(source.get("stacktrace") or log_details.get("stacktrace"))

            # AI ë¶„ì„ ê²°ê³¼
            ai_analysis = source.get("ai_analysis", {})
            ai_summary = ai_analysis.get("summary", "")
            ai_cause = ai_analysis.get("error_cause", "")
            ai_solution = ai_analysis.get("solution", "")

            # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            severity_label = severity_labels.get(severity, "UNKNOWN")
            summary_lines.append(f"{i}. [{exc_type}] {timestamp_str} | ì‹¬ê°ë„: {severity_label}")
            summary_lines.append(f"   ì„œë¹„ìŠ¤: {service}")

            # ë ˆì´ì–´/ì»´í¬ë„ŒíŠ¸
            if layer or component:
                loc_info = []
                if layer:
                    loc_info.append(f"Layer: {layer}")
                if component:
                    loc_info.append(f"Component: {component}")
                summary_lines.append(f"   ìœ„ì¹˜: {', '.join(loc_info)}")

            # í´ë˜ìŠ¤/ë©”ì„œë“œ
            if class_name and method_name:
                summary_lines.append(f"   ğŸ“ {class_name}.{method_name}")
            elif class_name:
                summary_lines.append(f"   ğŸ“ {class_name}")

            # HTTP ì •ë³´
            if http_method and request_uri:
                status_info = f" â†’ {response_status}" if response_status else ""
                summary_lines.append(f"   ğŸŒ {http_method} {request_uri}{status_info}")
            elif response_status:
                summary_lines.append(f"   ğŸ“Š HTTP {response_status}")

            # ì‹¤í–‰ ì‹œê°„
            if execution_time:
                summary_lines.append(f"   â±ï¸  {execution_time}ms")

            # ë©”ì‹œì§€
            summary_lines.append(f"   ğŸ’¬ {msg}...")

            # AI ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°) - ì¶•ì†Œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë¶€ë‹´ ê°ì†Œ
            if ai_summary:
                summary_lines.append(f"   ğŸ¤– AI: {ai_summary[:100]}")  # 200 â†’ 100ì
            if ai_cause:
                summary_lines.append(f"   ğŸ“Œ ì›ì¸: {ai_cause[:80]}")  # 150 â†’ 80ì
            if ai_solution:
                summary_lines.append(f"   ğŸ’¡ í•´ê²°: {ai_solution[:80]}")  # 150 â†’ 80ì

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì—¬ë¶€
            if has_stack:
                summary_lines.append(f"   ğŸ“š (ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìˆìŒ)")

            # ì¶”ì  IDë“¤ (trace_id, request_id, log_id)
            tracking_ids = []
            if log_id:
                tracking_ids.append(f"log_id: {log_id}")
            if trace_id:
                tracking_ids.append(f"trace_id: {trace_id}")
            if request_id:
                tracking_ids.append(f"request_id: {request_id}")

            if tracking_ids:
                summary_lines.append(f"   ({', '.join(tracking_ids)})")

            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def correlate_logs(
    project_uuid: str,
    log_id: int,
    correlation_type: str = "trace",
    time_window_minutes: int = 5,
    limit: int = 20
) -> str:
    """
    íŠ¹ì • ë¡œê·¸ì™€ ì—°ê´€ëœ ë‹¤ë¥¸ ë¡œê·¸ë“¤ì„ ì°¾ì•„ ìƒê´€ê´€ê³„ ë¶„ì„

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        log_id: ê¸°ì¤€ ë¡œê·¸ ID
        correlation_type: ìƒê´€ê´€ê³„ ìœ í˜• (trace|error_type|time_window)
            - trace: ê°™ì€ trace_idë¥¼ ê°€ì§„ ë¡œê·¸ë“¤ (ë¶„ì‚° ì¶”ì )
            - error_type: ê°™ì€ ì—ëŸ¬ íƒ€ì… ë°œìƒ ë¡œê·¸ë“¤
            - time_window: ì‹œê°„ëŒ€ ê¸°ë°˜ (Â±Në¶„ ì´ë‚´)
        time_window_minutes: time_window ì‚¬ìš© ì‹œ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ 5ë¶„)
        limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        ì—°ê´€ ë¡œê·¸ ëª©ë¡ ë° ë¶„ì„

    Examples:
        - "ì´ NullPointerExceptionì˜ ê·¼ë³¸ ì›ì¸ì€?"
        - "log_id 12345ì™€ ì—°ê´€ëœ ë¡œê·¸ ì¶”ì "
    """
    try:
        from app.core.opensearch import opensearch_client
        from datetime import datetime, timedelta

        client = opensearch_client
        index_name = f"logs_{project_uuid}"

        # 1. ê¸°ì¤€ ë¡œê·¸ ì¡°íšŒ
        try:
            base_log = client.get(index=index_name, id=str(log_id))
            base_source = base_log['_source']
        except Exception:
            return f"ë¡œê·¸ ID {log_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        summary_lines = [f"=== ë¡œê·¸ ìƒê´€ê´€ê³„ ë¶„ì„ (log_id: {log_id}) ===", ""]

        # ê¸°ì¤€ ë¡œê·¸ ì •ë³´
        base_timestamp = base_source.get('timestamp')
        base_service = base_source.get('service_name', 'unknown')
        base_level = base_source.get('level', 'INFO')
        base_message = base_source.get('message', '')[:150]

        summary_lines.append("**ê¸°ì¤€ ë¡œê·¸:**")
        summary_lines.append(f"- ì‹œê°„: {base_timestamp}")
        summary_lines.append(f"- ì„œë¹„ìŠ¤: {base_service}")
        summary_lines.append(f"- ë ˆë²¨: {base_level}")
        summary_lines.append(f"- ë©”ì‹œì§€: {base_message}")
        summary_lines.append("")

        # 2. ìƒê´€ê´€ê³„ íƒ€ì…ë³„ ê²€ìƒ‰
        query = None

        if correlation_type == "trace":
            # trace_id ê¸°ë°˜ (EC2: top-level field)
            trace_id = base_source.get('trace_id')
            if not trace_id:
                return f"ê¸°ì¤€ ë¡œê·¸ì— trace_idê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ correlation_typeì„ ì‚¬ìš©í•˜ì„¸ìš”."

            query = {
                "bool": {
                    "must": [
                        {"term": {"trace_id.keyword": trace_id}}
                    ],
                    "must_not": [
                        {"term": {"_id": str(log_id)}}  # ìê¸° ìì‹  ì œì™¸
                    ]
                }
            }
            summary_lines.append(f"**ìƒê´€ê´€ê³„ ìœ í˜•:** trace_id ì¶”ì  ({trace_id})")

        elif correlation_type == "error_type":
            # ê°™ì€ ì—ëŸ¬ íƒ€ì… (EC2: log_details.exception_type)
            error_type = base_source.get('log_details', {}).get('exception_type') or \
                        base_message.split(':')[0] if ':' in base_message else None

            if not error_type:
                return "ê¸°ì¤€ ë¡œê·¸ì—ì„œ ì—ëŸ¬ íƒ€ì…ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            query = {
                "bool": {
                    "should": [
                        {"match": {"message": error_type}},
                        {"term": {"log_details.exception_type": error_type}}
                    ],
                    "minimum_should_match": 1,
                    "must_not": [
                        {"term": {"_id": str(log_id)}}
                    ]
                }
            }
            summary_lines.append(f"**ìƒê´€ê´€ê³„ ìœ í˜•:** ê°™ì€ ì—ëŸ¬ íƒ€ì… ({error_type})")

        elif correlation_type == "time_window":
            # ì‹œê°„ëŒ€ ê¸°ë°˜ (Â±Në¶„)
            if not base_timestamp:
                return "ê¸°ì¤€ ë¡œê·¸ì— timestampê°€ ì—†ìŠµë‹ˆë‹¤."

            base_dt = datetime.fromisoformat(base_timestamp.replace('Z', '+00:00'))
            start_time = base_dt - timedelta(minutes=time_window_minutes)
            end_time = base_dt + timedelta(minutes=time_window_minutes)

            query = {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "timestamp": {
                                    "gte": start_time.isoformat(),
                                    "lte": end_time.isoformat()
                                }
                            }
                        }
                    ],
                    "must_not": [
                        {"term": {"_id": str(log_id)}}
                    ]
                }
            }
            summary_lines.append(f"**ìƒê´€ê´€ê³„ ìœ í˜•:** ì‹œê°„ ë²”ìœ„ (Â±{time_window_minutes}ë¶„)")

        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” correlation_type: {correlation_type}"

        # 3. ê²€ìƒ‰ ì‹¤í–‰
        response = client.search(
            index=index_name,
            body={
                "query": query,
                "sort": [{"timestamp": "asc"}],
                "size": limit
            }
        )

        hits = response['hits']['hits']
        total = response['hits']['total']['value']

        if total == 0:
            summary_lines.append("")
            summary_lines.append("ì—°ê´€ëœ ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return "\n".join(summary_lines)

        summary_lines.append(f"**ë°œê²¬:** ì´ {total}ê±´ ì¤‘ {len(hits)}ê±´ í‘œì‹œ")
        summary_lines.append("")

        # 4. ì—°ê´€ ë¡œê·¸ ëª©ë¡
        summary_lines.append("**ì—°ê´€ ë¡œê·¸ ëª©ë¡ (ì‹œê°„ìˆœ):**")
        summary_lines.append("")

        for i, hit in enumerate(hits, 1):
            source = hit['_source']
            related_id = hit.get('_id')
            timestamp = source.get('timestamp', 'N/A')
            level = source.get('level', 'INFO')
            service = source.get('service_name', 'unknown')
            message = source.get('message', '')[:200]

            summary_lines.append(f"{i}. [{level}] {timestamp}")
            summary_lines.append(f"   ğŸ”§ {service}")
            summary_lines.append(f"   ğŸ’¬ {message}")
            summary_lines.append(f"   (log_id: {related_id})")
            summary_lines.append("")

        # 5. ì¸ì‚¬ì´íŠ¸
        summary_lines.append("**ğŸ’¡ ë¶„ì„ ì¸ì‚¬ì´íŠ¸:**")
        if correlation_type == "trace":
            summary_lines.append(f"- ì´ ìš”ì²­ì€ {len(hits)+1}ê°œ ì„œë¹„ìŠ¤ë¥¼ ê±°ì³ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤")
            summary_lines.append("- trace_idë¡œ ì „ì²´ í˜¸ì¶œ íë¦„ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        elif correlation_type == "error_type":
            summary_lines.append(f"- ê°™ì€ ì—ëŸ¬ê°€ ì´ {total+1}ê±´ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
            summary_lines.append("- ì¬í˜„ ê°€ëŠ¥í•œ ë²„ê·¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤")
        elif correlation_type == "time_window":
            summary_lines.append(f"- {time_window_minutes}ë¶„ ì´ë‚´ {total}ê±´ì˜ ë¡œê·¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
            summary_lines.append("- ë™ì‹œë‹¤ë°œì  ë¬¸ì œ ë˜ëŠ” ì—°ì‡„ ì¥ì•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def analyze_errors_unified(
    project_uuid: str,
    group_by: str = "severity",
    sort_by: str = "severity",
    time_hours: int = 24,
    limit: int = 10,
    service_name: Optional[str] = None
) -> str:
    """í†µí•© ì—ëŸ¬ ë¶„ì„ (ì‹¬ê°ë„ë³„/ë¹ˆë„ë³„/ì„œë¹„ìŠ¤ë³„/APIë³„). group_by: severity/error_type/service/api, sort_by: severity/frequency/time âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„"""
    # ê¸°ì¡´ ë„êµ¬ ì¬ì‚¬ìš©
    if group_by == "severity" and sort_by == "severity":
        # get_recent_errors ë¡œì§
        return await get_recent_errors.ainvoke({
            "project_uuid": project_uuid,
            "limit": limit,
            "service_name": service_name,
            "time_hours": time_hours
        })
    elif group_by == "error_type":
        # get_error_frequency_ranking ë¡œì§
        from app.tools.monitoring_tools import get_error_frequency_ranking
        return await get_error_frequency_ranking.ainvoke({
            "project_uuid": project_uuid,
            "time_hours": time_hours,
            "limit": limit,
            "service_name": service_name
        })
    elif group_by == "api":
        # get_api_error_rates ë¡œì§
        from app.tools.monitoring_tools import get_api_error_rates
        return await get_api_error_rates.ainvoke({
            "project_uuid": project_uuid,
            "time_hours": time_hours,
            "limit": limit
        })
    elif group_by == "service":
        # get_service_health_status ë¡œì§
        from app.tools.monitoring_tools import get_service_health_status
        return await get_service_health_status.ainvoke({
            "project_uuid": project_uuid,
            "time_hours": time_hours,
            "service_name": service_name
        })
    else:
        return f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¡°í•©: group_by={group_by}, sort_by={sort_by}\n\nì§€ì›ë˜ëŠ” ì˜µì…˜:\n- group_by: severity|error_type|service|api\n- sort_by: severity|frequency|time"


@tool
async def analyze_single_log(
    project_uuid: str,
    log_id: int
) -> str:
    """ë‹¨ì¼ ë¡œê·¸ AI ì‹¬ì¸µ ë¶„ì„. "log_id 12345 ë¶„ì„", "ì—ëŸ¬ ê·¼ë³¸ ì›ì¸", "í•´ê²° ë°©ë²•" âš ï¸ log_id í•„ìˆ˜, 1íšŒ í˜¸ì¶œë¡œ ì™„ì „í•œ ë¶„ì„ ì œê³µ"""
    try:
        # v2-langgraph ì„œë¹„ìŠ¤ ì„í¬íŠ¸
        from app.services.log_analysis_service_v2 import log_analysis_service_v2

        # AI ë¶„ì„ ì‹¤í–‰
        result = await log_analysis_service_v2.analyze_log(
            log_id=log_id,
            project_uuid=project_uuid
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        analysis = result.analysis
        lines = [
            f"## ğŸ¤– AI ë¡œê·¸ ë¶„ì„ (log_id: {log_id})",
            ""
        ]

        # ìºì‹œ ì •ë³´
        if result.from_cache:
            cache_info = "âœ… ìºì‹œ ì‚¬ìš©"
            if result.similar_log_id:
                cache_info += f" (ìœ ì‚¬ ë¡œê·¸: {result.similar_log_id}, ìœ ì‚¬ë„: {result.similarity_score:.2f})"
            lines.append(f"**{cache_info}**")
            lines.append("")

        # ìš”ì•½
        if analysis.summary:
            lines.append("### ğŸ“‹ ìš”ì•½")
            lines.append(analysis.summary)
            lines.append("")

        # ê·¼ë³¸ ì›ì¸
        if analysis.error_cause:
            lines.append("### ğŸ” ê·¼ë³¸ ì›ì¸ (Root Cause)")
            lines.append(analysis.error_cause)
            lines.append("")

        # í•´ê²° ë°©ë²•
        if analysis.solution:
            lines.append("### ğŸ’¡ í•´ê²° ë°©ë²• (Solutions)")
            lines.append(analysis.solution)
            lines.append("")

        # íƒœê·¸
        if analysis.tags:
            lines.append("### ğŸ·ï¸ íƒœê·¸")
            lines.append(", ".join(analysis.tags))
            lines.append("")

        # ë¶„ì„ íƒ€ì…
        if hasattr(analysis, 'analysis_type') and analysis.analysis_type:
            lines.append(f"**ë¶„ì„ íƒ€ì…:** {analysis.analysis_type.value}")
        if hasattr(analysis, 'target_type') and analysis.target_type:
            lines.append(f"**ëŒ€ìƒ íƒ€ì…:** {analysis.target_type.value}")
        if hasattr(analysis, 'analyzed_at') and analysis.analyzed_at:
            lines.append(f"**ë¶„ì„ ì™„ë£Œ:** {analysis.analyzed_at}")

        return "\n".join(lines)

    except ValueError as e:
        # ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¶„ì„ ì‹¤íŒ¨
        error_msg = str(e)
        if "ì°¾ì„ ìˆ˜ ì—†" in error_msg or "not found" in error_msg.lower():
            return f"âŒ log_id '{log_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n1. log_idê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\n2. í•´ë‹¹ ë¡œê·¸ê°€ í”„ë¡œì íŠ¸ì— ì†í•´ ìˆëŠ”ì§€ í™•ì¸\n\nğŸ’¡ ë¡œê·¸ë¥¼ ë¨¼ì € ê²€ìƒ‰í•˜ë ¤ë©´ search_logs_by_keyword ë˜ëŠ” get_recent_errorsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        else:
            return f"âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"

    except Exception as e:
        return f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\në¡œê·¸ ë¶„ì„ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


@tool
async def analyze_request_patterns(
    project_uuid: str,
    api_path: str,
    time_hours: int = 24,
    only_errors: bool = True
) -> str:
    """
    ìš”ì²­ ë°”ë”” íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê³µí†µ ì—ëŸ¬ ì›ì¸ì„ ì°¾ìŠµë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: "ì´ APIëŠ” ì–´ë–¤ ìš”ì²­ì—ì„œ ì—ëŸ¬ë‚˜?", "íŠ¹ì • íŒŒë¼ë¯¸í„° ê°’ì´ ë¬¸ì œì¸ê°€?", "null ê°’ ë•Œë¬¸ì— ì—ëŸ¬ë‚˜ëŠ” í•„ë“œ"
    âš ï¸ 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤. request_bodyê°€ ì—†ìœ¼ë©´ ë¶„ì„ ë¶ˆê°€í•©ë‹ˆë‹¤.

    Args:
        api_path: API ê²½ë¡œ (í•„ìˆ˜, ì˜ˆ: "/api/users")
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        only_errors: ì—ëŸ¬ë§Œ ë¶„ì„ (ê¸°ë³¸ True)
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    must_clauses = [
        {"term": {"log_details.request_uri": api_path}},
        {"exists": {"field": "log_details.request_body"}},
        {"range": {"timestamp": {"gte": start_time.isoformat() + "Z", "lte": end_time.isoformat() + "Z"}}}
    ]

    if only_errors:
        must_clauses.append({"term": {"level": "ERROR"}})

    try:
        results = opensearch_client.search(
            index=index_pattern,
            body={"size": 100, "query": {"bool": {"must": must_clauses}}, "sort": [{"timestamp": "desc"}],
                  "_source": ["log_details.request_body", "log_details.http_method", "level", "message"]}
        )

        hits = results.get("hits", {}).get("hits", [])
        if not hits:
            return f"{api_path} APIì˜ ìš”ì²­ ë°ì´í„°ê°€ ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ í™•ì¸: API ê²½ë¡œ ì •í™•ì„±, request_body ë¡œê·¸ í¬í•¨ ì—¬ë¶€"

        # í•„ë“œë³„ null/ê°’ ë¶„ì„
        field_stats = {}
        for hit in hits:
            req_body = hit["_source"].get("log_details", {}).get("request_body")
            if isinstance(req_body, dict):
                for key, value in req_body.items():
                    if key not in field_stats:
                        field_stats[key] = {"total": 0, "null": 0, "values": []}
                    field_stats[key]["total"] += 1
                    if value is None or value == "":
                        field_stats[key]["null"] += 1
                    else:
                        field_stats[key]["values"].append(str(value)[:50])

        lines = [f"## ğŸ“‹ {api_path} ìš”ì²­ íŒ¨í„´ ë¶„ì„", f"**ë¶„ì„ ìƒ˜í”Œ:** {len(hits)}ê±´", ""]
        if field_stats:
            lines.extend(["### í•„ë“œë³„ í†µê³„", "", "| í•„ë“œ | ìƒ˜í”Œ ìˆ˜ | Null ë¹„ìœ¨ |", "|------|---------|-----------|"])
            for field, stats in sorted(field_stats.items()):
                null_pct = (stats["null"] / stats["total"] * 100) if stats["total"] > 0 else 0
                lines.append(f"| {field} | {stats['total']} | **{null_pct:.1f}%** |")
            lines.append("")

        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜\n")
        high_null = [f for f, s in field_stats.items() if (s["null"]/s["total"]*100) > 30]
        if high_null:
            lines.append(f"1. **Null ê²€ì¦ ì¶”ê°€:** {', '.join(high_null[:3])}")
        return "\n".join(lines)

    except Exception as e:
        return f"ìš”ì²­ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"


@tool
async def analyze_response_failures(
    project_uuid: str,
    api_path: str,
    time_hours: int = 24,
    status_code_filter: Optional[int] = None
) -> str:
    """
    ì‘ë‹µ ë°”ë””ì˜ ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: "ì´ APIëŠ” ì£¼ë¡œ ì–´ë–¤ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•´?", "validation ì—ëŸ¬ ì¢…ë¥˜", "ê°€ì¥ ë§ì€ ì‹¤íŒ¨ ì›ì¸"
    âš ï¸ 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤. response_bodyê°€ ì—†ìœ¼ë©´ ë¶„ì„ ë¶ˆê°€í•©ë‹ˆë‹¤.

    Args:
        api_path: API ê²½ë¡œ (í•„ìˆ˜)
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        status_code_filter: HTTP ìƒíƒœ ì½”ë“œ í•„í„° (ì„ íƒ)
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    must_clauses = [
        {"term": {"log_details.request_uri": api_path}},
        {"exists": {"field": "log_details.response_body"}},
        {"range": {"log_details.response_status": {"gte": 400}}},  # ì—ëŸ¬ ì‘ë‹µë§Œ
        {"range": {"timestamp": {"gte": start_time.isoformat() + "Z", "lte": end_time.isoformat() + "Z"}}}
    ]

    if status_code_filter:
        must_clauses.append({"term": {"log_details.response_status": status_code_filter}})

    try:
        results = opensearch_client.search(
            index=index_pattern,
            body={"size": 100, "query": {"bool": {"must": must_clauses}}, "sort": [{"timestamp": "desc"}],
                  "_source": ["log_details.response_body", "log_details.response_status", "timestamp"]}
        )

        hits = results.get("hits", {}).get("hits", [])
        if not hits:
            return f"{api_path} APIì˜ ì—ëŸ¬ ì‘ë‹µ ë°ì´í„°ê°€ ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ í™•ì¸: 4xx/5xx ì—ëŸ¬ ë°œìƒ ì—¬ë¶€"

        # ì—ëŸ¬ ë©”ì‹œì§€ ì§‘ê³„
        error_messages = {}
        for hit in hits:
            res_body = hit["_source"].get("log_details", {}).get("response_body")
            status = hit["_source"].get("log_details", {}).get("response_status", 0)

            error_msg = "Unknown"
            if isinstance(res_body, dict):
                # ë‹¤ì–‘í•œ ì—ëŸ¬ ë©”ì‹œì§€ í•„ë“œ íƒìƒ‰
                for key in ["error", "message", "error_message", "detail", "errorMessage"]:
                    if key in res_body:
                        error_msg = str(res_body[key])[:200]
                        break

            key = f"{status}: {error_msg}"
            error_messages[key] = error_messages.get(key, 0) + 1

        lines = [f"## ğŸ”´ {api_path} ì‹¤íŒ¨ ì‘ë‹µ ë¶„ì„", f"**ë¶„ì„ ìƒ˜í”Œ:** {len(hits)}ê±´", ""]
        if error_messages:
            lines.extend(["### ì—ëŸ¬ ë©”ì‹œì§€ ë¹ˆë„", "", "| ì—ëŸ¬ ë©”ì‹œì§€ | ë¹ˆë„ |", "|-------------|------|"])
            for msg, count in sorted(error_messages.items(), key=lambda x: x[1], reverse=True)[:15]:
                lines.append(f"| {msg[:80]} | **{count}ê±´** |")
            lines.append("")

        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜\n")
        if error_messages:
            top_error = max(error_messages.items(), key=lambda x: x[1])
            lines.append(f"1. **ê°€ì¥ ë§ì€ ì—ëŸ¬ ìˆ˜ì •:** {top_error[0][:100]} ({top_error[1]}ê±´)")
        return "\n".join(lines)

    except Exception as e:
        return f"ì‘ë‹µ ì‹¤íŒ¨ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"
