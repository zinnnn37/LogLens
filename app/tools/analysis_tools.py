"""
ë¶„ì„ ë„êµ¬ (Analysis Tools)
- ë¡œê·¸ í†µê³„, ìµœê·¼ ì—ëŸ¬ ë¶„ì„
"""

import re
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


def extract_exception_type(source: Dict[str, Any]) -> str:
    """
    ë¡œê·¸ ë°ì´í„°ì—ì„œ ì˜ˆì™¸ íƒ€ì…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„:
    1. log_details.exception_type (nested í•„ë“œ)
    2. ai_analysis.tags (AIê°€ ë¶„ì„í•œ íƒœê·¸)
    3. messageì—ì„œ ì •ê·œì‹ ì¶”ì¶œ
    4. "Unknown" ë°˜í™˜
    """
    # 1. log_details.exception_type ìš°ì„ 
    log_details = source.get("log_details", {})
    exc_type = log_details.get("exception_type")
    if exc_type and exc_type != "Unknown" and exc_type.strip():
        return exc_type

    # 2. AI ë¶„ì„ íƒœê·¸ì—ì„œ ì¶”ì¶œ
    ai_analysis = source.get("ai_analysis", {})
    ai_tags = ai_analysis.get("tags", [])
    if ai_tags:
        for tag in ai_tags:
            if "Exception" in tag or "Error" in tag or "Timeout" in tag:
                return tag

    # 3. messageì—ì„œ ì •ê·œì‹ ì¶”ì¶œ
    message = source.get("message", "")
    patterns = [
        r'([A-Z][a-zA-Z]*Exception)',
        r'([A-Z][a-zA-Z]*Error)',
        r'(DatabaseTimeout|ConnectionRefused|ConnectionPoolExhausted|PoolExhausted|Timeout)',
    ]
    for pattern in patterns:
        match = re.search(pattern, message)
        if match:
            return match.group(1)

    return "Unknown"


def assess_severity(source: Dict[str, Any]) -> int:
    """
    ì—ëŸ¬ ì‹¬ê°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

    Returns:
        1: CRITICAL (ìµœê³  ì‹¬ê°ë„) - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
        2: HIGH (ë†’ìŒ) - ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”
        3: MEDIUM (ì¤‘ê°„) - ìš°ì„  ì¡°ì¹˜ í•„ìš”
        4: LOW (ë‚®ìŒ) - ëª¨ë‹ˆí„°ë§ í•„ìš”
        5: MINIMAL (ìµœì†Œ) - ì •ë³´ì„±
    """
    exc_type = extract_exception_type(source)
    log_details = source.get("log_details", {})
    response_status = log_details.get("response_status", 0)

    # AI ë¶„ì„ì´ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
    ai_analysis = source.get("ai_analysis", {})
    analysis_type = ai_analysis.get("analysis_type", "").upper()
    if analysis_type == "CRITICAL":
        return 1
    elif analysis_type == "HIGH":
        return 2
    elif analysis_type == "MEDIUM":
        return 3

    # Database/Connection ì—ëŸ¬ = CRITICAL (ëª¨ë“  ì‚¬ìš©ì ì˜í–¥)
    critical_keywords = [
        "Database", "Connection", "Pool", "Timeout",
        "OutOfMemory", "StackOverflow", "Deadlock"
    ]
    if any(keyword in exc_type for keyword in critical_keywords):
        return 1

    # 5xx ì—ëŸ¬ = HIGH (ì„œë²„ ì˜¤ë¥˜)
    if 500 <= response_status < 600:
        return 2

    # Security/Auth ì—ëŸ¬ = HIGH (ë³´ì•ˆ ìœ„í—˜)
    security_keywords = ["Auth", "Security", "Unauthorized", "Token", "Permission"]
    if any(keyword in exc_type for keyword in security_keywords):
        return 2

    # NullPointer, Runtime = MEDIUM (íŠ¹ì • ê¸°ëŠ¥ ì˜í–¥)
    medium_keywords = ["NullPointer", "Runtime", "IllegalState", "IllegalArgument"]
    if any(keyword in exc_type for keyword in medium_keywords):
        return 3

    # 4xx ì—ëŸ¬ = LOW (í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜)
    if 400 <= response_status < 500:
        return 4

    # ê¸°íƒ€ = MINIMAL
    return 5


@tool
async def get_log_statistics(
    project_uuid: str,
    time_hours: int = 24,
    group_by: str = "level"
) -> str:
    """
    ë¡œê·¸ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸ (ì˜ˆ: "ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½í•´ì¤˜")
    - íŠ¹ì • ê¸°ê°„ì˜ ë¡œê·¸ ë¶„í¬ í™•ì¸ (ì˜ˆ: "ì˜¤ëŠ˜ ì—ëŸ¬ê°€ ëª‡ ê°œì•¼?")
    - ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ í˜„í™© íŒŒì•…

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: í†µê³„ ê¸°ê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)
        group_by: ì§‘ê³„ ê¸°ì¤€ (level, service_name, source_type ì¤‘ í•˜ë‚˜, ê¸°ë³¸ level)

    ì°¸ê³ :
    - project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ "ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤" ì‘ë‹µì€ ìœ íš¨í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ì‹œë„í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        í†µê³„ ì •ë³´ (ë ˆë²¨ë³„/ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ê°œìˆ˜, ì—ëŸ¬ìœ¨ ë“±)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # Aggregation ì¿¼ë¦¬
    query_body = {
        "query": {
            "range": {
                "timestamp": {
                    "gte": time_range["start"],
                    "lte": time_range["end"]
                }
            }
        },
        "size": 0,  # í†µê³„ë§Œ í•„ìš” (ë¬¸ì„œëŠ” ë¶ˆí•„ìš”)
        "aggs": {
            "by_level": {
                "terms": {
                    "field": "level",
                    "size": 10
                }
            },
            "by_service": {
                "terms": {
                    "field": "service_name",
                    "size": 10
                }
            },
            "by_source": {
                "terms": {
                    "field": "source_type",
                    "size": 10
                }
            },
            "timeline": {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": f"{max(1, time_hours // 24)}h"  # ì‹œê°„ëŒ€ë³„
                }
            }
        }
    }

    try:
        # OpenSearch Aggregation ì‹¤í–‰ (sync client)
        results = opensearch_client.search(
            index=index_pattern,
            body=query_body
        )

        # ì „ì²´ ë¡œê·¸ ê°œìˆ˜
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggs = results.get("aggregations", {})

        # ë ˆë²¨ë³„ í†µê³„
        level_buckets = aggs.get("by_level", {}).get("buckets", [])
        level_stats = {bucket["key"]: bucket["doc_count"] for bucket in level_buckets}

        # ì„œë¹„ìŠ¤ë³„ í†µê³„
        service_buckets = aggs.get("by_service", {}).get("buckets", [])
        service_stats = {bucket["key"]: bucket["doc_count"] for bucket in service_buckets}

        # ì†ŒìŠ¤ë³„ í†µê³„
        source_buckets = aggs.get("by_source", {}).get("buckets", [])
        source_stats = {bucket["key"]: bucket["doc_count"] for bucket in source_buckets}

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ë¡œê·¸ í†µê³„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì´ ë¡œê·¸ ê°œìˆ˜: {total_count}ê±´",
            ""
        ]

        # ë ˆë²¨ë³„
        if level_stats:
            summary_lines.append("ğŸ“Š ë ˆë²¨ë³„ ë¶„í¬:")
            for level, count in sorted(level_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {level}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì—ëŸ¬ìœ¨
        error_count = level_stats.get("ERROR", 0)
        if error_count > 0:
            error_rate = (error_count / total_count) * 100
            summary_lines.append(f"âš ï¸  ì—ëŸ¬ìœ¨: {error_rate:.2f}% ({error_count}ê±´)")
            summary_lines.append("")

        # ì„œë¹„ìŠ¤ë³„
        if service_stats and group_by in ["service_name", "all"]:
            summary_lines.append("ğŸ”§ ì„œë¹„ìŠ¤ë³„ ë¡œê·¸:")
            for service, count in sorted(service_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {service}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì†ŒìŠ¤ë³„
        if source_stats and group_by in ["source_type", "all"]:
            summary_lines.append("ğŸ“ ì†ŒìŠ¤ë³„ ë¡œê·¸:")
            for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
                summary_lines.append(f"  - {source}: {count}ê±´")
            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_recent_errors(
    project_uuid: str,
    limit: int = 10,
    service_name: Optional[str] = None,
    time_hours: int = 24
) -> str:
    """
    ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ìµœê·¼ ë°œìƒí•œ ì—ëŸ¬ í™•ì¸ (ì˜ˆ: "ìµœê·¼ ì—ëŸ¬ê°€ ë­ì•¼?")
    - íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ì—ëŸ¬ë§Œ ì¡°íšŒ (ì˜ˆ: "user-service ì—ëŸ¬ ë³´ì—¬ì¤˜")
    - ì—ëŸ¬ ë°œìƒ ë¹ˆë„ íŒŒì•…

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        limit: ì¡°íšŒí•  ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        service_name: íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¡°íšŒ (ì„ íƒ)
        time_hours: ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)

    ì°¸ê³ :
    - project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ "ERROR ë ˆë²¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤" ì‘ë‹µì€ ìœ íš¨í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ì‹œë„í•˜ì§€ ë§ˆì„¸ìš”.
    - âš ï¸ ì „ì²´ ì—ëŸ¬ í˜„í™©ì„ íŒŒì•…í•˜ë ¤ë©´ service_name í•„í„° ì—†ì´ ë¨¼ì € ì¡°íšŒí•˜ì„¸ìš”. ì„œë¹„ìŠ¤ë³„ ìƒì„¸ ë¶„ì„ì€ ê·¸ ë‹¤ìŒì— í•„ìš”ì‹œì—ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”.

    Returns:
        ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (ì‹œê°„ ì—­ìˆœ)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    query = {
        "bool": {
            "must": must_clauses,
            "filter": [
                {
                    "range": {
                        "timestamp": {
                            "gte": time_range["start"],
                            "lte": time_range["end"]
                        }
                    }
                }
            ]
        }
    }

    try:
        # OpenSearch ê²€ìƒ‰ (sync client)
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "query": query,
                "size": limit,
                "sort": [{"timestamp": "desc"}],
                "_source": [
                    "message", "level", "service_name", "timestamp", "log_id",
                    "stacktrace",  # í•„ë“œëª… ìˆ˜ì • (stack_trace -> stacktrace)
                    "layer", "component_name",
                    # Nested fields (log_details)
                    "log_details.exception_type",
                    "log_details.class_name",
                    "log_details.method_name",
                    "log_details.http_method",
                    "log_details.request_uri",
                    "log_details.response_status",
                    "log_details.execution_time",
                    "log_details.stacktrace",
                    # AI analysis fields
                    "ai_analysis.summary",
                    "ai_analysis.error_cause",
                    "ai_analysis.solution",
                    "ai_analysis.tags",
                    "ai_analysis.analysis_type"
                ]
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            service_filter = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë ˆë²¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤{service_filter}."

        # ì—ëŸ¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
        error_types = {}
        errors_with_severity = []  # (hit, severity) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        for hit in hits:
            source = hit["_source"]
            exc_type = extract_exception_type(source)
            severity = assess_severity(source)
            error_types[exc_type] = error_types.get(exc_type, 0) + 1
            errors_with_severity.append((hit, severity))

        # ì‹¬ê°ë„ìˆœ ì •ë ¬ (ë‚®ì€ ìˆ«ì = ë†’ì€ ì‹¬ê°ë„)
        errors_with_severity.sort(key=lambda x: (x[1], x[0]["_source"].get("timestamp", "")), reverse=True)

        # ê²°ê³¼ í¬ë§·íŒ…
        service_filter_str = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
        summary_lines = [
            f"=== ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (ìµœê·¼ {time_hours}ì‹œê°„){service_filter_str} ===",
            f"ìƒìœ„ {len(hits)}ê±´ í‘œì‹œ",  # "ì´ Xê±´" ì œê±° - LLM í˜¼ë€ ë°©ì§€
            ""
        ]

        # ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬
        if error_types:
            summary_lines.append("ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬:")
            for exc_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                summary_lines.append(f"  - {exc_type}: {count}ê±´")
            summary_lines.append("")

        # ìƒìœ„ ì—ëŸ¬ ëª©ë¡ (ì‹¬ê°ë„ìˆœ)
        summary_lines.append("ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (ì‹¬ê°ë„ìˆœ):")
        severity_labels = {1: "CRITICAL", 2: "HIGH", 3: "MEDIUM", 4: "LOW", 5: "MINIMAL"}

        for i, (hit, severity) in enumerate(errors_with_severity, 1):
            source = hit["_source"]
            msg = source.get("message", "")[:200]  # 400 â†’ 200ìë¡œ ì¶•ì†Œ (ì»¨í…ìŠ¤íŠ¸ ë¶€ë‹´ ê°ì†Œ)
            timestamp_str = source.get("timestamp", "")[:19]
            service = source.get("service_name", "unknown")
            log_id = source.get("log_id", "")
            layer = source.get("layer", "")
            component = source.get("component_name", "")

            # ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ
            exc_type = extract_exception_type(source)

            # log_details ì ‘ê·¼
            log_details = source.get("log_details", {})
            class_name = log_details.get("class_name", "")
            method_name = log_details.get("method_name", "")
            http_method = log_details.get("http_method", "")
            request_uri = log_details.get("request_uri", "")
            response_status = log_details.get("response_status")
            execution_time = log_details.get("execution_time")

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€
            has_stack = bool(source.get("stacktrace") or log_details.get("stacktrace"))

            # AI ë¶„ì„ ê²°ê³¼
            ai_analysis = source.get("ai_analysis", {})
            ai_summary = ai_analysis.get("summary", "")
            ai_cause = ai_analysis.get("error_cause", "")
            ai_solution = ai_analysis.get("solution", "")

            # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            severity_label = severity_labels.get(severity, "UNKNOWN")
            summary_lines.append(f"{i}. [{exc_type}] {timestamp_str} | ì‹¬ê°ë„: {severity_label}")
            summary_lines.append(f"   ì„œë¹„ìŠ¤: {service}")

            # ë ˆì´ì–´/ì»´í¬ë„ŒíŠ¸
            if layer or component:
                loc_info = []
                if layer:
                    loc_info.append(f"Layer: {layer}")
                if component:
                    loc_info.append(f"Component: {component}")
                summary_lines.append(f"   ìœ„ì¹˜: {', '.join(loc_info)}")

            # í´ë˜ìŠ¤/ë©”ì„œë“œ
            if class_name and method_name:
                summary_lines.append(f"   ğŸ“ {class_name}.{method_name}")
            elif class_name:
                summary_lines.append(f"   ğŸ“ {class_name}")

            # HTTP ì •ë³´
            if http_method and request_uri:
                status_info = f" â†’ {response_status}" if response_status else ""
                summary_lines.append(f"   ğŸŒ {http_method} {request_uri}{status_info}")
            elif response_status:
                summary_lines.append(f"   ğŸ“Š HTTP {response_status}")

            # ì‹¤í–‰ ì‹œê°„
            if execution_time:
                summary_lines.append(f"   â±ï¸  {execution_time}ms")

            # ë©”ì‹œì§€
            summary_lines.append(f"   ğŸ’¬ {msg}...")

            # AI ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°) - ì¶•ì†Œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë¶€ë‹´ ê°ì†Œ
            if ai_summary:
                summary_lines.append(f"   ğŸ¤– AI: {ai_summary[:100]}")  # 200 â†’ 100ì
            if ai_cause:
                summary_lines.append(f"   ğŸ“Œ ì›ì¸: {ai_cause[:80]}")  # 150 â†’ 80ì
            if ai_solution:
                summary_lines.append(f"   ğŸ’¡ í•´ê²°: {ai_solution[:80]}")  # 150 â†’ 80ì

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì—¬ë¶€
            if has_stack:
                summary_lines.append(f"   ğŸ“š (ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìˆìŒ)")

            # log_id
            if log_id:
                summary_lines.append(f"   (log_id: {log_id})")

            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def correlate_logs(
    project_uuid: str,
    log_id: int,
    correlation_type: str = "trace",
    time_window_minutes: int = 5,
    limit: int = 20
) -> str:
    """
    íŠ¹ì • ë¡œê·¸ì™€ ì—°ê´€ëœ ë‹¤ë¥¸ ë¡œê·¸ë“¤ì„ ì°¾ì•„ ìƒê´€ê´€ê³„ ë¶„ì„

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        log_id: ê¸°ì¤€ ë¡œê·¸ ID
        correlation_type: ìƒê´€ê´€ê³„ ìœ í˜• (trace|error_type|time_window)
            - trace: ê°™ì€ trace_idë¥¼ ê°€ì§„ ë¡œê·¸ë“¤ (ë¶„ì‚° ì¶”ì )
            - error_type: ê°™ì€ ì—ëŸ¬ íƒ€ì… ë°œìƒ ë¡œê·¸ë“¤
            - time_window: ì‹œê°„ëŒ€ ê¸°ë°˜ (Â±Në¶„ ì´ë‚´)
        time_window_minutes: time_window ì‚¬ìš© ì‹œ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ 5ë¶„)
        limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        ì—°ê´€ ë¡œê·¸ ëª©ë¡ ë° ë¶„ì„

    Examples:
        - "ì´ NullPointerExceptionì˜ ê·¼ë³¸ ì›ì¸ì€?"
        - "log_id 12345ì™€ ì—°ê´€ëœ ë¡œê·¸ ì¶”ì "
    """
    try:
        from app.db.opensearch import get_opensearch_client
        from datetime import datetime, timedelta

        client = get_opensearch_client()
        index_name = f"logs_{project_uuid}"

        # 1. ê¸°ì¤€ ë¡œê·¸ ì¡°íšŒ
        try:
            base_log = client.get(index=index_name, id=str(log_id))
            base_source = base_log['_source']
        except Exception:
            return f"ë¡œê·¸ ID {log_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        summary_lines = [f"=== ë¡œê·¸ ìƒê´€ê´€ê³„ ë¶„ì„ (log_id: {log_id}) ===", ""]

        # ê¸°ì¤€ ë¡œê·¸ ì •ë³´
        base_timestamp = base_source.get('timestamp')
        base_service = base_source.get('service_name', 'unknown')
        base_level = base_source.get('level', 'INFO')
        base_message = base_source.get('message', '')[:150]

        summary_lines.append("**ê¸°ì¤€ ë¡œê·¸:**")
        summary_lines.append(f"- ì‹œê°„: {base_timestamp}")
        summary_lines.append(f"- ì„œë¹„ìŠ¤: {base_service}")
        summary_lines.append(f"- ë ˆë²¨: {base_level}")
        summary_lines.append(f"- ë©”ì‹œì§€: {base_message}")
        summary_lines.append("")

        # 2. ìƒê´€ê´€ê³„ íƒ€ì…ë³„ ê²€ìƒ‰
        query = None

        if correlation_type == "trace":
            # trace_id ê¸°ë°˜ (EC2: top-level field)
            trace_id = base_source.get('trace_id')
            if not trace_id:
                return f"ê¸°ì¤€ ë¡œê·¸ì— trace_idê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ correlation_typeì„ ì‚¬ìš©í•˜ì„¸ìš”."

            query = {
                "bool": {
                    "must": [
                        {"term": {"trace_id.keyword": trace_id}}
                    ],
                    "must_not": [
                        {"term": {"_id": str(log_id)}}  # ìê¸° ìì‹  ì œì™¸
                    ]
                }
            }
            summary_lines.append(f"**ìƒê´€ê´€ê³„ ìœ í˜•:** trace_id ì¶”ì  ({trace_id})")

        elif correlation_type == "error_type":
            # ê°™ì€ ì—ëŸ¬ íƒ€ì… (EC2: log_details.exception_type)
            error_type = base_source.get('log_details', {}).get('exception_type') or \
                        base_message.split(':')[0] if ':' in base_message else None

            if not error_type:
                return "ê¸°ì¤€ ë¡œê·¸ì—ì„œ ì—ëŸ¬ íƒ€ì…ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            query = {
                "bool": {
                    "should": [
                        {"match": {"message": error_type}},
                        {"term": {"log_details.exception_type": error_type}}
                    ],
                    "minimum_should_match": 1,
                    "must_not": [
                        {"term": {"_id": str(log_id)}}
                    ]
                }
            }
            summary_lines.append(f"**ìƒê´€ê´€ê³„ ìœ í˜•:** ê°™ì€ ì—ëŸ¬ íƒ€ì… ({error_type})")

        elif correlation_type == "time_window":
            # ì‹œê°„ëŒ€ ê¸°ë°˜ (Â±Në¶„)
            if not base_timestamp:
                return "ê¸°ì¤€ ë¡œê·¸ì— timestampê°€ ì—†ìŠµë‹ˆë‹¤."

            base_dt = datetime.fromisoformat(base_timestamp.replace('Z', '+00:00'))
            start_time = base_dt - timedelta(minutes=time_window_minutes)
            end_time = base_dt + timedelta(minutes=time_window_minutes)

            query = {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "timestamp": {
                                    "gte": start_time.isoformat(),
                                    "lte": end_time.isoformat()
                                }
                            }
                        }
                    ],
                    "must_not": [
                        {"term": {"_id": str(log_id)}}
                    ]
                }
            }
            summary_lines.append(f"**ìƒê´€ê´€ê³„ ìœ í˜•:** ì‹œê°„ ë²”ìœ„ (Â±{time_window_minutes}ë¶„)")

        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” correlation_type: {correlation_type}"

        # 3. ê²€ìƒ‰ ì‹¤í–‰
        response = client.search(
            index=index_name,
            body={
                "query": query,
                "sort": [{"timestamp": "asc"}],
                "size": limit
            }
        )

        hits = response['hits']['hits']
        total = response['hits']['total']['value']

        if total == 0:
            summary_lines.append("")
            summary_lines.append("ì—°ê´€ëœ ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return "\n".join(summary_lines)

        summary_lines.append(f"**ë°œê²¬:** ì´ {total}ê±´ ì¤‘ {len(hits)}ê±´ í‘œì‹œ")
        summary_lines.append("")

        # 4. ì—°ê´€ ë¡œê·¸ ëª©ë¡
        summary_lines.append("**ì—°ê´€ ë¡œê·¸ ëª©ë¡ (ì‹œê°„ìˆœ):**")
        summary_lines.append("")

        for i, hit in enumerate(hits, 1):
            source = hit['_source']
            related_id = hit.get('_id')
            timestamp = source.get('timestamp', 'N/A')
            level = source.get('level', 'INFO')
            service = source.get('service_name', 'unknown')
            message = source.get('message', '')[:200]

            summary_lines.append(f"{i}. [{level}] {timestamp}")
            summary_lines.append(f"   ğŸ”§ {service}")
            summary_lines.append(f"   ğŸ’¬ {message}")
            summary_lines.append(f"   (log_id: {related_id})")
            summary_lines.append("")

        # 5. ì¸ì‚¬ì´íŠ¸
        summary_lines.append("**ğŸ’¡ ë¶„ì„ ì¸ì‚¬ì´íŠ¸:**")
        if correlation_type == "trace":
            summary_lines.append(f"- ì´ ìš”ì²­ì€ {len(hits)+1}ê°œ ì„œë¹„ìŠ¤ë¥¼ ê±°ì³ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤")
            summary_lines.append("- trace_idë¡œ ì „ì²´ í˜¸ì¶œ íë¦„ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        elif correlation_type == "error_type":
            summary_lines.append(f"- ê°™ì€ ì—ëŸ¬ê°€ ì´ {total+1}ê±´ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
            summary_lines.append("- ì¬í˜„ ê°€ëŠ¥í•œ ë²„ê·¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤")
        elif correlation_type == "time_window":
            summary_lines.append(f"- {time_window_minutes}ë¶„ ì´ë‚´ {total}ê±´ì˜ ë¡œê·¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
            summary_lines.append("- ë™ì‹œë‹¤ë°œì  ë¬¸ì œ ë˜ëŠ” ì—°ì‡„ ì¥ì•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def analyze_errors_unified(
    project_uuid: str,
    group_by: str = "severity",
    sort_by: str = "severity",
    time_hours: int = 24,
    limit: int = 10,
    service_name: Optional[str] = None
) -> str:
    """
    í†µí•© ì—ëŸ¬ ë¶„ì„ ë„êµ¬ (ê¸°ì¡´ 3ê°œ ë„êµ¬ í†µí•©)

    í•˜ë‚˜ì˜ ë„êµ¬ë¡œ ë‹¤ì–‘í•œ ì—ëŸ¬ ë¶„ì„ ìˆ˜í–‰ (ì‹¬ê°ë„ë³„/ë¹ˆë„ë³„/APIë³„)

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        group_by: ê·¸ë£¹í•‘ ê¸°ì¤€
            - severity: ì‹¬ê°ë„ë³„ ê·¸ë£¹í•‘ (CRITICAL > HIGH > MEDIUM)
            - error_type: ì—ëŸ¬ íƒ€ì…ë³„ (NullPointerException ë“±)
            - service: ì„œë¹„ìŠ¤ë³„
            - api: API ì—”ë“œí¬ì¸íŠ¸ë³„
        sort_by: ì •ë ¬ ê¸°ì¤€
            - severity: ì‹¬ê°ë„ìˆœ (CRITICAL ìš°ì„ )
            - frequency: ë°œìƒ ë¹ˆë„ìˆœ
            - time: ìµœê·¼ ë°œìƒ ì‹œê°„ìˆœ
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        service_name: íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ë¶„ì„ (ì„ íƒ)

    Returns:
        ê·¸ë£¹í•‘/ì •ë ¬ëœ ì—ëŸ¬ ë¶„ì„ ê²°ê³¼

    Examples:
        - "ê°€ì¥ ì‹¬ê°í•œ ì—ëŸ¬" â†’ group_by=severity, sort_by=severity
        - "ê°€ì¥ ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬" â†’ group_by=error_type, sort_by=frequency
        - "ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ í˜„í™©" â†’ group_by=service
        - "APIë³„ ì—ëŸ¬ìœ¨" â†’ group_by=api
    """
    # ê¸°ì¡´ ë„êµ¬ ì¬ì‚¬ìš©
    if group_by == "severity" and sort_by == "severity":
        # get_recent_errors ë¡œì§
        return await get_recent_errors.ainvoke({
            "project_uuid": project_uuid,
            "limit": limit,
            "service_name": service_name,
            "time_hours": time_hours
        })
    elif group_by == "error_type":
        # get_error_frequency_ranking ë¡œì§
        from app.tools.monitoring_tools import get_error_frequency_ranking
        return await get_error_frequency_ranking.ainvoke({
            "project_uuid": project_uuid,
            "time_hours": time_hours,
            "limit": limit,
            "service_name": service_name
        })
    elif group_by == "api":
        # get_api_error_rates ë¡œì§
        from app.tools.monitoring_tools import get_api_error_rates
        return await get_api_error_rates.ainvoke({
            "project_uuid": project_uuid,
            "time_hours": time_hours,
            "limit": limit
        })
    elif group_by == "service":
        # get_service_health_status ë¡œì§
        from app.tools.monitoring_tools import get_service_health_status
        return await get_service_health_status.ainvoke({
            "project_uuid": project_uuid,
            "time_hours": time_hours,
            "service_name": service_name
        })
    else:
        return f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¡°í•©: group_by={group_by}, sort_by={sort_by}\n\nì§€ì›ë˜ëŠ” ì˜µì…˜:\n- group_by: severity|error_type|service|api\n- sort_by: severity|frequency|time"
