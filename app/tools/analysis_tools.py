"""
ë¶„ì„ ë„êµ¬ (Analysis Tools)
- ë¡œê·¸ í†µê³„, ìµœê·¼ ì—ëŸ¬ ë¶„ì„
"""

from typing import Optional
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


@tool
async def get_log_statistics(
    project_uuid: str,
    time_hours: int = 24,
    group_by: str = "level"
) -> str:
    """
    ë¡œê·¸ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸ (ì˜ˆ: "ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½í•´ì¤˜")
    - íŠ¹ì • ê¸°ê°„ì˜ ë¡œê·¸ ë¶„í¬ í™•ì¸ (ì˜ˆ: "ì˜¤ëŠ˜ ì—ëŸ¬ê°€ ëª‡ ê°œì•¼?")
    - ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ í˜„í™© íŒŒì•…

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID (ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹)
        time_hours: í†µê³„ ê¸°ê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)
        group_by: ì§‘ê³„ ê¸°ì¤€ (level, service_name, source_type ì¤‘ í•˜ë‚˜, ê¸°ë³¸ level)

    Returns:
        í†µê³„ ì •ë³´ (ë ˆë²¨ë³„/ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ê°œìˆ˜, ì—ëŸ¬ìœ¨ ë“±)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid}_*"

    # Aggregation ì¿¼ë¦¬
    query_body = {
        "query": {
            "range": {
                "timestamp": {
                    "gte": time_range["start"],
                    "lte": time_range["end"]
                }
            }
        },
        "size": 0,  # í†µê³„ë§Œ í•„ìš” (ë¬¸ì„œëŠ” ë¶ˆí•„ìš”)
        "aggs": {
            "by_level": {
                "terms": {
                    "field": "level",
                    "size": 10
                }
            },
            "by_service": {
                "terms": {
                    "field": "service_name",
                    "size": 10
                }
            },
            "by_source": {
                "terms": {
                    "field": "source_type",
                    "size": 10
                }
            },
            "timeline": {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": f"{max(1, time_hours // 24)}h"  # ì‹œê°„ëŒ€ë³„
                }
            }
        }
    }

    try:
        # OpenSearch Aggregation ì‹¤í–‰
        results = await opensearch_client.search(
            index=index_pattern,
            body=query_body
        )

        # ì „ì²´ ë¡œê·¸ ê°œìˆ˜
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggs = results.get("aggregations", {})

        # ë ˆë²¨ë³„ í†µê³„
        level_buckets = aggs.get("by_level", {}).get("buckets", [])
        level_stats = {bucket["key"]: bucket["doc_count"] for bucket in level_buckets}

        # ì„œë¹„ìŠ¤ë³„ í†µê³„
        service_buckets = aggs.get("by_service", {}).get("buckets", [])
        service_stats = {bucket["key"]: bucket["doc_count"] for bucket in service_buckets}

        # ì†ŒìŠ¤ë³„ í†µê³„
        source_buckets = aggs.get("by_source", {}).get("buckets", [])
        source_stats = {bucket["key"]: bucket["doc_count"] for bucket in source_buckets}

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ë¡œê·¸ í†µê³„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì´ ë¡œê·¸ ê°œìˆ˜: {total_count}ê±´",
            ""
        ]

        # ë ˆë²¨ë³„
        if level_stats:
            summary_lines.append("ğŸ“Š ë ˆë²¨ë³„ ë¶„í¬:")
            for level, count in sorted(level_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {level}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì—ëŸ¬ìœ¨
        error_count = level_stats.get("ERROR", 0)
        if error_count > 0:
            error_rate = (error_count / total_count) * 100
            summary_lines.append(f"âš ï¸  ì—ëŸ¬ìœ¨: {error_rate:.2f}% ({error_count}ê±´)")
            summary_lines.append("")

        # ì„œë¹„ìŠ¤ë³„
        if service_stats and group_by in ["service_name", "all"]:
            summary_lines.append("ğŸ”§ ì„œë¹„ìŠ¤ë³„ ë¡œê·¸:")
            for service, count in sorted(service_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = (count / total_count) * 100
                summary_lines.append(f"  - {service}: {count}ê±´ ({percentage:.1f}%)")
            summary_lines.append("")

        # ì†ŒìŠ¤ë³„
        if source_stats and group_by in ["source_type", "all"]:
            summary_lines.append("ğŸ“ ì†ŒìŠ¤ë³„ ë¡œê·¸:")
            for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
                summary_lines.append(f"  - {source}: {count}ê±´")
            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_recent_errors(
    project_uuid: str,
    limit: int = 10,
    service_name: Optional[str] = None,
    time_hours: int = 24
) -> str:
    """
    ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ìµœê·¼ ë°œìƒí•œ ì—ëŸ¬ í™•ì¸ (ì˜ˆ: "ìµœê·¼ ì—ëŸ¬ê°€ ë­ì•¼?")
    - íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ì—ëŸ¬ë§Œ ì¡°íšŒ (ì˜ˆ: "user-service ì—ëŸ¬ ë³´ì—¬ì¤˜")
    - ì—ëŸ¬ ë°œìƒ ë¹ˆë„ íŒŒì•…

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID (ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹)
        limit: ì¡°íšŒí•  ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        service_name: íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¡°íšŒ (ì„ íƒ)
        time_hours: ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)

    Returns:
        ìµœê·¼ ì—ëŸ¬ ëª©ë¡ (ì‹œê°„ ì—­ìˆœ)
    """
    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    time_range = {
        "start": start_time.isoformat() + "Z",
        "end": end_time.isoformat() + "Z"
    }

    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid}_*"

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    query = {
        "bool": {
            "must": must_clauses,
            "filter": [
                {
                    "range": {
                        "timestamp": {
                            "gte": time_range["start"],
                            "lte": time_range["end"]
                        }
                    }
                }
            ]
        }
    }

    try:
        # OpenSearch ê²€ìƒ‰
        results = await opensearch_client.search(
            index=index_pattern,
            body={
                "query": query,
                "size": limit,
                "sort": [{"timestamp": "desc"}],
                "_source": [
                    "message", "level", "service_name", "timestamp",
                    "log_id", "exception_type", "stack_trace"
                ]
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            service_filter = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë ˆë²¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤{service_filter}."

        # ì—ëŸ¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸
        error_types = {}
        for hit in hits:
            exc_type = hit["_source"].get("exception_type", "Unknown")
            error_types[exc_type] = error_types.get(exc_type, 0) + 1

        # ê²°ê³¼ í¬ë§·íŒ…
        service_filter_str = f" (ì„œë¹„ìŠ¤: {service_name})" if service_name else ""
        summary_lines = [
            f"=== ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (ìµœê·¼ {time_hours}ì‹œê°„){service_filter_str} ===",
            f"ì´ {total_count}ê±´ì˜ ì—ëŸ¬ ë°œìƒ, ìƒìœ„ {len(hits)}ê±´ í‘œì‹œ",
            ""
        ]

        # ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬
        if error_types:
            summary_lines.append("ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬:")
            for exc_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                summary_lines.append(f"  - {exc_type}: {count}ê±´")
            summary_lines.append("")

        # ìƒìœ„ ì—ëŸ¬ ëª©ë¡
        summary_lines.append("ìµœê·¼ ì—ëŸ¬ ëª©ë¡:")
        for i, hit in enumerate(hits, 1):
            source = hit["_source"]
            msg = source.get("message", "")[:150]
            timestamp_str = source.get("timestamp", "")[:19]
            service = source.get("service_name", "unknown")
            log_id = source.get("log_id", "")
            exc_type = source.get("exception_type", "Unknown")
            has_stack = bool(source.get("stack_trace"))

            summary_lines.append(f"{i}. [{exc_type}] {timestamp_str}")
            summary_lines.append(f"   ì„œë¹„ìŠ¤: {service}")
            summary_lines.append(f"   ë©”ì‹œì§€: {msg}...")
            if has_stack:
                summary_lines.append(f"   (ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìˆìŒ)")
            if log_id:
                summary_lines.append(f"   (log_id: {log_id})")
            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
