"""
AI vs DB í†µê³„ ë¹„êµ ë„êµ¬ (Statistics Comparison Tools)
- DB ì§ì ‘ ì¡°íšŒ ê²°ê³¼ì™€ LLM ê¸°ë°˜ í†µê³„ ì¶”ë¡ ì˜ ì •í™•ë„ë¥¼ ê²€ì¦
- LLMì´ DB ì¿¼ë¦¬ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ìˆ˜ì¹˜ë¡œ ì¦ëª…
"""

import asyncio
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

from app.core.opensearch import opensearch_client
from app.core.config import settings
from app.tools.sampling_strategies import sample_stratified_vector_knn, sample_random_stratified

logger = logging.getLogger(__name__)


def _get_db_statistics(project_uuid: str, time_hours: int = 24) -> Dict[str, Any]:
    """
    OpenSearchì—ì„œ ì§ì ‘ í†µê³„ ì¿¼ë¦¬ ì‹¤í–‰ (Ground Truth)
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    query_body = {
        "track_total_hits": True,  # 10,000ê±´ ì œí•œ í•´ì œ - ì •í™•í•œ ì´ ê°œìˆ˜ ë°˜í™˜
        "size": 0,
        "query": {
            "range": {
                "timestamp": {
                    "gte": start_time.isoformat() + "Z",
                    "lte": end_time.isoformat() + "Z"
                }
            }
        },
        "aggs": {
            "by_level": {
                "terms": {"field": "level", "size": 10}
            },
            "hourly_distribution": {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": "1h",
                    "time_zone": "Asia/Seoul",
                    "min_doc_count": 0
                },
                "aggs": {
                    "by_level": {
                        "terms": {"field": "level", "size": 5}
                    }
                }
            }
        }
    }

    results = opensearch_client.search(index=index_pattern, body=query_body)

    # ì „ì²´ í†µê³„ ì¶”ì¶œ
    total_logs = results.get("hits", {}).get("total", {}).get("value", 0)
    level_buckets = results.get("aggregations", {}).get("by_level", {}).get("buckets", [])

    level_stats = {}
    for bucket in level_buckets:
        level_stats[bucket["key"]] = bucket["doc_count"]

    error_count = level_stats.get("ERROR", 0)
    warn_count = level_stats.get("WARN", 0)
    info_count = level_stats.get("INFO", 0)
    error_rate = (error_count / total_logs * 100) if total_logs > 0 else 0

    # ì‹œê°„ëŒ€ë³„ ë¶„í¬
    hourly_buckets = results.get("aggregations", {}).get("hourly_distribution", {}).get("buckets", [])
    hourly_data = []
    peak_hour = ""
    peak_count = 0

    for bucket in hourly_buckets:
        hour_str = bucket.get("key_as_string", "")[:13]  # "2024-01-01T15"
        total = bucket.get("doc_count", 0)
        level_breakdown = {}
        for level_bucket in bucket.get("by_level", {}).get("buckets", []):
            level_breakdown[level_bucket["key"]] = level_bucket["doc_count"]

        hourly_data.append({
            "hour": hour_str,
            "total": total,
            "error": level_breakdown.get("ERROR", 0),
            "warn": level_breakdown.get("WARN", 0),
            "info": level_breakdown.get("INFO", 0)
        })

        if total > peak_count:
            peak_count = total
            peak_hour = hour_str

    return {
        "total_logs": total_logs,
        "error_count": error_count,
        "warn_count": warn_count,
        "info_count": info_count,
        "error_rate": round(error_rate, 2),
        "peak_hour": peak_hour,
        "peak_count": peak_count,
        "hourly_data": hourly_data[-24:]  # ìµœê·¼ 24ì‹œê°„ë§Œ
    }


def _get_log_samples(project_uuid: str, time_hours: int = 24, sample_size: int = 100) -> List[Dict[str, Any]]:
    """
    LLM ë¶„ì„ì„ ìœ„í•œ ë¡œê·¸ ìƒ˜í”Œ ì¶”ì¶œ (ëœë¤ ìƒ˜í”Œë§)
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # ëœë¤ ìƒ˜í”Œë§ - function_score with random_score ì‚¬ìš©
    query_body = {
        "size": sample_size,
        "query": {
            "function_score": {
                "query": {
                    "range": {
                        "timestamp": {
                            "gte": start_time.isoformat() + "Z",
                            "lte": end_time.isoformat() + "Z"
                        }
                    }
                },
                "random_score": {},  # ëœë¤ ìŠ¤ì½”ì–´ë§
                "boost_mode": "replace"  # ì›ë˜ ì ìˆ˜ ë¬´ì‹œí•˜ê³  ëœë¤ ì ìˆ˜ë§Œ ì‚¬ìš©
            }
        },
        "_source": ["level", "timestamp", "service_name", "message"]
    }

    results = opensearch_client.search(index=index_pattern, body=query_body)

    samples = []
    for hit in results.get("hits", {}).get("hits", []):
        source = hit.get("_source", {})
        samples.append({
            "level": source.get("level", "UNKNOWN"),
            "timestamp": source.get("timestamp", ""),
            "service": source.get("service_name", "unknown"),
            "message": source.get("message", "")[:200]  # í† í° ì ˆì•½
        })

    return samples


async def _get_stratified_log_samples(
    project_uuid: str,
    time_hours: int = 24,
    sample_size: int = 100,
    level_counts: Dict[str, int] = None
) -> List[Dict[str, Any]]:
    """
    Vector KNN ê¸°ë°˜ ì¸µí™” ìƒ˜í”Œë§ (embedding í™œìš©)

    í´ë°±: Vector ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ëœë¤ ìƒ˜í”Œë§
    """
    try:
        # Vector KNN ìƒ˜í”Œë§ ì‹œë„
        logger.info(f"Vector KNN ìƒ˜í”Œë§ ì‹œì‘: project={project_uuid}, size={sample_size}")

        samples = await sample_stratified_vector_knn(
            opensearch_client=opensearch_client,
            project_uuid=project_uuid,
            time_hours=time_hours,
            k=sample_size,
            k_per_level=level_counts
        )

        logger.info(f"âœ… Vector KNN ìƒ˜í”Œë§ ì™„ë£Œ: {len(samples)}ê°œ")
        return samples

    except Exception as e:
        # í´ë°±: ëœë¤ ìƒ˜í”Œë§
        logger.warning(f"âš ï¸ Vector KNN ì‹¤íŒ¨, ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ í´ë°±: {e}")
        return _get_stratified_log_samples_random(
            project_uuid, time_hours, sample_size, level_counts
        )


def _get_stratified_log_samples_random(
    project_uuid: str,
    time_hours: int = 24,
    sample_size: int = 100,
    level_counts: Dict[str, int] = None
) -> List[Dict[str, Any]]:
    """
    ëœë¤ ì¸µí™” ìƒ˜í”Œë§ (í´ë°±ìš©): ë ˆë²¨ë³„ ë¹„ìœ¨ì— ë§ê²Œ ìƒ˜í”Œ ì¶”ì¶œ
    í¬ì†Œ ì´ë²¤íŠ¸(ERROR/WARN)ë„ ë°˜ë“œì‹œ í¬í•¨
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    samples = []

    if not level_counts:
        # level_countsê°€ ì—†ìœ¼ë©´ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ í´ë°±
        return _get_log_samples(project_uuid, time_hours, sample_size)

    total_logs = sum(level_counts.values())
    if total_logs == 0:
        return []

    # ë ˆë²¨ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚° (ë¹„ìœ¨ ê¸°ë°˜ + ìµœì†Œ ë³´ì¥)
    level_sample_sizes = {}
    remaining_samples = sample_size

    for level in ["ERROR", "WARN", "INFO"]:
        actual_count = level_counts.get(level, 0)
        if actual_count == 0:
            level_sample_sizes[level] = 0
            continue

        # ë¹„ìœ¨ ê¸°ë°˜ í• ë‹¹
        ratio = actual_count / total_logs
        allocated = int(sample_size * ratio)

        # í¬ì†Œ ì´ë²¤íŠ¸ëŠ” ìµœì†Œ 1ê°œ ë³´ì¥ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        if allocated == 0 and actual_count > 0:
            allocated = min(5, actual_count)  # ìµœì†Œ 5ê°œ ë˜ëŠ” ì‹¤ì œ ê°œìˆ˜

        # ì‹¤ì œ ê°œìˆ˜ë³´ë‹¤ ë§ì´ í• ë‹¹í•˜ì§€ ì•ŠìŒ
        allocated = min(allocated, actual_count, remaining_samples)
        level_sample_sizes[level] = allocated
        remaining_samples -= allocated

    # ë‚¨ì€ ìƒ˜í”Œì€ INFOì— í• ë‹¹ (ê°€ì¥ ë§ì€ ë ˆë²¨)
    if remaining_samples > 0 and "INFO" in level_sample_sizes:
        additional = min(remaining_samples, level_counts.get("INFO", 0) - level_sample_sizes["INFO"])
        level_sample_sizes["INFO"] += max(0, additional)

    # ê° ë ˆë²¨ë³„ë¡œ ëœë¤ ìƒ˜í”Œë§
    for level, count in level_sample_sizes.items():
        if count == 0:
            continue

        query_body = {
            "size": count,
            "query": {
                "function_score": {
                    "query": {
                        "bool": {
                            "must": [
                                {
                                    "range": {
                                        "timestamp": {
                                            "gte": start_time.isoformat() + "Z",
                                            "lte": end_time.isoformat() + "Z"
                                        }
                                    }
                                },
                                {
                                    "term": {"level": level}
                                }
                            ]
                        }
                    },
                    "random_score": {},
                    "boost_mode": "replace"
                }
            },
            "_source": ["level", "timestamp", "service_name", "message"]
        }

        try:
            results = opensearch_client.search(index=index_pattern, body=query_body)

            for hit in results.get("hits", {}).get("hits", []):
                source = hit.get("_source", {})
                samples.append({
                    "level": source.get("level", "UNKNOWN"),
                    "timestamp": source.get("timestamp", ""),
                    "service": source.get("service_name", "unknown"),
                    "message": source.get("message", "")[:200]
                })
        except Exception as e:
            print(f"Warning: Failed to sample {level} logs: {e}")
            continue

    return samples


def _llm_estimate_statistics(
    log_samples: List[Dict[str, Any]],
    total_sample_count: int,
    time_hours: int,
    actual_level_counts: Optional[Dict[str, int]] = None,
    sample_metadata: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    LLMì—ê²Œ ë¡œê·¸ ìƒ˜í”Œì„ ì£¼ê³  ì „ì²´ í†µê³„ë¥¼ ê²€ì¦í•˜ê²Œ í•¨ (íŒíŠ¸ ê¸°ë°˜)

    Args:
        log_samples: ì¸µí™” ìƒ˜í”Œë§ëœ ë¡œê·¸ ëª©ë¡
        total_sample_count: ì‹¤ì œ ì´ ë¡œê·¸ ìˆ˜
        time_hours: ë¶„ì„ ê¸°ê°„ (ì‹œê°„)
        actual_level_counts: DBì—ì„œ ì¡°íšŒí•œ ì‹¤ì œ ë ˆë²¨ë³„ ê°œìˆ˜ (íŒíŠ¸)
        sample_metadata: ìƒ˜í”Œë§ ë©”íƒ€ë°ì´í„° (IPW ê°€ì¤‘ì¹˜ í¬í•¨)
    """
    # ìƒ˜í”Œ ìš”ì•½
    sample_level_counts = {}
    hourly_counts = {}

    for sample in log_samples:
        level = sample.get("level", "UNKNOWN")
        sample_level_counts[level] = sample_level_counts.get(level, 0) + 1

        timestamp = sample.get("timestamp", "")
        if timestamp:
            hour = timestamp[:13]  # "2024-01-01T15"
            hourly_counts[hour] = hourly_counts.get(hour, 0) + 1

    sample_summary = {
        "sample_size": len(log_samples),
        "level_distribution": sample_level_counts,
        "hourly_distribution": hourly_counts
    }

    # IPW ê°€ì¤‘ì¹˜ í…Œì´ë¸” ìƒì„± (sample_metadataê°€ ì œê³µëœ ê²½ìš°)
    weight_table_section = ""
    if sample_metadata and "weights" in sample_metadata:
        weights = sample_metadata["weights"]
        level_counts = sample_metadata.get("level_counts", {})
        sample_counts = sample_metadata.get("sample_counts", {})

        weight_table = "## ğŸ“Š ê°€ì¤‘ ìƒ˜í”Œë§ ì •ë³´ (Inverse Probability Weighting)\n\n"
        weight_table += "**ì¤‘ìš”**: ì´ ìƒ˜í”Œì€ í¬ì†Œ ì´ë²¤íŠ¸ ì¸ì‹ 2ë‹¨ê³„ ìƒ˜í”Œë§ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        weight_table += "ê° ìƒ˜í”Œì´ ëŒ€í‘œí•˜ëŠ” ì‹¤ì œ ë¡œê·¸ ìˆ˜(ê°€ì¤‘ì¹˜)ë¥¼ ê³ ë ¤í•˜ì—¬ ì¶”ë¡ í•˜ì„¸ìš”.\n\n"
        weight_table += "| ë ˆë²¨ | ì‹¤ì œ ê°œìˆ˜ | ìƒ˜í”Œ ê°œìˆ˜ | ê°€ì¤‘ì¹˜ (IPW) | ì˜ë¯¸ |\n"
        weight_table += "|------|-----------|-----------|--------------|------|\n"

        for level in ["ERROR", "WARN", "INFO"]:
            actual = level_counts.get(level, 0)
            sampled = sample_counts.get(level, 0)
            weight = weights.get(level, 0.0)

            if sampled > 0:
                meaning = f"ìƒ˜í”Œ 1ê°œ = ì‹¤ì œ {weight:.1f}ê°œ"
            else:
                meaning = "ìƒ˜í”Œ ì—†ìŒ"

            weight_table += f"| {level} | {actual:,} | {sampled} | Ã—{weight:.2f} | {meaning} |\n"

        weight_table += "\n**ì¶”ë¡  ë°©ë²•**: ê° ë ˆë²¨ì˜ ìƒ˜í”Œ ê°œìˆ˜ì— í•´ë‹¹ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ë©´ ì‹¤ì œ ê°œìˆ˜ê°€ ë©ë‹ˆë‹¤.\n"
        weight_table += f"- ì˜ˆ: ERROR {sample_counts.get('ERROR', 0)}ê°œ Ã— {weights.get('ERROR', 0):.2f} = {level_counts.get('ERROR', 0):,}ê°œ\n\n"

        weight_table_section = weight_table

    # ì‹¤ì œ ë ˆë²¨ë³„ ê°œìˆ˜ê°€ ì œê³µëœ ê²½ìš°: LLMì´ ê²€ì¦ë§Œ ìˆ˜í–‰
    if actual_level_counts:
        actual_error = actual_level_counts.get("ERROR", 0)
        actual_warn = actual_level_counts.get("WARN", 0)
        actual_info = actual_level_counts.get("INFO", 0)
        actual_error_rate = round((actual_error / total_sample_count * 100) if total_sample_count > 0 else 0, 2)

        # LLM í”„ë¡¬í”„íŠ¸ (ê²€ì¦ ëª¨ë“œ)
        llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            temperature=0.1,
            openai_api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL
        )

        prompt = f"""ë‹¹ì‹ ì€ ë¡œê·¸ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. DB í†µê³„ì™€ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¹„êµ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤.

## DB ì‹¤ì œ í†µê³„ (Ground Truth)
- ì´ ë¡œê·¸ ìˆ˜: {total_sample_count:,}ê°œ
- ERROR: {actual_error:,}ê°œ ({actual_error_rate}%)
- WARN: {actual_warn:,}ê°œ
- INFO: {actual_info:,}ê°œ
- ë¶„ì„ ê¸°ê°„: ìµœê·¼ {time_hours}ì‹œê°„

## ì¸µí™” ìƒ˜í”Œ ë°ì´í„°
- ìƒ˜í”Œ í¬ê¸°: {sample_summary['sample_size']}ê°œ
- ë ˆë²¨ë³„ ë¶„í¬ (ìƒ˜í”Œ): {json.dumps(sample_summary['level_distribution'], ensure_ascii=False)}
- ìƒ˜í”Œë§ ë°©ì‹: ì¸µí™” ìƒ˜í”Œë§ (í¬ì†Œ ì´ë²¤íŠ¸ ìµœì†Œ 5ê°œ ë³´ì¥)

## ê²€ì¦ ì‘ì—…
1. ìƒ˜í”Œì´ ì „ì²´ ë°ì´í„°ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”.
2. í¬ì†Œ ì´ë²¤íŠ¸(ERROR/WARN)ê°€ ì˜ í¬ì°©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
3. ìƒ˜í”Œ í’ˆì§ˆì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "estimated_total_logs": {total_sample_count},
    "estimated_error_count": {actual_error},
    "estimated_warn_count": {actual_warn},
    "estimated_info_count": {actual_info},
    "estimated_error_rate": {actual_error_rate},
    "confidence_score": <ìƒ˜í”Œ í’ˆì§ˆ ì‹ ë¢°ë„ 0-100>,
    "reasoning": "<ê²€ì¦ ê²°ê³¼ ë° ì¸ì‚¬ì´íŠ¸ 1-2ë¬¸ì¥>"
}}

ì¤‘ìš”:
- í†µê³„ ê°’ì€ DB ì‹¤ì œ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì‹ ë¢°ë„ëŠ” ìƒ˜í”Œì´ ì „ì²´ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ”ì§€ì— ëŒ€í•œ í‰ê°€ì…ë‹ˆë‹¤.
- reasoningì— ìƒ˜í”Œ í’ˆì§ˆê³¼ ë°ì´í„° íŠ¹ì„±ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”."""

        try:
            response = llm.invoke(prompt)
            content = response.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            # DB ê°’ìœ¼ë¡œ ê°•ì œ ì„¤ì • (ê²€ì¦ ëª¨ë“œ)
            result["estimated_total_logs"] = total_sample_count
            result["estimated_error_count"] = actual_error
            result["estimated_warn_count"] = actual_warn
            result["estimated_info_count"] = actual_info
            result["estimated_error_rate"] = actual_error_rate
            return result
        except Exception as e:
            # í´ë°±: DB ê°’ ì§ì ‘ ë°˜í™˜
            return {
                "estimated_total_logs": total_sample_count,
                "estimated_error_count": actual_error,
                "estimated_warn_count": actual_warn,
                "estimated_info_count": actual_info,
                "estimated_error_rate": actual_error_rate,
                "confidence_score": 100,  # DB ê°’ì´ë¯€ë¡œ 100% ì‹ ë¢°
                "reasoning": f"DB í†µê³„ ì§ì ‘ ì‚¬ìš© (LLM ê²€ì¦ ì‹¤íŒ¨: {str(e)})"
            }

    # ê¸°ì¡´ ë¡œì§: ì‹¤ì œ ê°œìˆ˜ê°€ ì—†ëŠ” ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
    llm = ChatOpenAI(
        model=settings.LLM_MODEL,
        temperature=0.1,
        openai_api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL
    )

    prompt = f"""ë‹¹ì‹ ì€ ë¡œê·¸ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¸µí™” ìƒ˜í”Œë§ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ í†µê³„ë¥¼ ì¶”ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.

## âš ï¸ ì¤‘ìš”: ìƒ˜í”Œë§ í¸í–¥ ì£¼ì˜!

ì´ ìƒ˜í”Œì€ ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©°, **í¬ì†Œ ì´ë²¤íŠ¸(ERROR/WARN)ëŠ” ì¶©ë¶„í•œ ëŒ€í‘œì„± í™•ë³´ë¥¼ ìœ„í•´
ìµœì†Œ 5ê°œì”© ë³´ì¥ë˜ì–´ ê³¼ëŒ€í‘œí˜„(oversampling)ë˜ì—ˆìŠµë‹ˆë‹¤.**

### ì‹¤ì œ í”„ë¡œë•ì…˜ ë¡œê·¸ì˜ ì¼ë°˜ì  íŠ¹ì„±
- **ERROR**: ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ì˜ 0.3~1% (ê±´ê°•í•œ ì‹œìŠ¤í…œ ê¸°ì¤€)
- **WARN**: ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ì˜ 0.01~0.3% (ë§¤ìš° í¬ì†Œ)
- **INFO**: ëŒ€ë¶€ë¶„ì˜ ë¡œê·¸ë¥¼ ì°¨ì§€ (98~99%+)

**ê²½ê³ **: ìƒ˜í”Œì—ì„œ ERRORê°€ 5%ë¼ê³  í•´ì„œ ì „ì²´ì˜ 5%ê°€ ì•„ë‹™ë‹ˆë‹¤!
ìµœì†Œ ë³´ì¥(minimum guarantee) ë•Œë¬¸ì— ì‹¤ì œë³´ë‹¤ 10~100ë°° ê³¼ëŒ€í‘œí˜„ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## íŒíŠ¸ ì •ë³´ (DBì—ì„œ ì œê³µ)
- ì‹¤ì œ ì´ ë¡œê·¸ ìˆ˜: {total_sample_count:,}ê°œ
- ë¶„ì„ ê¸°ê°„: ìµœê·¼ {time_hours}ì‹œê°„

## ì¸µí™” ìƒ˜í”Œ ë°ì´í„°
- ìƒ˜í”Œ í¬ê¸°: {sample_summary['sample_size']}ê°œ
- ë ˆë²¨ë³„ ë¶„í¬ (ìƒ˜í”Œ): {json.dumps(sample_summary['level_distribution'], ensure_ascii=False)}
- ìƒ˜í”Œë§ ë°©ì‹: ë ˆë²¨ë³„ ë¹„ìœ¨ ê¸°ë°˜ + í¬ì†Œ ì´ë²¤íŠ¸ ìµœì†Œ 5ê°œ ë³´ì¥

{weight_table_section}

## ì¶”ë¡  ê°€ì´ë“œë¼ì¸

{"**ìš°ì„ ìˆœìœ„**: ìœ„ì˜ ê°€ì¤‘ì¹˜ í…Œì´ë¸”ì´ ì œê³µëœ ê²½ìš°, í•´ë‹¹ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ê³„ì‚°í•˜ì„¸ìš”!" if weight_table_section else ""}

1. **ERROR ë¡œê·¸ ì¶”ë¡ **:
   - {"ê°€ì¤‘ì¹˜ í…Œì´ë¸” ì‚¬ìš©: ERROR ìƒ˜í”Œ ê°œìˆ˜ Ã— ê°€ì¤‘ì¹˜ = ì‹¤ì œ ERROR ê°œìˆ˜" if weight_table_section else "ìƒ˜í”Œì— ERRORê°€ ë§ë”ë¼ë„ ì‹¤ì œëŠ” 0.3~1% ë²”ìœ„ì¼ ê°€ëŠ¥ì„± ë†’ìŒ"}
   - ìƒ˜í”Œ ë¹„ìœ¨ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ì§€ ë§ê³ , ì¼ë°˜ì  ë²”ìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ ì¶”ë¡ 

2. **WARN ë¡œê·¸ ì¶”ë¡ **:
   - {"ê°€ì¤‘ì¹˜ í…Œì´ë¸” ì‚¬ìš©: WARN ìƒ˜í”Œ ê°œìˆ˜ Ã— ê°€ì¤‘ì¹˜ = ì‹¤ì œ WARN ê°œìˆ˜" if weight_table_section else "WARNì€ ERRORë³´ë‹¤ ë” í¬ì†Œ (0.01~0.3% ë²”ìœ„)"}
   - ìƒ˜í”Œì— ì¡´ì¬í•œë‹¤ë©´ ìµœì†Œ ë³´ì¥ìœ¼ë¡œ ì¸í•œ ê³¼ëŒ€í‘œí˜„ì¼ ê°€ëŠ¥ì„± ë†’ìŒ

3. **INFO ë¡œê·¸ ì¶”ë¡ **:
   - {"ê°€ì¤‘ì¹˜ í…Œì´ë¸” ì‚¬ìš©: INFO ìƒ˜í”Œ ê°œìˆ˜ Ã— ê°€ì¤‘ì¹˜ = ì‹¤ì œ INFO ê°œìˆ˜" if weight_table_section else "ëŒ€ë¶€ë¶„ì˜ ë¡œê·¸ëŠ” INFO (ì „ì²´ì˜ 98~99%+)"}
   - INFO = ì´ ë¡œê·¸ ìˆ˜ - ERROR - WARN

{"ê°€ì¤‘ì¹˜ í…Œì´ë¸”ì´ ì œê³µëœ ê²½ìš° ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜," if weight_table_section else ""}
ì´ ë¡œê·¸ ìˆ˜({total_sample_count:,}ê°œ)ëŠ” í™•ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
ìƒ˜í”Œì˜ **ìƒëŒ€ì  ë¹„ìœ¨**ì€ ì°¸ê³ í•˜ë˜ **ì ˆëŒ€ì  ë¹„ìœ¨**ì€ í”„ë¡œë•ì…˜ í™˜ê²½ì˜ ì¼ë°˜ì  íŠ¹ì„±ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
{{
    "estimated_total_logs": {total_sample_count},
    "estimated_error_count": <ì¶”ë¡ í•œ ERROR ë¡œê·¸ ìˆ˜>,
    "estimated_warn_count": <ì¶”ë¡ í•œ WARN ë¡œê·¸ ìˆ˜>,
    "estimated_info_count": <ì¶”ë¡ í•œ INFO ë¡œê·¸ ìˆ˜>,
    "estimated_error_rate": <ì¶”ë¡ í•œ ì—ëŸ¬ìœ¨ (%)>,
    "confidence_score": <ì¶”ë¡  ì‹ ë¢°ë„ 0-100>,
    "reasoning": "<ì¶”ë¡  ê·¼ê±° 1-2ë¬¸ì¥>"
}}

ì¤‘ìš”:
- ì´ ë¡œê·¸ ìˆ˜ëŠ” {total_sample_count}ìœ¼ë¡œ ê³ ì •ì…ë‹ˆë‹¤.
- ìƒ˜í”Œ ë¹„ìœ¨ì„ ë¬´ë¹„íŒì ìœ¼ë¡œ ì ìš©í•˜ì§€ ë§ˆì„¸ìš”! í¬ì†Œ ì´ë²¤íŠ¸ëŠ” ê³¼ëŒ€í‘œí˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
- ERRORëŠ” ë³´í†µ 0.3~1%, WARNì€ 0.01~0.3% ë²”ìœ„ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì„¸ìš”.
- ERROR + WARN + INFO = ì´ ë¡œê·¸ ìˆ˜ê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."""

    try:
        response = llm.invoke(prompt)
        content = response.content.strip()

        # JSON íŒŒì‹±
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        result = json.loads(content)
        # ì´ ë¡œê·¸ ìˆ˜ê°€ íŒíŠ¸ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        result["estimated_total_logs"] = total_sample_count
        return result
    except Exception as e:
        # í´ë°±: ë‹¨ìˆœ ë¹„ìœ¨ ê³„ì‚°
        sample_total = len(log_samples)
        if sample_total == 0:
            return {
                "estimated_total_logs": total_sample_count,
                "estimated_error_count": 0,
                "estimated_warn_count": 0,
                "estimated_info_count": 0,
                "estimated_error_rate": 0.0,
                "confidence_score": 0,
                "reasoning": f"LLM ì¶”ë¡  ì‹¤íŒ¨: {str(e)}"
            }

        error_ratio = sample_level_counts.get("ERROR", 0) / sample_total
        warn_ratio = sample_level_counts.get("WARN", 0) / sample_total
        info_ratio = sample_level_counts.get("INFO", 0) / sample_total

        return {
            "estimated_total_logs": total_sample_count,
            "estimated_error_count": int(total_sample_count * error_ratio),
            "estimated_warn_count": int(total_sample_count * warn_ratio),
            "estimated_info_count": int(total_sample_count * info_ratio),
            "estimated_error_rate": round(error_ratio * 100, 2),
            "confidence_score": 85,
            "reasoning": f"ìƒ˜í”Œ ë¹„ìœ¨ ì§ì ‘ ì ìš© (LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)"
        }


def _calculate_accuracy(db_stats: Dict[str, Any], ai_stats: Dict[str, Any]) -> Dict[str, Any]:
    """
    DB í†µê³„ì™€ AI ì¶”ë¡  í†µê³„ì˜ ì •í™•ë„ ê³„ì‚°
    """
    def accuracy_percentage(actual, predicted):
        if actual == 0:
            return 100.0 if predicted == 0 else 0.0
        error = abs(actual - predicted)
        return max(0, round((1 - error / actual) * 100, 2))

    total_accuracy = accuracy_percentage(
        db_stats["total_logs"],
        ai_stats.get("estimated_total_logs", 0)
    )
    error_count_accuracy = accuracy_percentage(
        db_stats["error_count"],
        ai_stats.get("estimated_error_count", 0)
    )
    warn_count_accuracy = accuracy_percentage(
        db_stats["warn_count"],
        ai_stats.get("estimated_warn_count", 0)
    )
    info_count_accuracy = accuracy_percentage(
        db_stats["info_count"],
        ai_stats.get("estimated_info_count", 0)
    )

    # ì—ëŸ¬ìœ¨ ì •í™•ë„ (ì ˆëŒ€ ì˜¤ì°¨ ê¸°ë°˜)
    error_rate_diff = abs(db_stats["error_rate"] - ai_stats.get("estimated_error_rate", 0))
    error_rate_accuracy = max(0, round(100 - error_rate_diff * 10, 2))  # 1% ì°¨ì´ë‹¹ 10ì  ê°ì 

    # ì¢…í•© ì •í™•ë„ (ê°€ì¤‘ í‰ê· )
    overall_accuracy = round(
        total_accuracy * 0.3 +
        error_count_accuracy * 0.3 +
        error_rate_accuracy * 0.2 +
        warn_count_accuracy * 0.1 +
        info_count_accuracy * 0.1,
        2
    )

    return {
        "total_logs_accuracy": total_accuracy,
        "error_count_accuracy": error_count_accuracy,
        "warn_count_accuracy": warn_count_accuracy,
        "info_count_accuracy": info_count_accuracy,
        "error_rate_accuracy": error_rate_accuracy,
        "overall_accuracy": overall_accuracy,
        "ai_confidence": ai_stats.get("confidence_score", 0)
    }


@tool
async def compare_ai_vs_db_statistics(
    project_uuid: str,
    time_hours: int = 24,
    sample_size: int = 100
) -> str:
    """
    AI ì¶”ë¡  í†µê³„ì™€ DB ì§ì ‘ ì¡°íšŒ í†µê³„ë¥¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… OpenSearchì—ì„œ ì§ì ‘ í†µê³„ ì¿¼ë¦¬ (Ground Truth)
    - âœ… ë¡œê·¸ ìƒ˜í”Œì„ LLMì—ê²Œ ì œê³µí•˜ì—¬ ì „ì²´ í†µê³„ ì¶”ë¡ 
    - âœ… ë‘ ê²°ê³¼ì˜ ì •í™•ë„/ì˜¤ì°¨ìœ¨ ê³„ì‚°
    - âœ… AIê°€ DB ì¿¼ë¦¬ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ ê²€ì¦

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "AIê°€ DBë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•´ì¤˜"
    2. "í†µê³„ ì¶”ë¡  ì •í™•ë„ë¥¼ í™•ì¸í•´ì¤˜"
    3. "LLM ê¸°ë°˜ ë¶„ì„ì˜ ì‹ ë¢°ë„ëŠ”?"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - LLM API í˜¸ì¶œì´ í¬í•¨ë˜ì–´ ì•½ê°„ì˜ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤
    - temperature=0.1ë¡œ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        sample_size: LLMì— ì œê³µí•  ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ 100ê°œ)

    Returns:
        DB í†µê³„, AI ì¶”ë¡  í†µê³„, ì •í™•ë„ ì§€í‘œ, ê²€ì¦ ê²°ê³¼
    """
    try:
        # 1. DBì—ì„œ ì§ì ‘ í†µê³„ ì¡°íšŒ (Ground Truth)
        db_stats = _get_db_statistics(project_uuid, time_hours)

        if db_stats["total_logs"] == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # 2. ë¡œê·¸ ìƒ˜í”Œ ì¶”ì¶œ
        log_samples = _get_log_samples(project_uuid, time_hours, sample_size)

        if not log_samples:
            return f"ë¡œê·¸ ìƒ˜í”Œì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3. LLM ê¸°ë°˜ í†µê³„ ì¶”ë¡ 
        ai_stats = _llm_estimate_statistics(log_samples, db_stats["total_logs"], time_hours)

        # 4. ì •í™•ë„ ê³„ì‚°
        accuracy_metrics = _calculate_accuracy(db_stats, ai_stats)

        # 5. ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            "=" * 50,
            "  AI vs DB í†µê³„ ë¹„êµ ê²€ì¦ ê²°ê³¼",
            "=" * 50,
            "",
            f"ğŸ“Š ë¶„ì„ ê¸°ê°„: ìµœê·¼ {time_hours}ì‹œê°„",
            f"ğŸ“Š ìƒ˜í”Œ í¬ê¸°: {len(log_samples)}ê°œ / ì „ì²´ {db_stats['total_logs']:,}ê°œ",
            "",
            "=" * 50,
            "  1. DB ì§ì ‘ ì¡°íšŒ ê²°ê³¼ (Ground Truth)",
            "=" * 50,
            f"  ì´ ë¡œê·¸ ìˆ˜: {db_stats['total_logs']:,}ê°œ",
            f"  ERROR: {db_stats['error_count']:,}ê°œ",
            f"  WARN: {db_stats['warn_count']:,}ê°œ",
            f"  INFO: {db_stats['info_count']:,}ê°œ",
            f"  ì—ëŸ¬ìœ¨: {db_stats['error_rate']:.2f}%",
            f"  í”¼í¬ ì‹œê°„: {db_stats['peak_hour']} ({db_stats['peak_count']:,}ê±´)",
            "",
            "=" * 50,
            "  2. AI(LLM) ì¶”ë¡  ê²°ê³¼",
            "=" * 50,
            f"  ì´ ë¡œê·¸ ìˆ˜: {ai_stats.get('estimated_total_logs', 0):,}ê°œ",
            f"  ERROR: {ai_stats.get('estimated_error_count', 0):,}ê°œ",
            f"  WARN: {ai_stats.get('estimated_warn_count', 0):,}ê°œ",
            f"  INFO: {ai_stats.get('estimated_info_count', 0):,}ê°œ",
            f"  ì—ëŸ¬ìœ¨: {ai_stats.get('estimated_error_rate', 0):.2f}%",
            f"  AI ì‹ ë¢°ë„: {ai_stats.get('confidence_score', 0)}%",
            f"  ì¶”ë¡  ê·¼ê±°: {ai_stats.get('reasoning', 'N/A')}",
            "",
            "=" * 50,
            "  3. ì •í™•ë„ ê²€ì¦ ê²°ê³¼",
            "=" * 50,
            f"  ğŸ“ˆ ì´ ë¡œê·¸ ìˆ˜ ì¼ì¹˜ìœ¨: {accuracy_metrics['total_logs_accuracy']:.1f}%",
            f"  ğŸ“ˆ ERROR ìˆ˜ ì¼ì¹˜ìœ¨: {accuracy_metrics['error_count_accuracy']:.1f}%",
            f"  ğŸ“ˆ WARN ìˆ˜ ì¼ì¹˜ìœ¨: {accuracy_metrics['warn_count_accuracy']:.1f}%",
            f"  ğŸ“ˆ INFO ìˆ˜ ì¼ì¹˜ìœ¨: {accuracy_metrics['info_count_accuracy']:.1f}%",
            f"  ğŸ“ˆ ì—ëŸ¬ìœ¨ ì •í™•ë„: {accuracy_metrics['error_rate_accuracy']:.1f}%",
            "",
            f"  â­ ì¢…í•© ì •í™•ë„: {accuracy_metrics['overall_accuracy']:.1f}%",
            "",
        ]

        # 6. ê²€ì¦ ê²°ë¡ 
        overall = accuracy_metrics['overall_accuracy']
        if overall >= 95:
            verdict = "ğŸ† **ë§¤ìš° ìš°ìˆ˜**: AIê°€ DB ì¿¼ë¦¬ë¥¼ ì™„ë²½íˆ ëŒ€ì²´ ê°€ëŠ¥"
            explanation = "ì˜¤ì°¨ìœ¨ 5% ë¯¸ë§Œìœ¼ë¡œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‹ ë¢°ì„± ìˆê²Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif overall >= 90:
            verdict = "âœ… **ìš°ìˆ˜**: AIê°€ DB ì¿¼ë¦¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥"
            explanation = "ì˜¤ì°¨ìœ¨ 10% ë¯¸ë§Œìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ë¶„ì„ ì—…ë¬´ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif overall >= 80:
            verdict = "ğŸŸ¡ **ì–‘í˜¸**: AIê°€ ë³´ì¡° ë„êµ¬ë¡œì„œ ìœ ìš©"
            explanation = "ì˜¤ì°¨ìœ¨ 20% ë¯¸ë§Œìœ¼ë¡œ íŠ¸ë Œë“œ ë¶„ì„ê³¼ ì´ìƒ íƒì§€ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif overall >= 70:
            verdict = "ğŸŸ  **ë³´í†µ**: ê°œì„ ì´ í•„ìš”"
            explanation = "ì˜¤ì°¨ìœ¨ì´ ë†’ì•„ ì¶”ê°€ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            verdict = "ğŸ”´ **ë¯¸í¡**: AI ì¶”ë¡  ë¡œì§ ì¬ê²€í†  í•„ìš”"
            explanation = "ì •í™•ë„ê°€ ë‚®ì•„ í”„ë¡¬í”„íŠ¸ë‚˜ ìƒ˜í”Œë§ ì „ëµ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."

        summary_lines.extend([
            "=" * 50,
            "  4. ê²€ì¦ ê²°ë¡ ",
            "=" * 50,
            f"  {verdict}",
            "",
            f"  {explanation}",
            "",
            "=" * 50,
            "  5. ê¸°ìˆ ì  ê²€ì¦ í¬ì¸íŠ¸",
            "=" * 50,
            "  - Temperature 0.1ë¡œ ì¼ê´€ëœ ì¶”ë¡  ë³´ì¥",
            "  - Structured Outputìœ¼ë¡œ í˜•ì‹ ì˜¤ë¥˜ ë°©ì§€",
            "  - ìƒ˜í”Œ ê¸°ë°˜ ì¶”ë¡ ìœ¼ë¡œ í† í° ë¹„ìš© ì ˆê°",
            "  - ìë™í™”ëœ ì •í™•ë„ ì¸¡ì •ìœ¼ë¡œ ì‹ ë¢°ì„± ê²€ì¦",
            "",
            f"  ğŸ’¡ ì´ ê²°ê³¼ëŠ” LLMì´ ë‹¨ìˆœ DB ì§‘ê³„ë¥¼ ë„˜ì–´",
            f"     'ì‚¬ëŒ ìˆ˜ì¤€ì˜ ë°ì´í„° ë¶„ì„ ì—­ëŸ‰'ì„ ë³´ìœ í•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤.",
            "=" * 50,
        ])

        return "\n".join(summary_lines)

    except Exception as e:
        return f"AI vs DB í†µê³„ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_hourly_comparison(
    project_uuid: str,
    time_hours: int = 24
) -> str:
    """
    ì‹œê°„ëŒ€ë³„ ë¡œê·¸ í†µê³„ë¥¼ DBì™€ AI ì¶”ë¡ ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… 1ì‹œê°„ ë‹¨ìœ„ ë¡œê·¸ ë¶„í¬ DB ì¡°íšŒ
    - âœ… ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
    - âœ… í”¼í¬ ì‹œê°„, íŠ¸ë Œë“œ ë°©í–¥ í™•ì¸

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ì‹œê°„ëŒ€ë³„ ë¡œê·¸ ì¶”ì„¸ë¥¼ ë³´ì—¬ì¤˜"
    2. "1ì‹œê°„ ë‹¨ìœ„ í†µê³„ë¥¼ ë¹„êµí•´ì¤˜"

    Returns:
        ì‹œê°„ëŒ€ë³„ ë¡œê·¸ í†µê³„, í”¼í¬ ì‹œê°„, íŠ¸ë Œë“œ ë¶„ì„
    """
    try:
        db_stats = _get_db_statistics(project_uuid, time_hours)

        if db_stats["total_logs"] == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        summary_lines = [
            f"## ğŸ“Š ì‹œê°„ëŒ€ë³„ ë¡œê·¸ ë¶„í¬ (ìµœê·¼ {time_hours}ì‹œê°„)",
            "",
            f"**ì´ ë¡œê·¸**: {db_stats['total_logs']:,}ê°œ",
            f"**ì—ëŸ¬ìœ¨**: {db_stats['error_rate']:.2f}%",
            f"**í”¼í¬ ì‹œê°„**: {db_stats['peak_hour']} ({db_stats['peak_count']:,}ê±´)",
            "",
            "### ì‹œê°„ëŒ€ë³„ ìƒì„¸",
            "| ì‹œê°„ | ì´ ë¡œê·¸ | ERROR | WARN | INFO |",
            "|------|---------|-------|------|------|"
        ]

        for hourly in db_stats["hourly_data"][-12:]:  # ìµœê·¼ 12ì‹œê°„ë§Œ
            hour_display = hourly["hour"][-5:] if len(hourly["hour"]) >= 5 else hourly["hour"]
            summary_lines.append(
                f"| {hour_display} | {hourly['total']:,} | {hourly['error']} | {hourly['warn']} | {hourly['info']} |"
            )

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì‹œê°„ëŒ€ë³„ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ========== ERROR ì „ìš© ë¹„êµ í•¨ìˆ˜ë“¤ ==========

async def _get_db_error_statistics(
    project_uuid: str,
    time_hours: int = 24
) -> Dict[str, Any]:
    """
    OpenSearchì—ì„œ ERROR ë¡œê·¸ í†µê³„ ì¡°íšŒ

    Returns:
        {
            "total_logs": int,        # ì „ì²´ ë¡œê·¸ ìˆ˜
            "total_errors": int,      # ERROR ë¡œê·¸ ìˆ˜
            "error_rate": float,      # ERROR ë¹„ìœ¨ (%)
            "peak_error_hour": str,   # ERROR ìµœë‹¤ ë°œìƒ ì‹œê°„
            "peak_error_count": int   # ìµœë‹¤ ì‹œê°„ ERROR ìˆ˜
        }
    """
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    query = {
        "size": 0,
        "query": {
            "range": {
                "timestamp": {
                    "gte": start_time.isoformat() + "Z",
                    "lte": end_time.isoformat() + "Z"
                }
            }
        },
        "aggs": {
            # ERROR ë¡œê·¸ ìˆ˜
            "error_logs": {
                "filter": {"term": {"level": "ERROR"}}
            },
            # ì‹œê°„ëŒ€ë³„ ERROR ë¶„í¬
            "error_by_hour": {
                "filter": {"term": {"level": "ERROR"}},
                "aggs": {
                    "hours": {
                        "date_histogram": {
                            "field": "timestamp",
                            "fixed_interval": "1h",
                            "time_zone": "Asia/Seoul"
                        }
                    }
                }
            }
        }
    }

    result = await asyncio.to_thread(
        opensearch_client.search,
        index=index_pattern,
        body=query,
        track_total_hits=True
    )

    total_logs = result["hits"]["total"]["value"]
    total_errors = result["aggregations"]["error_logs"]["doc_count"]
    error_rate = (total_errors / total_logs * 100) if total_logs > 0 else 0.0

    # Peak ERROR hour ì°¾ê¸°
    hour_buckets = result["aggregations"]["error_by_hour"]["hours"]["buckets"]
    peak_hour = None
    peak_count = 0
    if hour_buckets:
        peak_bucket = max(hour_buckets, key=lambda b: b["doc_count"])
        peak_hour = peak_bucket["key_as_string"]
        peak_count = peak_bucket["doc_count"]

    return {
        "total_logs": total_logs,
        "total_errors": total_errors,
        "error_rate": round(error_rate, 2),
        "peak_error_hour": peak_hour,
        "peak_error_count": peak_count
    }


async def _sample_errors_with_vector(
    project_uuid: str,
    time_hours: int,
    sample_size: int
) -> Tuple[List[Dict], Optional[Dict]]:
    """
    Vector KNNìœ¼ë¡œ ERROR ë¡œê·¸ ìƒ˜í”Œë§ + ê·¸ë£¹í•‘ ì •ë³´

    Returns:
        (samples, vector_info)
        - samples: ERROR ë¡œê·¸ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        - vector_info: Vector ê·¸ë£¹í•‘ ë©”íƒ€ë°ì´í„°
    """
    # ë²¡í„°í™”ëœ ERROR ë¡œê·¸ ê°œìˆ˜ í™•ì¸
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    vector_check_query = {
        "size": 0,
        "query": {
            "bool": {
                "must": [
                    {"term": {"level": "ERROR"}},
                    {"exists": {"field": "log_vector"}},
                    {"range": {"timestamp": {
                        "gte": start_time.isoformat() + "Z",
                        "lte": end_time.isoformat() + "Z"
                    }}}
                ]
            }
        }
    }

    vector_result = await asyncio.to_thread(
        opensearch_client.search,
        index=index_pattern,
        body=vector_check_query,
        track_total_hits=True
    )

    vectorized_count = vector_result["hits"]["total"]["value"]

    # ì „ì²´ ERROR ìˆ˜ ì¡°íšŒ
    total_error_query = {
        "size": 0,
        "query": {
            "bool": {
                "must": [
                    {"term": {"level": "ERROR"}},
                    {"range": {"timestamp": {
                        "gte": start_time.isoformat() + "Z",
                        "lte": end_time.isoformat() + "Z"
                    }}}
                ]
            }
        }
    }

    total_error_result = await asyncio.to_thread(
        opensearch_client.search,
        index=index_pattern,
        body=total_error_query,
        track_total_hits=True
    )

    total_errors = total_error_result["hits"]["total"]["value"]
    vectorization_rate = (vectorized_count / total_errors * 100) if total_errors > 0 else 0.0

    # ë‹¤ì–‘ì„±ì„ ìœ„í•´ í•­ìƒ ëœë¤ ìƒ˜í”Œë§ ì‚¬ìš©
    # (Vector KNNì€ íŠ¹ì • ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ê²ƒë§Œ ì„ íƒí•˜ì—¬ í¸í–¥ë¨)
    samples = await sample_random_stratified(
        project_uuid=project_uuid,
        k_per_level={"ERROR": sample_size},
        time_hours=time_hours
    )
    sampling_method = "random_diverse"
    logger.info(f"âœ… ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘í•œ ERROR í™•ë³´: {len(samples)}ê°œ")

    vector_info = {
        "vectorized_error_count": vectorized_count,
        "vectorization_rate": round(vectorization_rate, 1),
        "sampling_method": sampling_method,
        "sample_distribution": f"{len(samples)}ê°œ ERROR ë¡œê·¸ ìƒ˜í”Œ"
    }

    return samples, vector_info


def _llm_estimate_error_statistics(
    error_samples: List[Dict],
    total_logs: int,
    time_hours: int
) -> Dict[str, Any]:
    """
    LLMìœ¼ë¡œ ERROR í†µê³„ ì¶”ì •

    Returns:
        {
            "estimated_total_errors": int,
            "estimated_error_rate": float,
            "confidence_score": int,
            "reasoning": str
        }
    """
    sample_count = len(error_samples)

    # ìƒ˜í”Œ ìš”ì•½ (ë©”ì‹œì§€ë§Œ, ìµœëŒ€ 200ì)
    sample_summaries = []
    for i, log in enumerate(error_samples[:50], 1):  # ìµœëŒ€ 50ê°œë§Œ
        msg = log.get("message", "")[:200]
        sample_summaries.append(f"{i}. {msg}")

    prompt = f"""ë‹¹ì‹ ì€ ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ìˆ˜ì§‘ëœ ERROR ë¡œê·¸ ìƒ˜í”Œì„ ë¶„ì„í•˜ì—¬ ì „ì²´ ERROR í†µê³„ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

## ì£¼ì–´ì§„ ì •ë³´

**ì „ì²´ ë¡œê·¸ ìˆ˜**: {total_logs:,}ê°œ
**ERROR ìƒ˜í”Œ ìˆ˜**: {sample_count}ê°œ (ì „ì²´ ERROR ë¡œê·¸ì—ì„œ ìƒ˜í”Œë§ë¨)

**ERROR ìƒ˜í”Œ ë©”ì‹œì§€** (ìµœëŒ€ 50ê°œ):
{chr(10).join(sample_summaries)}

## ì¶”ì • ê³¼ì œ

1. **ì „ì²´ ERROR ë¡œê·¸ ê°œìˆ˜ ì¶”ì •**
   - ìƒ˜í”Œ ë¹„ìœ¨ì„ ê³ ë ¤í•˜ì—¬ ì „ì²´ ERROR ìˆ˜ ì¶”ì •
   - ê±´ê°•í•œ ì‹œìŠ¤í…œ: ERROR 0.3~1%
   - ë¬¸ì œ ìˆëŠ” ì‹œìŠ¤í…œ: ERROR 1~5%+

2. **ERROR ë°œìƒë¥  ì¶”ì •** (%)
   - ERROR ë¡œê·¸ / ì „ì²´ ë¡œê·¸ * 100

3. **ì‹ ë¢°ë„ ì ìˆ˜** (0~100)
   - ìƒ˜í”Œì´ ì¶©ë¶„í•œê°€?
   - ìƒ˜í”Œ í’ˆì§ˆì´ ì¢‹ì€ê°€?

4. **ì¶”ë¡  ê·¼ê±°**
   - ì™œ ì´ë ‡ê²Œ ì¶”ì •í–ˆëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ)

{{
  "estimated_total_errors": <ìˆ«ì>,
  "estimated_error_rate": <ì†Œìˆ˜ì  2ìë¦¬>,
  "confidence_score": <0~100 ì •ìˆ˜>,
  "reasoning": "<ì¶”ë¡  ê·¼ê±°, 2-3ë¬¸ì¥>"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    llm = ChatOpenAI(
        model=settings.LLM_MODEL,
        temperature=0.1,
        openai_api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL
    )
    response = llm.invoke(prompt)
    result_text = response.content.strip()

    # JSON íŒŒì‹±
    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0].strip()
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0].strip()

        result = json.loads(result_text)

        return {
            "estimated_total_errors": result["estimated_total_errors"],
            "estimated_error_rate": round(result["estimated_error_rate"], 2),
            "confidence_score": result["confidence_score"],
            "reasoning": result["reasoning"]
        }

    except Exception as e:
        logger.error(f"LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}, ì‘ë‹µ: {result_text[:500]}")

        # í´ë°±: ë‹¨ìˆœ ë¹„ìœ¨ ê³„ì‚°
        estimated_errors = int(sample_count * (total_logs / sample_count)) if sample_count > 0 else 0
        estimated_rate = (estimated_errors / total_logs * 100) if total_logs > 0 else 0.0

        return {
            "estimated_total_errors": estimated_errors,
            "estimated_error_rate": round(estimated_rate, 2),
            "confidence_score": 50,
            "reasoning": "LLM ì¶”ë¡  ì‹¤íŒ¨, ë‹¨ìˆœ ë¹„ìœ¨ë¡œ ì¶”ì •"
        }


def _calculate_error_accuracy(
    db_stats: Dict[str, Any],
    ai_stats: Dict[str, Any]
) -> Dict[str, float]:
    """
    ERROR í†µê³„ ì •í™•ë„ ê³„ì‚°

    Returns:
        {
            "error_count_accuracy": float,
            "error_rate_accuracy": float,
            "overall_accuracy": float
        }
    """
    def accuracy_percentage(actual, predicted):
        if actual == 0:
            return 100.0 if predicted == 0 else 0.0
        error = abs(actual - predicted)
        return max(0, round((1 - error / actual) * 100, 2))

    # ERROR ê°œìˆ˜ ì •í™•ë„
    error_count_acc = accuracy_percentage(
        db_stats["total_errors"],
        ai_stats["estimated_total_errors"]
    )

    # ERROR ë¹„ìœ¨ ì •í™•ë„ (10ì  per 1% ì°¨ì´)
    rate_diff = abs(db_stats["error_rate"] - ai_stats["estimated_error_rate"])
    error_rate_acc = max(0, round(100 - rate_diff * 10, 2))

    # ì¢…í•© ì •í™•ë„ (ê°€ì¤‘ í‰ê· )
    overall_acc = round(
        error_count_acc * 0.6 + error_rate_acc * 0.4,
        2
    )

    return {
        "error_count_accuracy": error_count_acc,
        "error_rate_accuracy": error_rate_acc,
        "overall_accuracy": overall_acc
    }


def _calculate_dynamic_threshold(vectors: List[List[float]], min_threshold: float = 0.3) -> float:
    """
    Centroid ê¸°ë°˜ ë™ì  threshold ê³„ì‚°

    ê° ERROR ìƒ˜í”Œê³¼ centroidì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ ,
    ìµœì†Œ ìœ ì‚¬ë„ - marginì„ thresholdë¡œ ì„¤ì •í•˜ì—¬
    ëª¨ë“  ERROR ìƒ˜í”Œì´ threshold ì´ìƒì´ ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

    Args:
        vectors: ERROR ìƒ˜í”Œë“¤ì˜ ë²¡í„° ë¦¬ìŠ¤íŠ¸
        min_threshold: ìµœì†Œ threshold (ê¸°ë³¸ 0.3)

    Returns:
        ë™ì ìœ¼ë¡œ ê³„ì‚°ëœ threshold
    """
    import numpy as np

    if len(vectors) < 2:
        return 0.5  # ìƒ˜í”Œ ë¶€ì¡± ì‹œ ê¸°ë³¸ê°’

    vectors_array = np.array(vectors)

    # 1. Centroid ê³„ì‚° (í‰ê·  ë²¡í„°)
    centroid = np.mean(vectors_array, axis=0)

    # 2. Centroid ì •ê·œí™”
    centroid_norm = centroid / np.linalg.norm(centroid)

    # 3. ê° ìƒ˜í”Œ ì •ê·œí™”
    norms = np.linalg.norm(vectors_array, axis=1, keepdims=True)
    normalized = vectors_array / norms

    # 4. ê° ìƒ˜í”Œê³¼ centroidì˜ cosine similarity ê³„ì‚°
    similarities_to_centroid = np.dot(normalized, centroid_norm)

    # 5. ìµœì†Œ ìœ ì‚¬ë„ - marginì„ thresholdë¡œ ì„¤ì •
    min_sim = np.min(similarities_to_centroid)
    mean_sim = np.mean(similarities_to_centroid)
    margin = 0.05  # ì•½ê°„ì˜ ì—¬ìœ  ë§ˆì§„

    dynamic_threshold = min_sim - margin

    logger.info(
        f"ğŸ“Š Centroid-based threshold: "
        f"min_sim={min_sim:.3f}, mean_sim={mean_sim:.3f}, "
        f"margin={margin}, threshold={dynamic_threshold:.3f}"
    )

    # ìµœì†Œê°’ í´ë¨í•‘
    return max(dynamic_threshold, min_threshold)


async def _vector_estimate_error_count(
    error_samples: List[Dict],
    project_uuid: str,
    time_hours: int,
    total_logs: int,
    similarity_threshold: float = None  # Noneì´ë©´ ë™ì  ê³„ì‚°
) -> Dict[str, Any]:
    """
    Vector ìœ ì‚¬ë„ ê¸°ë°˜ ERROR-like ë¡œê·¸ counting

    ERROR ìƒ˜í”Œë“¤ì˜ centroid(í‰ê·  ë²¡í„°)ë¥¼ ê³„ì‚°í•˜ê³ ,
    ì „ì²´ ë¡œê·¸ì—ì„œ centroidì™€ ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ ë¡œê·¸ë¥¼ countingí•©ë‹ˆë‹¤.

    Args:
        error_samples: ë²¡í„°ê°€ ìˆëŠ” ERROR ë¡œê·¸ ìƒ˜í”Œë“¤
        project_uuid: í”„ë¡œì íŠ¸ UUID
        time_hours: ë¶„ì„ ê¸°ê°„
        total_logs: ì „ì²´ ë¡œê·¸ ìˆ˜ (error_rate ê³„ì‚°ìš©)
        similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (Noneì´ë©´ ë™ì  ê³„ì‚°)

    Returns:
        {
            "estimated_total_errors": int,
            "estimated_error_rate": float,
            "confidence_score": int,
            "reasoning": str
        }
    """
    import numpy as np

    # 1. ERROR ìƒ˜í”Œë“¤ì˜ ë²¡í„° ì¶”ì¶œ
    vectors = [s.get('log_vector') for s in error_samples if s.get('log_vector')]

    if not vectors:
        logger.warning("No vectorized ERROR samples available for centroid calculation")
        return {
            "estimated_total_errors": 0,
            "estimated_error_rate": 0.0,
            "confidence_score": 0,
            "reasoning": "ë²¡í„°í™”ëœ ERROR ìƒ˜í”Œ ì—†ìŒ"
        }

    # 2. ë™ì  threshold ê³„ì‚° (Noneì´ë©´)
    if similarity_threshold is None:
        similarity_threshold = _calculate_dynamic_threshold(vectors)
        logger.info(f"ğŸ“Š Using dynamic threshold: {similarity_threshold:.3f}")

    logger.info(f"ğŸ“Š Vector estimation: {len(vectors)} ERROR samples with vectors")

    # 2. Centroid ê³„ì‚° (í‰ê·  ë²¡í„°)
    centroid = np.mean(vectors, axis=0).tolist()
    logger.info(f"ğŸ“Š Centroid calculated from {len(vectors)} vectors")

    # 3. OpenSearch KNN ê²€ìƒ‰ (ì „ì²´ ë¡œê·¸ ëŒ€ìƒ, ìœ ì‚¬ë„ threshold ì ìš©)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    try:
        # KNN ì¿¼ë¦¬ë¡œ ìœ ì‚¬í•œ ë¡œê·¸ ê²€ìƒ‰
        knn_query = {
            "size": 0,  # countë§Œ í•„ìš”
            "query": {
                "bool": {
                    "must": [
                        {
                            "script_score": {
                                "query": {
                                    "bool": {
                                        "filter": [
                                            {"range": {"timestamp": {
                                                "gte": start_time.isoformat() + 'Z',
                                                "lte": end_time.isoformat() + 'Z'
                                            }}},
                                            {"exists": {"field": "log_vector"}}
                                        ]
                                    }
                                },
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'log_vector') + 1.0",
                                    "params": {"query_vector": centroid}
                                }
                            }
                        }
                    ],
                    "filter": [
                        {"range": {"_score": {"gte": similarity_threshold + 1.0}}}  # +1.0 because cosineSimilarity returns [-1, 1] + 1 = [0, 2]
                    ]
                }
            },
            "track_total_hits": True
        }

        # ë¨¼ì € ì „ì²´ ìœ ì‚¬ ë¡œê·¸ ìˆ˜ë¥¼ aggregationìœ¼ë¡œ ê³„ì‚°
        # script_scoreëŠ” filterì—ì„œ ì‚¬ìš© ë¶ˆê°€í•˜ë¯€ë¡œ, ëŒ€ì•ˆ ì ‘ê·¼ë²• ì‚¬ìš©
        # OpenSearchì—ì„œëŠ” min_scoreë¥¼ ì‚¬ìš©

        # ëŒ€ì•ˆ: KNNìœ¼ë¡œ top-k ê²€ìƒ‰ í›„ threshold ì´ìƒë§Œ count
        knn_search_query = {
            "size": 10000,  # ì¶©ë¶„íˆ í° ê°’
            "min_score": similarity_threshold + 1.0,  # cosineSimilarity + 1.0 ìŠ¤ì½”ì–´ì— ë§ì¶¤ (0.5 â†’ 1.5)
            "query": {
                "script_score": {
                    "query": {
                        "bool": {
                            "filter": [
                                {"range": {"timestamp": {
                                    "gte": start_time.isoformat() + 'Z',
                                    "lte": end_time.isoformat() + 'Z'
                                }}},
                                {"exists": {"field": "log_vector"}}
                            ]
                        }
                    },
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'log_vector') + 1.0",
                        "params": {"query_vector": centroid}
                    }
                }
            },
            "_source": False,  # ë¬¸ì„œ ë‚´ìš© ë¶ˆí•„ìš”, countë§Œ
            "track_total_hits": True
        }

        result = opensearch_client.search(
            index=index_pattern,
            body=knn_search_query
        )

        # 4. Counting - threshold ì´ìƒì¸ ê²°ê³¼ë§Œ
        hits = result.get('hits', {}).get('hits', [])
        # min_scoreê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìˆ˜ë™ í•„í„°ë§
        similar_count = sum(1 for hit in hits if hit.get('_score', 0) >= similarity_threshold + 1.0)

        # total_hits ì‚¬ìš© (ë” ì •í™•í•  ìˆ˜ ìˆìŒ)
        total_hits = result.get('hits', {}).get('total', {})
        if isinstance(total_hits, dict):
            reported_count = total_hits.get('value', 0)
        else:
            reported_count = total_hits

        # ë” í° ê°’ ì‚¬ìš© (size ì œí•œìœ¼ë¡œ ì¸í•œ ì†ì‹¤ ë°©ì§€)
        similar_count = max(similar_count, reported_count)

        logger.info(f"ğŸ“Š Vector KNN search: {similar_count} logs with similarity >= {similarity_threshold}")

    except Exception as e:
        logger.error(f"Vector KNN search failed: {e}")
        # í´ë°±: ìƒ˜í”Œ ê¸°ë°˜ ì¶”ì •
        similar_count = len(error_samples)

    # 5. ê²°ê³¼ ê³„ì‚°
    error_rate = (similar_count / total_logs * 100) if total_logs > 0 else 0.0

    # ì‹ ë¢°ë„: ìƒ˜í”Œ ìˆ˜ ê¸°ë°˜
    confidence = min(100, len(vectors) * 2)

    return {
        "estimated_total_errors": similar_count,
        "estimated_error_rate": round(error_rate, 2),
        "confidence_score": confidence,
        "reasoning": f"ERROR centroid ê¸°ì¤€ ìœ ì‚¬ë„ {similarity_threshold} ì´ìƒ ë¡œê·¸ {similar_count}ê°œ (ìƒ˜í”Œ {len(vectors)}ê°œ ê¸°ë°˜)"
    }
