"""
íŒ¨í„´ íƒì§€ ë„êµ¬ (Pattern Detection Tools)
- ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§, ë™ì‹œì„± ì´ìŠˆ, ì¬ë°œ ì£¼ê¸°, ì—ëŸ¬ ìƒì¡´ ì‹œê°„
"""

from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from langchain_core.tools import tool
import re

from app.core.opensearch import opensearch_client
from app.tools.common_fields import BASE_FIELDS, LOG_DETAILS_FIELDS


def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    ë‘ í…ìŠ¤íŠ¸ì˜ ë‹¨ìˆœ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard similarity)

    Returns:
        0.0 ~ 1.0 ì‚¬ì´ì˜ ìœ ì‚¬ë„ ì ìˆ˜
    """
    if not text1 or not text2:
        return 0.0

    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°œí–‰, ê³µë°±, íŠ¹ìˆ˜ë¬¸ì)
    words1 = set(re.findall(r'\w+', text1.lower()))
    words2 = set(re.findall(r'\w+', text2.lower()))

    if not words1 or not words2:
        return 0.0

    # Jaccard similarity
    intersection = len(words1 & words2)
    union = len(words1 | words2)

    return intersection / union if union > 0 else 0.0


@tool
async def cluster_stack_traces(
    project_uuid: str,
    exception_type: Optional[str] = None,
    time_hours: int = 168,
    similarity_threshold: float = 0.8
) -> str:
    """
    ìœ ì‚¬í•œ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ê·¼ë³¸ ì›ì¸ ê·¸ë£¹ì„ ì°¾ìŠµë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… stack_trace í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard similarity)
    - âœ… ê°™ì€ exception_type + ìœ ì‚¬ stack_trace ê·¸ë£¹í•‘
    - âœ… ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ì—ëŸ¬ ì„ ì •
    - âœ… ë°œìƒ ë¹ˆë„ ë° ì„œë¹„ìŠ¤ ë¶„í¬ ë¶„ì„
    - âŒ ë³µì¡í•œ ML í´ëŸ¬ìŠ¤í„°ë§ì€ ë¯¸ì‚¬ìš© (ë‹¨ìˆœ ìœ ì‚¬ë„ ê¸°ë°˜)

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ë¹„ìŠ·í•œ ì—ëŸ¬ë¼ë¦¬ ë¬¶ì–´ì¤˜"
    2. "NullPointerExceptionì˜ íŒ¨í„´ ë¶„ì„"
    3. "ê°™ì€ ì›ì¸ì˜ ì—ëŸ¬ë“¤"
    4. "ì¤‘ë³µ ì—ëŸ¬ ì œê±°"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - stack_traceê°€ ì—†ëŠ” ë¡œê·¸ëŠ” ì œì™¸ë©ë‹ˆë‹¤
    - ê¸°ë³¸ ë¶„ì„ ê¸°ê°„ì€ 7ì¼ì…ë‹ˆë‹¤

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        exception_type: ì˜ˆì™¸ íƒ€ì… í•„í„° (ì„ íƒ, ì˜ˆ: "NullPointerException")
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 168ì‹œê°„=7ì¼)
        similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.8)

    ê´€ë ¨ ë„êµ¬:
    - get_error_frequency_ranking: ì—ëŸ¬ íƒ€ì…ë³„ ë¹ˆë„
    - analyze_errors_unified: ì—ëŸ¬ í†µí•© ë¶„ì„
    - get_recent_errors: ìµœê·¼ ì—ëŸ¬ ëª©ë¡

    Returns:
        ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í´ëŸ¬ìŠ¤í„° (ê·¸ë£¹ë³„ ëŒ€í‘œ ì—ëŸ¬, ë¹ˆë„)
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}},
        {"exists": {"field": "stack_trace"}},
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if exception_type:
        must_clauses.append({
            "bool": {
                "should": [
                    {"term": {"log_details.exception_type": exception_type}},
                    {"match": {"message": exception_type}}
                ],
                "minimum_should_match": 1
            }
        })

    try:
        # OpenSearch ê²€ìƒ‰
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 100,  # í´ëŸ¬ìŠ¤í„°ë§ìš© ìƒ˜í”Œ
                "query": {"bool": {"must": must_clauses}},
                "sort": [{"timestamp": "desc"}],
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            from app.tools.response_templates import get_empty_result_message

            conditions = ["stack_trace í•„ë“œ ì¡´ì¬"]
            if exception_type:
                conditions.append(f"ì˜ˆì™¸ íƒ€ì… = {exception_type}")

            suggestions = [
                "stack_trace í•„ë“œê°€ ë¡œê·¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”",
                "exception_type í•„í„°ë¥¼ ì œê±°í•´ë³´ì„¸ìš”" if exception_type else "í•„í„° ì—†ì´ ì „ì²´ ì¡°íšŒ",
                "ì‹œê°„ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš” (time_hours=336 for 14ì¼)"
            ]

            return get_empty_result_message(
                conditions=", ".join(conditions),
                time_hours=time_hours,
                suggestions=suggestions
            ) + "\n\nâš ï¸ similarity_thresholdë¥¼ ì¡°ì •í•´ì„œ ì¬í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”."

        # í´ëŸ¬ìŠ¤í„°ë§
        clusters = []  # List[Dict]
        processed_indices = set()

        for i, hit in enumerate(hits):
            if i in processed_indices:
                continue

            source = hit["_source"]
            stack_trace = source.get("stack_trace", "")
            log_details = source.get("log_details", {})
            exc_type = log_details.get("exception_type", "Unknown")

            # ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„±
            cluster = {
                "exception_type": exc_type,
                "representative": source,
                "members": [source],
                "log_ids": [source.get("log_id")]
            }

            processed_indices.add(i)

            # ë‚˜ë¨¸ì§€ ë¡œê·¸ì™€ ìœ ì‚¬ë„ ë¹„êµ
            for j, other_hit in enumerate(hits[i+1:], start=i+1):
                if j in processed_indices:
                    continue

                other_source = other_hit["_source"]
                other_stack = other_source.get("stack_trace", "")
                other_exc = other_source.get("log_details", {}).get("exception_type", "Unknown")

                # ê°™ì€ ì˜ˆì™¸ íƒ€ì…ë§Œ ë¹„êµ
                if exc_type != other_exc:
                    continue

                # ìœ ì‚¬ë„ ê³„ì‚°
                similarity = calculate_text_similarity(stack_trace, other_stack)

                if similarity >= similarity_threshold:
                    cluster["members"].append(other_source)
                    cluster["log_ids"].append(other_source.get("log_id"))
                    processed_indices.add(j)

            clusters.append(cluster)

        # ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬
        clusters.sort(key=lambda x: len(x["members"]), reverse=True)

        # ê²°ê³¼ í¬ë§·íŒ…
        filter_suffix = f" ({exception_type})" if exception_type else ""
        lines = [
            f"## ğŸ” ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í´ëŸ¬ìŠ¤í„° ë¶„ì„{filter_suffix}",
            f"**ê¸°ê°„:** ìµœê·¼ {time_hours}ì‹œê°„",
            f"**ë¶„ì„ ìƒ˜í”Œ:** {len(hits)}ê±´",
            f"**ë°œê²¬ í´ëŸ¬ìŠ¤í„°:** {len(clusters)}ê°œ",
            f"**ìœ ì‚¬ë„ ì„ê³„ê°’:** {similarity_threshold}",
            ""
        ]

        # í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ì •ë³´
        for i, cluster in enumerate(clusters[:10], 1):  # ìƒìœ„ 10ê°œë§Œ
            rep = cluster["representative"]
            member_count = len(cluster["members"])
            exc_type = cluster["exception_type"]

            lines.append(f"### ê·¸ë£¹ {i}: {exc_type} ({member_count}ê±´)")
            lines.append("")

            # ëŒ€í‘œ ì—ëŸ¬ ì •ë³´
            class_name = rep.get("class_name", "")
            method_name = rep.get("method_name", "")
            service = rep.get("service_name", "unknown")
            message = rep.get("message", "")[:200]

            lines.append(f"**ë°œìƒ ìœ„ì¹˜:** {service}")
            if class_name and method_name:
                lines.append(f"**ë©”ì„œë“œ:** {class_name}.{method_name}")

            lines.append(f"**ë©”ì‹œì§€:** {message}")

            # ì„œë¹„ìŠ¤ ë¶„í¬
            services = [m.get("service_name", "unknown") for m in cluster["members"]]
            service_counts = Counter(services)
            if len(service_counts) > 1:
                lines.append(f"**ì„œë¹„ìŠ¤ ë¶„í¬:** {', '.join([f'{s}({c})' for s, c in service_counts.most_common(3)])}")

            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìƒ˜í”Œ (ê°„ëµ)
            stack_trace = rep.get("stack_trace", "")
            stack_lines = stack_trace.split("\n")

            if stack_lines:
                lines.append("")
                lines.append("**ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìƒ˜í”Œ (ì²˜ìŒ 5ì¤„):**")
                lines.append("```")
                for line in stack_lines[:5]:
                    if line.strip():
                        lines.append(line.strip())
                lines.append("...")
                lines.append("```")

            # log_id ëª©ë¡ (ì²˜ìŒ 5ê°œ)
            lines.append("")
            lines.append(f"**í¬í•¨ log_id:** {', '.join([str(lid) for lid in cluster['log_ids'][:5]])}")
            if member_count > 5:
                lines.append(f"(ì™¸ {member_count - 5}ê±´)")

            lines.append("")

        # ìš”ì•½
        lines.append("### ğŸ’¡ í´ëŸ¬ìŠ¤í„°ë§ ìš”ì•½")
        lines.append("")

        total_errors = sum([len(c["members"]) for c in clusters])
        lines.append(f"- **ì´ ì—ëŸ¬:** {total_errors}ê±´")
        lines.append(f"- **í´ëŸ¬ìŠ¤í„° ìˆ˜:** {len(clusters)}ê°œ")

        if clusters:
            largest_cluster = clusters[0]
            largest_percentage = (len(largest_cluster["members"]) / total_errors * 100)
            lines.append(f"- **ìµœëŒ€ í´ëŸ¬ìŠ¤í„°:** {largest_cluster['exception_type']} ({len(largest_cluster['members'])}ê±´, {largest_percentage:.1f}%)")

        # ì¤‘ë³µ ì—ëŸ¬ í†µê³„
        single_occurrence = len([c for c in clusters if len(c["members"]) == 1])
        multiple_occurrence = len(clusters) - single_occurrence

        lines.append(f"- **ë°˜ë³µ ì—ëŸ¬ íŒ¨í„´:** {multiple_occurrence}ê°œ (í´ëŸ¬ìŠ¤í„° í¬ê¸° 2+)")
        lines.append(f"- **ë‹¨ë… ì—ëŸ¬:** {single_occurrence}ê°œ")

        # ê¶Œì¥ ì¡°ì¹˜
        lines.append("")
        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
        lines.append("")

        if clusters:
            top_cluster = clusters[0]
            lines.append(f"1. **{top_cluster['exception_type']} ìš°ì„  ìˆ˜ì •** ({len(top_cluster['members'])}ê±´ìœ¼ë¡œ ê°€ì¥ ë§ìŒ)")

            if len(top_cluster["members"]) > 5:
                lines.append(f"   â†’ ê°™ì€ ì›ì¸ìœ¼ë¡œ ë°˜ë³µ ë°œìƒ ì¤‘, ê·¼ë³¸ ì›ì¸ íŒŒì•… í•„ìš”")

        if multiple_occurrence > 0:
            lines.append(f"2. **ë°˜ë³µ íŒ¨í„´ {multiple_occurrence}ê°œ ì§‘ì¤‘ ë¶„ì„** (íš¨ìœ¨ì  ìˆ˜ì • ê°€ëŠ¥)")

        if single_occurrence > 10:
            lines.append(f"3. **ë‹¨ë… ì—ëŸ¬ {single_occurrence}ê°œ ëª¨ë‹ˆí„°ë§** (ì¬ë°œ ì‹œ íŒ¨í„´ í™•ì¸)")

        return "\n".join(lines)

    except Exception as e:
        return f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def detect_concurrency_issues(
    project_uuid: str,
    time_hours: int = 24,
    min_thread_errors: int = 3
) -> str:
    """
    ìŠ¤ë ˆë“œ ì´ë¦„ íŒ¨í„´ìœ¼ë¡œ ë™ì‹œì„± ì´ìŠˆë¥¼ íƒì§€í•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… thread_nameìœ¼ë¡œ ê·¸ë£¹í•‘
    - âœ… ê°™ì€ ìŠ¤ë ˆë“œì—ì„œ ì§§ì€ ì‹œê°„ ë‚´ ë‹¤ìˆ˜ ì—ëŸ¬ íƒì§€
    - âœ… exception_typeì— "Deadlock", "Timeout", "Lock" í¬í•¨ ì‹œ ê²½ê³ 
    - âœ… ìŠ¤ë ˆë“œ í’€ ê³ ê°ˆ íŒ¨í„´ ì‹ë³„
    - âŒ ì‹¤ì œ ë°ë“œë½ ë¶„ì„ì€ JVM í”„ë¡œíŒŒì¼ë§ í•„ìš”

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ë™ì‹œì„± ë¬¸ì œ ìˆì–´?"
    2. "ê°™ì€ ìŠ¤ë ˆë“œì—ì„œ ë°˜ë³µ ì—ëŸ¬"
    3. "ë°ë“œë½ ì˜ì‹¬ íŒ¨í„´"
    4. "ìŠ¤ë ˆë“œ í’€ ì´ìŠˆ íƒì§€"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - thread_nameì´ ì—†ëŠ” ë¡œê·¸ëŠ” ì œì™¸ë©ë‹ˆë‹¤
    - ê¸°ë³¸ ë¶„ì„ ê¸°ê°„ì€ 24ì‹œê°„ì…ë‹ˆë‹¤

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        min_thread_errors: ì˜ì‹¬ ìŠ¤ë ˆë“œ ìµœì†Œ ì—ëŸ¬ ìˆ˜ (ê¸°ë³¸ 3ê±´)

    ê´€ë ¨ ë„êµ¬:
    - detect_resource_issues: ë¦¬ì†ŒìŠ¤ ì´ìŠˆ (ë©”ëª¨ë¦¬, CPU)
    - analyze_error_by_layer: ë ˆì´ì–´ë³„ ì—ëŸ¬ ë¶„ì„

    Returns:
        ë™ì‹œì„± ì´ìŠˆ íŒ¨í„´, ì˜ì‹¬ ìŠ¤ë ˆë“œ ëª©ë¡
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    try:
        # OpenSearch ê²€ìƒ‰
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 200,  # ë™ì‹œì„± ì´ìŠˆ ë¶„ì„ìš©
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"level": "ERROR"}},
                            {"exists": {"field": "thread_name"}},
                            {"range": {
                                "timestamp": {
                                    "gte": start_time.isoformat() + "Z",
                                    "lte": end_time.isoformat() + "Z"
                                }
                            }}
                        ]
                    }
                },
                "sort": [{"timestamp": "asc"}],
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS
            }
        )

        hits = results.get("hits", {}).get("hits", [])

        if not hits:
            from app.tools.response_templates import get_no_problem_message
            return get_no_problem_message(
                analysis_type="ë™ì‹œì„± ë¬¸ì œ",
                time_hours=time_hours,
                note="ë¡œê·¸ì— thread_name í•„ë“œê°€ ì—†ìœ¼ë©´ ì´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )

        # ìŠ¤ë ˆë“œë³„ ì—ëŸ¬ ê·¸ë£¹í•‘
        thread_errors = defaultdict(list)  # {thread_name: [error_logs]}

        for hit in hits:
            source = hit["_source"]
            thread_name = source.get("thread_name", "unknown")
            thread_errors[thread_name].append(source)

        # ì˜ì‹¬ ìŠ¤ë ˆë“œ í•„í„°ë§
        suspicious_threads = {}

        for thread, errors in thread_errors.items():
            if len(errors) < min_thread_errors:
                continue

            # ì‹œê°„ ê°„ê²© ë¶„ì„ (ì§§ì€ ì‹œê°„ ë‚´ ë‹¤ìˆ˜ ì—ëŸ¬ = ì˜ì‹¬)
            timestamps = [e.get("timestamp", "") for e in errors]
            timestamps_dt = []

            for ts in timestamps:
                try:
                    timestamps_dt.append(datetime.fromisoformat(ts.replace("Z", "")))
                except:
                    pass

            if len(timestamps_dt) < 2:
                continue

            # í‰ê·  ê°„ê²© ê³„ì‚°
            intervals = []
            for i in range(len(timestamps_dt) - 1):
                interval = (timestamps_dt[i+1] - timestamps_dt[i]).total_seconds()
                intervals.append(interval)

            avg_interval = sum(intervals) / len(intervals) if intervals else 0

            # ë™ì‹œì„± í‚¤ì›Œë“œ íƒì§€
            concurrency_keywords = ["deadlock", "timeout", "lock", "concurrent", "pool"]
            keyword_matches = []

            for error in errors:
                message = error.get("message", "").lower()
                exc_type = error.get("log_details", {}).get("exception_type", "").lower()

                for keyword in concurrency_keywords:
                    if keyword in message or keyword in exc_type:
                        keyword_matches.append(keyword)

            suspicious_threads[thread] = {
                "error_count": len(errors),
                "avg_interval": avg_interval,
                "keywords": list(set(keyword_matches)),
                "errors": errors
            }

        # ê²°ê³¼ í¬ë§·íŒ…
        lines = [
            f"## ğŸ§µ ë™ì‹œì„± ì´ìŠˆ íƒì§€",
            f"**ê¸°ê°„:** ìµœê·¼ {time_hours}ì‹œê°„",
            f"**ë¶„ì„ ìŠ¤ë ˆë“œ:** {len(thread_errors)}ê°œ",
            f"**ì˜ì‹¬ ìŠ¤ë ˆë“œ:** {len(suspicious_threads)}ê°œ",
            ""
        ]

        if not suspicious_threads:
            lines.append("### ğŸŸ¢ ë°œê²¬ëœ ë™ì‹œì„± ì´ìŠˆ ì—†ìŒ")
            lines.append("")
            lines.append("ìµœê·¼ ê¸°ê°„ ë™ì•ˆ ë™ì‹œì„± ê´€ë ¨ ë¬¸ì œ íŒ¨í„´ì´ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "\n".join(lines)

        # ì˜ì‹¬ ìŠ¤ë ˆë“œ ìƒì„¸ ì •ë³´
        lines.append("### ğŸš¨ ì˜ì‹¬ ìŠ¤ë ˆë“œ ëª©ë¡")
        lines.append("")

        # ì—ëŸ¬ ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_threads = sorted(suspicious_threads.items(), key=lambda x: x[1]["error_count"], reverse=True)

        for i, (thread_name, info) in enumerate(sorted_threads[:10], 1):
            error_count = info["error_count"]
            avg_interval = info["avg_interval"]
            keywords = info["keywords"]

            lines.append(f"#### {i}. {thread_name}")
            lines.append(f"- **ì—ëŸ¬ ìˆ˜:** {error_count}ê±´")
            lines.append(f"- **í‰ê·  ê°„ê²©:** {avg_interval:.1f}ì´ˆ")

            if keywords:
                lines.append(f"- **ğŸ”´ ë™ì‹œì„± í‚¤ì›Œë“œ íƒì§€:** {', '.join(keywords)}")

            # ì—ëŸ¬ íŒ¨í„´ íŒì •
            if avg_interval < 5 and error_count >= 3:
                lines.append(f"- **íŒ¨í„´:** ğŸ”´ ì§§ì€ ì‹œê°„ ë‚´ ë°˜ë³µ ì—ëŸ¬ (ë™ì‹œì„± ì´ìŠˆ ì˜ì‹¬)")
            elif "deadlock" in keywords or "lock" in keywords:
                lines.append(f"- **íŒ¨í„´:** ğŸ”´ ë°ë“œë½ ë˜ëŠ” ë½ ê²½í•© ì˜ì‹¬")
            elif "timeout" in keywords or "pool" in keywords:
                lines.append(f"- **íŒ¨í„´:** ğŸŸ¡ ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ")

            # ëŒ€í‘œ ì—ëŸ¬ ë©”ì‹œì§€
            first_error = info["errors"][0]
            message = first_error.get("message", "")[:150]
            lines.append(f"- **ì—ëŸ¬ ì˜ˆì‹œ:** {message}")

            lines.append("")

        # ì „ì²´ í†µê³„
        lines.append("### ğŸ“Š í†µê³„ ìš”ì•½")
        lines.append("")

        total_suspicious_errors = sum([t["error_count"] for t in suspicious_threads.values()])
        lines.append(f"- **ì˜ì‹¬ ìŠ¤ë ˆë“œ ì´ ì—ëŸ¬:** {total_suspicious_errors}ê±´")

        # í‚¤ì›Œë“œ í†µê³„
        all_keywords = []
        for info in suspicious_threads.values():
            all_keywords.extend(info["keywords"])

        if all_keywords:
            keyword_counts = Counter(all_keywords)
            lines.append(f"- **ê°€ì¥ ë§ì€ í‚¤ì›Œë“œ:** {', '.join([f'{k}({c})' for k, c in keyword_counts.most_common(3)])}")

        # ê¶Œì¥ ì¡°ì¹˜
        lines.append("")
        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
        lines.append("")

        if "deadlock" in all_keywords or "lock" in all_keywords:
            lines.append("1. **ë°ë“œë½ ë¶„ì„:** JVM Thread Dump ìˆ˜ì§‘ ë° ë¶„ì„")
            lines.append("2. **ë½ ê²½í•© í•´ì†Œ:** synchronized ë¸”ë¡ ìµœì†Œí™”, Lock íƒ€ì„ì•„ì›ƒ ì„¤ì •")

        if "timeout" in all_keywords or "pool" in all_keywords:
            lines.append("3. **ì»¤ë„¥ì…˜ í’€ ì¦ì„¤:** DB ì»¤ë„¥ì…˜ í’€, ìŠ¤ë ˆë“œ í’€ í¬ê¸° ì¦ê°€")
            lines.append("4. **íƒ€ì„ì•„ì›ƒ ì¡°ì •:** ì ì ˆí•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •")

        if sorted_threads:
            worst_thread = sorted_threads[0][0]
            lines.append(f"5. **{worst_thread} ìŠ¤ë ˆë“œ ì§‘ì¤‘ ë¶„ì„:** ì½”ë“œ ë¦¬ë·° ë° í”„ë¡œíŒŒì¼ë§")

        lines.append("")
        lines.append("ğŸ’¡ **ì¶”ê°€ ë¶„ì„:** detect_resource_issues ë„êµ¬ë¡œ ë©”ëª¨ë¦¬/CPU ì´ìŠˆ í™•ì¸")

        return "\n".join(lines)

    except Exception as e:
        return f"ë™ì‹œì„± ì´ìŠˆ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def detect_recurring_errors(
    project_uuid: str,
    time_hours: int = 720,  # 30ì¼
    min_occurrences: int = 3
) -> str:
    """
    ì£¼ê¸°ì ìœ¼ë¡œ ë°˜ë³µë˜ëŠ” ì—ëŸ¬ë¥¼ íƒì§€í•©ë‹ˆë‹¤ (ë§¤ì¼ ê°™ì€ ì‹œê°„, ë§¤ì£¼ íŠ¹ì • ìš”ì¼).

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… exception_type + class_nameìœ¼ë¡œ ì—ëŸ¬ ê·¸ë£¹í•‘
    - âœ… ë°œìƒ ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ì‹œê°„ëŒ€, ìš”ì¼)
    - âœ… ì£¼ê¸°ì„± íƒì§€ (ë§¤ì¼, ë§¤ì£¼, íŠ¹ì • ì‹œê°„ëŒ€)
    - âœ… ë°°ì¹˜ ì‘ì—… ë˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ ì´ìŠˆ ì‹ë³„
    - âŒ ë³µì¡í•œ í†µê³„ ë¶„ì„ì€ ë¯¸ì§€ì›

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ì£¼ê¸°ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ìˆì–´?"
    2. "ë§¤ì¼ ê°™ì€ ì‹œê°„ì— ë°œìƒí•˜ëŠ” ì—ëŸ¬"
    3. "ë°°ì¹˜ ì‘ì—… ì—ëŸ¬ íŒ¨í„´"
    4. "ìŠ¤ì¼€ì¤„ëŸ¬ ì´ìŠˆ íƒì§€"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - ê¸°ë³¸ ë¶„ì„ ê¸°ê°„ì€ 30ì¼ì…ë‹ˆë‹¤ (íŒ¨í„´ ë°œê²¬ì— í•„ìš”)
    - exception_typeì´ ì—†ìœ¼ë©´ ì •í™•ë„ ë‚®ìŒ

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 720ì‹œê°„=30ì¼)
        min_occurrences: íŒ¨í„´ ì¸ì‹ ìµœì†Œ ë°œìƒ íšŸìˆ˜ (ê¸°ë³¸ 3íšŒ)

    ê´€ë ¨ ë„êµ¬:
    - analyze_error_lifetime: ì—ëŸ¬ ìƒì¡´ ì‹œê°„
    - cluster_stack_traces: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§

    Returns:
        ì¬ë°œ ì£¼ê¸° íŒ¨í„´, ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ì—ëŸ¬ ëª©ë¡
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    try:
        # OpenSearch ê²€ìƒ‰
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 500,  # ì£¼ê¸° ë¶„ì„ìš© ì¶©ë¶„í•œ ìƒ˜í”Œ
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"level": "ERROR"}},
                            {"range": {
                                "timestamp": {
                                    "gte": start_time.isoformat() + "Z",
                                    "lte": end_time.isoformat() + "Z"
                                }
                            }}
                        ]
                    }
                },
                "sort": [{"timestamp": "asc"}],
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS
            }
        )

        hits = results.get("hits", {}).get("hits", [])

        if not hits:
            from app.tools.response_templates import get_no_problem_message
            return get_no_problem_message(
                analysis_type="ì—ëŸ¬",
                time_hours=time_hours,
                note="ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤"
            ) + "\n\nâš ï¸ min_occurrencesë¥¼ ì¡°ì •í•´ì„œ ì¬í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”."

        # ì—ëŸ¬ ê·¸ë£¹í•‘ (exception_type + class_name)
        error_groups = defaultdict(list)  # {error_key: [timestamps]}

        for hit in hits:
            source = hit["_source"]
            log_details = source.get("log_details", {})
            exc_type = log_details.get("exception_type", "Unknown")
            class_name = source.get("class_name", "Unknown")
            timestamp = source.get("timestamp", "")

            error_key = f"{exc_type} @ {class_name}"

            try:
                dt = datetime.fromisoformat(timestamp.replace("Z", ""))
                error_groups[error_key].append(dt)
            except:
                pass

        # íŒ¨í„´ ë¶„ì„
        recurring_patterns = []

        for error_key, timestamps in error_groups.items():
            if len(timestamps) < min_occurrences:
                continue

            # ì‹œê°„ëŒ€ ë¶„ì„ (hour of day)
            hours = [dt.hour for dt in timestamps]
            hour_counts = Counter(hours)

            # ìš”ì¼ ë¶„ì„ (0=Monday, 6=Sunday)
            weekdays = [dt.weekday() for dt in timestamps]
            weekday_counts = Counter(weekdays)

            # ì£¼ê¸°ì„± íŒì •
            patterns_found = []

            # 1. ë§¤ì¼ ê°™ì€ ì‹œê°„ (íŠ¹ì • ì‹œê°„ëŒ€ì— 80% ì´ìƒ ì§‘ì¤‘)
            most_common_hour = hour_counts.most_common(1)[0]
            if most_common_hour[1] / len(hours) >= 0.8:
                patterns_found.append(f"ë§¤ì¼ {most_common_hour[0]:02d}:00 ê²½")

            # 2. íŠ¹ì • ìš”ì¼ (íŠ¹ì • ìš”ì¼ì— 80% ì´ìƒ ì§‘ì¤‘)
            most_common_weekday = weekday_counts.most_common(1)[0]
            weekday_names = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
            if most_common_weekday[1] / len(weekdays) >= 0.8:
                patterns_found.append(f"ë§¤ì£¼ {weekday_names[most_common_weekday[0]]}ìš”ì¼")

            # 3. ì‹œê°„ ê°„ê²© ë¶„ì„ (ë™ì¼ ê°„ê²© ë°˜ë³µ)
            if len(timestamps) >= 3:
                intervals = []
                for i in range(len(timestamps) - 1):
                    interval_hours = (timestamps[i+1] - timestamps[i]).total_seconds() / 3600
                    intervals.append(interval_hours)

                # í‰ê·  ê°„ê²©
                avg_interval = sum(intervals) / len(intervals) if intervals else 0

                # ê°„ê²©ì´ ì¼ì •í•œì§€ (í‘œì¤€í¸ì°¨ í™•ì¸)
                if intervals:
                    variance = sum([(x - avg_interval) ** 2 for x in intervals]) / len(intervals)
                    std_dev = variance ** 0.5

                    # ê°„ê²©ì´ ë¹„êµì  ì¼ì • (í‘œì¤€í¸ì°¨ê°€ í‰ê· ì˜ 20% ì´í•˜)
                    if avg_interval > 0 and std_dev / avg_interval < 0.2:
                        if 23 <= avg_interval <= 25:  # í•˜ë£¨
                            patterns_found.append(f"ë§¤ì¼ (24ì‹œê°„ ê°„ê²©)")
                        elif 167 <= avg_interval <= 169:  # ì¼ì£¼ì¼
                            patterns_found.append(f"ë§¤ì£¼ (7ì¼ ê°„ê²©)")
                        else:
                            patterns_found.append(f"ì¼ì • ê°„ê²© ({avg_interval:.1f}ì‹œê°„)")

            if patterns_found:
                recurring_patterns.append({
                    "error_key": error_key,
                    "count": len(timestamps),
                    "patterns": patterns_found,
                    "first_occurrence": min(timestamps),
                    "last_occurrence": max(timestamps),
                    "hour_distribution": hour_counts.most_common(3),
                    "weekday_distribution": weekday_counts.most_common(3)
                })

        # ê²°ê³¼ í¬ë§·íŒ…
        lines = [
            f"## ğŸ” ì¬ë°œ ì£¼ê¸° ì—ëŸ¬ ë¶„ì„",
            f"**ê¸°ê°„:** ìµœê·¼ {time_hours}ì‹œê°„ ({time_hours//24}ì¼)",
            f"**ë¶„ì„ ì—ëŸ¬:** {len(error_groups)}ê°œ íƒ€ì…",
            f"**ì£¼ê¸° íŒ¨í„´ ë°œê²¬:** {len(recurring_patterns)}ê°œ",
            ""
        ]

        if not recurring_patterns:
            lines.append("### ğŸŸ¢ ì£¼ê¸°ì  ì—ëŸ¬ íŒ¨í„´ ì—†ìŒ")
            lines.append("")
            lines.append("ë¶„ì„ ê¸°ê°„ ë™ì•ˆ íŠ¹ì • ì£¼ê¸°ë¡œ ë°˜ë³µë˜ëŠ” ì—ëŸ¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            lines.append("")
            lines.append("ğŸ’¡ **ì°¸ê³ :** ë” ê¸´ ê¸°ê°„ ë¶„ì„ì„ ìœ„í•´ time_hoursë¥¼ ì¦ê°€ì‹œì¼œ ë³´ì„¸ìš” (ì˜ˆ: 1440=60ì¼)")
            return "\n".join(lines)

        # íŒ¨í„´ë³„ ìƒì„¸ ì •ë³´
        lines.append("### ğŸš¨ ì£¼ê¸°ì  ì—ëŸ¬ ëª©ë¡")
        lines.append("")

        # ë°œìƒ íšŸìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_patterns = sorted(recurring_patterns, key=lambda x: x["count"], reverse=True)

        for i, pattern in enumerate(sorted_patterns[:10], 1):
            error_key = pattern["error_key"]
            count = pattern["count"]
            patterns = pattern["patterns"]
            first = pattern["first_occurrence"]
            last = pattern["last_occurrence"]

            lines.append(f"#### {i}. {error_key}")
            lines.append(f"- **ë°œìƒ íšŸìˆ˜:** {count}íšŒ")
            lines.append(f"- **ê¸°ê°„:** {first.strftime('%Y-%m-%d')} ~ {last.strftime('%Y-%m-%d')}")
            lines.append(f"- **ğŸ” ì£¼ê¸° íŒ¨í„´:** {', '.join(patterns)}")

            # ì‹œê°„ëŒ€ ë¶„í¬
            hour_dist = pattern["hour_distribution"]
            hour_str = ", ".join([f"{h:02d}ì‹œ({c}íšŒ)" for h, c in hour_dist])
            lines.append(f"- **ì£¼ìš” ì‹œê°„ëŒ€:** {hour_str}")

            # ìš”ì¼ ë¶„í¬
            weekday_dist = pattern["weekday_distribution"]
            weekday_names = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
            weekday_str = ", ".join([f"{weekday_names[w]}ìš”ì¼({c}íšŒ)" for w, c in weekday_dist])
            lines.append(f"- **ì£¼ìš” ìš”ì¼:** {weekday_str}")

            lines.append("")

        # í†µê³„ ìš”ì•½
        lines.append("### ğŸ“Š ì£¼ê¸° íŒ¨í„´ í†µê³„")
        lines.append("")

        # íŒ¨í„´ íƒ€ì…ë³„ ì§‘ê³„
        all_pattern_types = []
        for p in recurring_patterns:
            all_pattern_types.extend(p["patterns"])

        pattern_type_counts = Counter(all_pattern_types)
        lines.append("**ì£¼ê¸° íƒ€ì… ë¶„í¬:**")
        for pattern_type, count in pattern_type_counts.most_common():
            lines.append(f"- {pattern_type}: {count}ê°œ ì—ëŸ¬")

        lines.append("")

        # ê¶Œì¥ ì¡°ì¹˜
        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
        lines.append("")

        # ë°°ì¹˜ ì‘ì—… ì˜ì‹¬
        batch_patterns = [p for p in recurring_patterns if any("ë§¤ì¼" in pat or "ë§¤ì£¼" in pat for pat in p["patterns"])]
        if batch_patterns:
            lines.append("1. **ë°°ì¹˜ ì‘ì—… ì ê²€:**")
            for p in batch_patterns[:3]:
                lines.append(f"   - {p['error_key']}: {', '.join(p['patterns'])}")

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • í™•ì¸
        lines.append("2. **ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • í™•ì¸:** Cron í‘œí˜„ì‹ ë° ì‹¤í–‰ ì¡°ê±´ ê²€í† ")
        lines.append("3. **ë¦¬ì†ŒìŠ¤ ê°€ìš©ì„±:** í•´ë‹¹ ì‹œê°„ëŒ€ì˜ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸")

        if sorted_patterns:
            worst_pattern = sorted_patterns[0]
            lines.append(f"4. **ìš°ì„  ìˆ˜ì • ëŒ€ìƒ:** {worst_pattern['error_key']} ({worst_pattern['count']}íšŒ ë°œìƒ)")

        lines.append("")
        lines.append("ğŸ’¡ **ì¶”ê°€ ë¶„ì„:** get_traffic_by_time ë„êµ¬ë¡œ í•´ë‹¹ ì‹œê°„ëŒ€ íŠ¸ë˜í”½ í™•ì¸")

        return "\n".join(lines)

    except Exception as e:
        return f"ì¬ë°œ ì£¼ê¸° ì—ëŸ¬ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def analyze_error_lifetime(
    project_uuid: str,
    exception_type: Optional[str] = None,
    time_hours: int = 720  # 30ì¼
) -> str:
    """
    ë™ì¼ ì—ëŸ¬ì˜ ìµœì´ˆ ë°œìƒ ~ í•´ê²° ì‹œì ê¹Œì§€ ì‹œê°„ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… exception_type + class_name + method_nameìœ¼ë¡œ ì—ëŸ¬ ê·¸ë£¹í•‘
    - âœ… ê° ì—ëŸ¬ì˜ ìµœì´ˆ ë°œìƒ ì‹œê°„ ~ ë§ˆì§€ë§‰ ë°œìƒ ì‹œê°„ ê³„ì‚°
    - âœ… 7ì¼ ì´ìƒ ì§€ì† â†’ "ì¥ê¸° ë¯¸í•´ê²°"
    - âœ… ìµœê·¼ 24ì‹œê°„ ë¯¸ë°œìƒ â†’ "í•´ê²°ë¨"
    - âŒ ì‹¤ì œ ì½”ë“œ ìˆ˜ì • ì—¬ë¶€ëŠ” ì•Œ ìˆ˜ ì—†ìŒ (ë¡œê·¸ ê¸°ë°˜ ì¶”ì •)

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ì´ ì—ëŸ¬ê°€ ì–¸ì œë¶€í„° ë°œìƒí–ˆì–´?"
    2. "ì–¼ë§ˆë‚˜ ì˜¤ë˜ ì§€ì†ëœ ë¬¸ì œì•¼?"
    3. "í•´ê²°ë˜ì§€ ì•Šì€ ì˜¤ë˜ëœ ì—ëŸ¬"
    4. "ì—ëŸ¬ í•´ê²° ì†ë„ í‰ê°€"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - ê¸°ë³¸ ë¶„ì„ ê¸°ê°„ì€ 30ì¼ì…ë‹ˆë‹¤
    - "í•´ê²°ë¨" íŒì •ì€ ë¡œê·¸ ë¶€ì¬ ê¸°ì¤€ (ì‹¤ì œ ìˆ˜ì •ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        exception_type: ì˜ˆì™¸ íƒ€ì… í•„í„° (ì„ íƒ)
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 720ì‹œê°„=30ì¼)

    ê´€ë ¨ ë„êµ¬:
    - detect_recurring_errors: ì¬ë°œ ì£¼ê¸° ë¶„ì„
    - cluster_stack_traces: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§

    Returns:
        ì—ëŸ¬ ìƒì¡´ ì‹œê°„, ì¥ê¸° ë¯¸í•´ê²° ì—ëŸ¬, í•´ê²°ëœ ì—ëŸ¬
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}},
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if exception_type:
        must_clauses.append({
            "bool": {
                "should": [
                    {"term": {"log_details.exception_type": exception_type}},
                    {"match": {"message": exception_type}}
                ],
                "minimum_should_match": 1
            }
        })

    try:
        # OpenSearch ê²€ìƒ‰
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 500,
                "query": {"bool": {"must": must_clauses}},
                "sort": [{"timestamp": "asc"}],
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS
            }
        )

        hits = results.get("hits", {}).get("hits", [])

        if not hits:
            filter_msg = f" ({exception_type})" if exception_type else ""
            return (
                f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì—ëŸ¬{filter_msg}ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                f"ğŸŸ¢ ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤."
            )

        # ì—ëŸ¬ ê·¸ë£¹í•‘ (exception_type + class_name + method_name)
        error_lifetimes = defaultdict(lambda: {"first": None, "last": None, "count": 0, "sample": None})

        for hit in hits:
            source = hit["_source"]
            log_details = source.get("log_details", {})
            exc_type = log_details.get("exception_type", "Unknown")
            class_name = source.get("class_name", "Unknown")
            method_name = source.get("method_name", "Unknown")
            timestamp = source.get("timestamp", "")

            error_key = f"{exc_type} @ {class_name}.{method_name}"

            try:
                dt = datetime.fromisoformat(timestamp.replace("Z", ""))

                if error_lifetimes[error_key]["first"] is None or dt < error_lifetimes[error_key]["first"]:
                    error_lifetimes[error_key]["first"] = dt

                if error_lifetimes[error_key]["last"] is None or dt > error_lifetimes[error_key]["last"]:
                    error_lifetimes[error_key]["last"] = dt

                error_lifetimes[error_key]["count"] += 1

                if error_lifetimes[error_key]["sample"] is None:
                    error_lifetimes[error_key]["sample"] = source
            except:
                pass

        # ìƒì¡´ ì‹œê°„ ê³„ì‚°
        error_analysis = []
        now = datetime.utcnow()

        for error_key, data in error_lifetimes.items():
            first = data["first"]
            last = data["last"]

            if not first or not last:
                continue

            # ìƒì¡´ ê¸°ê°„ (ì¼)
            lifetime_hours = (last - first).total_seconds() / 3600
            lifetime_days = lifetime_hours / 24

            # ë§ˆì§€ë§‰ ë°œìƒ ì´í›„ ê²½ê³¼ ì‹œê°„
            time_since_last = (now - last).total_seconds() / 3600

            # ìƒíƒœ íŒì •
            if time_since_last <= 24:
                status = "ğŸ”´ ì§„í–‰ ì¤‘"
            elif time_since_last <= 72:
                status = "ğŸŸ¡ ìµœê·¼ í•´ê²°"
            else:
                status = "ğŸŸ¢ í•´ê²°ë¨"

            # ì¥ê¸° ë¯¸í•´ê²°
            if lifetime_days >= 7 and time_since_last <= 24:
                status = "ğŸš¨ ì¥ê¸° ë¯¸í•´ê²°"

            error_analysis.append({
                "error_key": error_key,
                "first": first,
                "last": last,
                "lifetime_days": lifetime_days,
                "time_since_last": time_since_last,
                "count": data["count"],
                "status": status,
                "sample": data["sample"]
            })

        # ê²°ê³¼ í¬ë§·íŒ…
        filter_suffix = f" ({exception_type})" if exception_type else ""
        lines = [
            f"## â³ ì—ëŸ¬ ìƒì¡´ ì‹œê°„ ë¶„ì„{filter_suffix}",
            f"**ê¸°ê°„:** ìµœê·¼ {time_hours}ì‹œê°„ ({time_hours//24}ì¼)",
            f"**ë¶„ì„ ì—ëŸ¬:** {len(error_analysis)}ê°œ íƒ€ì…",
            ""
        ]

        # ì—ëŸ¬ë³„ ìƒì¡´ ì‹œê°„ í…Œì´ë¸”
        lines.append("### ğŸ“‹ ì—ëŸ¬ë³„ ìƒì¡´ ê¸°ê°„")
        lines.append("")
        lines.append("| ì—ëŸ¬ | ìµœì´ˆ ë°œìƒ | ë§ˆì§€ë§‰ ë°œìƒ | ì§€ì† ê¸°ê°„ | ìƒíƒœ | ë°œìƒ íšŸìˆ˜ |")
        lines.append("|------|-----------|-------------|-----------|------|-----------|")

        # ì§€ì† ê¸°ê°„ ê¸°ì¤€ ì •ë ¬ (ê¸´ ê²ƒë¶€í„°)
        sorted_errors = sorted(error_analysis, key=lambda x: x["lifetime_days"], reverse=True)

        for err in sorted_errors[:20]:  # ìƒìœ„ 20ê°œ
            first_str = err["first"].strftime("%m-%d %H:%M")
            last_str = err["last"].strftime("%m-%d %H:%M")
            lifetime_str = f"{err['lifetime_days']:.1f}ì¼" if err['lifetime_days'] >= 1 else f"{err['lifetime_days']*24:.1f}ì‹œê°„"

            lines.append(
                f"| {err['error_key'][:50]} | {first_str} | {last_str} | **{lifetime_str}** | {err['status']} | {err['count']} |"
            )

        lines.append("")

        # ìƒíƒœë³„ ë¶„ë¥˜
        status_groups = defaultdict(list)
        for err in error_analysis:
            status_groups[err["status"]].append(err)

        lines.append("### ğŸ“Š ìƒíƒœë³„ í†µê³„")
        lines.append("")

        for status in ["ğŸš¨ ì¥ê¸° ë¯¸í•´ê²°", "ğŸ”´ ì§„í–‰ ì¤‘", "ğŸŸ¡ ìµœê·¼ í•´ê²°", "ğŸŸ¢ í•´ê²°ë¨"]:
            count = len(status_groups[status])
            if count > 0:
                lines.append(f"- **{status}**: {count}ê°œ ({count / len(error_analysis) * 100:.1f}%)")

        # ì¥ê¸° ë¯¸í•´ê²° ì—ëŸ¬ ê°•ì¡°
        long_term_errors = [e for e in error_analysis if "ì¥ê¸° ë¯¸í•´ê²°" in e["status"]]

        if long_term_errors:
            lines.append("")
            lines.append("### ğŸš¨ ì¥ê¸° ë¯¸í•´ê²° ì—ëŸ¬ (7ì¼ ì´ìƒ)")
            lines.append("")

            for i, err in enumerate(sorted(long_term_errors, key=lambda x: x["lifetime_days"], reverse=True)[:5], 1):
                lines.append(f"{i}. **{err['error_key']}**")
                lines.append(f"   - ì§€ì† ê¸°ê°„: {err['lifetime_days']:.0f}ì¼")
                lines.append(f"   - ìµœì´ˆ ë°œìƒ: {err['first'].strftime('%Y-%m-%d %H:%M')}")
                lines.append(f"   - ë°œìƒ íšŸìˆ˜: {err['count']}íšŒ")

                # ìƒ˜í”Œ ë©”ì‹œì§€
                sample = err['sample']
                if sample:
                    message = sample.get("message", "")[:150]
                    lines.append(f"   - ë©”ì‹œì§€: {message}")

                lines.append("")

        # í‰ê·  í•´ê²° ì‹œê°„
        resolved_errors = [e for e in error_analysis if "í•´ê²°ë¨" in e["status"] or "ìµœê·¼ í•´ê²°" in e["status"]]
        if resolved_errors:
            avg_resolution_days = sum([e["lifetime_days"] for e in resolved_errors]) / len(resolved_errors)
            lines.append("")
            lines.append(f"### âš¡ í‰ê·  í•´ê²° ì‹œê°„: {avg_resolution_days:.1f}ì¼")
            lines.append(f"(í•´ê²°ëœ ì—ëŸ¬ {len(resolved_errors)}ê°œ ê¸°ì¤€)")

        # ê¶Œì¥ ì¡°ì¹˜
        lines.append("")
        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
        lines.append("")

        if long_term_errors:
            worst_error = max(long_term_errors, key=lambda x: x["lifetime_days"])
            lines.append(f"1. **ì¦‰ì‹œ ìˆ˜ì • í•„ìš”:** {worst_error['error_key']} ({worst_error['lifetime_days']:.0f}ì¼ ì§€ì†)")

        ongoing_errors = status_groups.get("ğŸ”´ ì§„í–‰ ì¤‘", [])
        if len(ongoing_errors) > 5:
            lines.append(f"2. **ì§„í–‰ ì¤‘ ì—ëŸ¬ {len(ongoing_errors)}ê°œ ëª¨ë‹ˆí„°ë§:** ì¬ë°œ ë°©ì§€ ëŒ€ì±… ìˆ˜ë¦½")

        if resolved_errors and avg_resolution_days > 7:
            lines.append(f"3. **í•´ê²° ì†ë„ ê°œì„ :** í‰ê·  {avg_resolution_days:.1f}ì¼ â†’ ëª©í‘œ 3ì¼ ì´ë‚´")

        lines.append("")
        lines.append("ğŸ’¡ **ì°¸ê³ :** 'í•´ê²°ë¨'ì€ ìµœê·¼ 3ì¼ê°„ ë¯¸ë°œìƒ ê¸°ì¤€ì´ë©°, ì‹¤ì œ ìˆ˜ì •ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        return "\n".join(lines)

    except Exception as e:
        return f"ì—ëŸ¬ ìƒì¡´ ì‹œê°„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
