"""
ì•Œë¦¼ ë„êµ¬ (Alert Tools) - P1 Priority
- ì•Œë¦¼ ì¡°ê±´ í‰ê°€, ë¦¬ì†ŒìŠ¤ ì´ìŠˆ ê°ì§€
"""

from typing import Optional, List
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


@tool
async def evaluate_alert_conditions(
    project_uuid: str,
    time_window_minutes: int = 5,  # ìµœê·¼ Në¶„
    error_rate_threshold: float = 5.0,  # ì—ëŸ¬ìœ¨ ì„ê³„ì¹˜ (%)
    spike_threshold: float = 50.0,  # ê¸‰ì¦ íŒë‹¨ ì„ê³„ì¹˜ (%)
    service_name: Optional[str] = None
) -> str:
    """
    ì•Œë¦¼ì´ í•„ìš”í•œ ìƒí™©ì¸ì§€ í‰ê°€í•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - "ì•Œë¦¼ì„ ë³´ë‚´ì•¼ í•  ì‹¬ê°í•œ ìƒí™©ì´ ìˆë‚˜ìš”?"
    - "ì—ëŸ¬ìœ¨ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ì—ˆë‚˜ìš”?"
    - "ë¹„ì •ìƒì ì¸ íŠ¸ë˜í”½ ê¸‰ì¦ì´ ìˆë‚˜ìš”?"
    - "ë°˜ë³µë˜ëŠ” CRITICAL ì—ëŸ¬ê°€ ìˆë‚˜ìš”?"

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_window_minutes: í‰ê°€ ì‹œê°„ ìœˆë„ìš° (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ 5ë¶„)
        error_rate_threshold: ì—ëŸ¬ìœ¨ ì„ê³„ì¹˜ (%, ê¸°ë³¸ 5.0%)
        spike_threshold: ê¸‰ì¦ íŒë‹¨ ì„ê³„ì¹˜ (%, ê¸°ë³¸ 50%)
        service_name: ì„œë¹„ìŠ¤ ì´ë¦„ í•„í„° (ì„ íƒ)

    ì°¸ê³ : project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        ì•Œë¦¼ í•„ìš” ì—¬ë¶€, ìœ„ë°˜í•œ ì¡°ê±´, ì‹¬ê°ë„
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    recent_start = end_time - timedelta(minutes=time_window_minutes)
    baseline_start = end_time - timedelta(minutes=time_window_minutes * 3)  # ë¹„êµ ê¸°ì¤€ (3ë°° ê¸°ê°„)
    baseline_end = recent_start

    # Query êµ¬ì„±
    must_clauses = []
    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # OpenSearch Aggregation Query (ë‹¤ì¤‘ í‰ê°€)
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}} if must_clauses else {"match_all": {}},
                "aggs": {
                    # ìµœê·¼ ìœˆë„ìš°
                    "recent_window": {
                        "filter": {
                            "range": {
                                "timestamp": {
                                    "gte": recent_start.isoformat() + "Z",
                                    "lte": end_time.isoformat() + "Z"
                                }
                            }
                        },
                        "aggs": {
                            "total_logs": {"value_count": {"field": "log_id"}},
                            "error_logs": {"filter": {"term": {"level": "ERROR"}}},
                            "critical_errors": {
                                "filter": {
                                    "bool": {
                                        "must": [
                                            {"term": {"level": "ERROR"}},
                                            {
                                                "bool": {
                                                    "should": [
                                                        {"match": {"log_details.exception_type": "Database"}},
                                                        {"match": {"log_details.exception_type": "OutOfMemory"}},
                                                        {"match": {"log_details.exception_type": "Connection"}},
                                                        {"match": {"message": "OutOfMemoryError"}},
                                                        {"match": {"message": "StackOverflowError"}},
                                                        {"range": {"log_details.response_status": {"gte": 500, "lt": 600}}}
                                                    ]
                                                }
                                            }
                                        ]
                                    }
                                }
                            },
                            "p95_latency": {
                                "percentiles": {
                                    "field": "log_details.execution_time",
                                    "percents": [95]
                                }
                            }
                        }
                    },
                    # ê¸°ì¤€ ìœˆë„ìš° (ë¹„êµìš©)
                    "baseline_window": {
                        "filter": {
                            "range": {
                                "timestamp": {
                                    "gte": baseline_start.isoformat() + "Z",
                                    "lt": baseline_end.isoformat() + "Z"
                                }
                            }
                        },
                        "aggs": {
                            "total_logs": {"value_count": {"field": "log_id"}},
                            "error_logs": {"filter": {"term": {"level": "ERROR"}}}
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        aggs = results.get("aggregations", {})
        recent = aggs.get("recent_window", {})
        baseline = aggs.get("baseline_window", {})

        # ìµœê·¼ ìœˆë„ìš° ë°ì´í„°
        recent_total = recent.get("total_logs", {}).get("value", 0)
        recent_errors = recent.get("error_logs", {}).get("doc_count", 0)
        recent_critical = recent.get("critical_errors", {}).get("doc_count", 0)
        recent_error_rate = (recent_errors / recent_total * 100) if recent_total > 0 else 0
        recent_p95 = recent.get("p95_latency", {}).get("values", {}).get("95.0")

        # ê¸°ì¤€ ìœˆë„ìš° ë°ì´í„°
        baseline_total = baseline.get("total_logs", {}).get("value", 0)
        baseline_errors = baseline.get("error_logs", {}).get("doc_count", 0)
        baseline_error_rate = (baseline_errors / baseline_total * 100) if baseline_total > 0 else 0

        # íŠ¸ë˜í”½ ë³€í™”ìœ¨
        if baseline_total > 0:
            traffic_change = ((recent_total - baseline_total) / baseline_total * 100)
        else:
            traffic_change = 100 if recent_total > 0 else 0

        # ì•Œë¦¼ ì¡°ê±´ í‰ê°€
        alerts = []
        severity_scores = []

        # ì¡°ê±´ 1: ì—ëŸ¬ìœ¨ ì„ê³„ì¹˜ ì´ˆê³¼
        if recent_error_rate > error_rate_threshold:
            severity = "ğŸ”´ CRITICAL" if recent_error_rate > error_rate_threshold * 2 else "ğŸŸ  HIGH"
            alerts.append({
                "condition": "ì—ëŸ¬ìœ¨ ì„ê³„ì¹˜ ì´ˆê³¼",
                "severity": severity,
                "current": f"{recent_error_rate:.2f}%",
                "threshold": f"{error_rate_threshold}%",
                "message": f"ì—ëŸ¬ìœ¨ì´ {recent_error_rate:.2f}%ë¡œ ì„ê³„ì¹˜ {error_rate_threshold}%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."
            })
            severity_scores.append(3 if "CRITICAL" in severity else 2)

        # ì¡°ê±´ 2: CRITICAL ì—ëŸ¬ ë°œìƒ
        if recent_critical > 0:
            severity = "ğŸ”´ CRITICAL" if recent_critical >= 3 else "ğŸŸ  HIGH"
            alerts.append({
                "condition": "CRITICAL ì—ëŸ¬ ë°œìƒ",
                "severity": severity,
                "current": f"{recent_critical}ê±´",
                "threshold": "0ê±´",
                "message": f"Database/OutOfMemory/5xx ë“± ì‹¬ê°í•œ ì—ëŸ¬ê°€ {recent_critical}ê±´ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            })
            severity_scores.append(3 if recent_critical >= 3 else 2)

        # ì¡°ê±´ 3: íŠ¸ë˜í”½ ê¸‰ì¦/ê¸‰ê°
        if abs(traffic_change) > spike_threshold:
            if traffic_change > 0:
                severity = "ğŸŸ¡ MEDIUM"
                direction = "ê¸‰ì¦"
                severity_scores.append(1)
            else:
                severity = "ğŸŸ¡ MEDIUM"
                direction = "ê¸‰ê°"
                severity_scores.append(1)

            alerts.append({
                "condition": f"íŠ¸ë˜í”½ {direction}",
                "severity": severity,
                "current": f"{traffic_change:+.1f}%",
                "threshold": f"Â±{spike_threshold}%",
                "message": f"íŠ¸ë˜í”½ì´ ê¸°ì¤€ ëŒ€ë¹„ {traffic_change:+.1f}% {direction}í–ˆìŠµë‹ˆë‹¤."
            })

        # ì¡°ê±´ 4: P95 ë ˆì´í„´ì‹œ ì´ìƒ (5ì´ˆ ì´ìƒ)
        if recent_p95 and recent_p95 > 5000:
            severity = "ğŸŸ¡ MEDIUM" if recent_p95 < 10000 else "ğŸŸ  HIGH"
            alerts.append({
                "condition": "ì‘ë‹µ ì‹œê°„ ì§€ì—°",
                "severity": severity,
                "current": f"{recent_p95:.0f}ms",
                "threshold": "5000ms",
                "message": f"P95 ì‘ë‹µ ì‹œê°„ì´ {recent_p95:.0f}msë¡œ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤."
            })
            severity_scores.append(2 if recent_p95 >= 10000 else 1)

        # ì¡°ê±´ 5: ì—ëŸ¬ìœ¨ ê¸‰ì¦ (ì´ì „ ëŒ€ë¹„)
        if baseline_error_rate > 0:
            error_rate_change = ((recent_error_rate - baseline_error_rate) / baseline_error_rate * 100)
            if error_rate_change > spike_threshold:
                severity = "ğŸŸ  HIGH"
                alerts.append({
                    "condition": "ì—ëŸ¬ìœ¨ ê¸‰ì¦",
                    "severity": severity,
                    "current": f"{error_rate_change:+.1f}%",
                    "threshold": f"+{spike_threshold}%",
                    "message": f"ì—ëŸ¬ìœ¨ì´ ì´ì „ ëŒ€ë¹„ {error_rate_change:+.1f}% ê¸‰ì¦í–ˆìŠµë‹ˆë‹¤."
                })
                severity_scores.append(2)

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì•Œë¦¼ ì¡°ê±´ í‰ê°€ (ìµœê·¼ {time_window_minutes}ë¶„) ===",
            ""
        ]

        if service_name:
            summary_lines.append(f"ğŸ”§ ì„œë¹„ìŠ¤: {service_name}")
            summary_lines.append("")

        # ì¢…í•© íŒì •
        if not alerts:
            summary_lines.append("âœ… **ì •ìƒ**: ëª¨ë“  ì¡°ê±´ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
            summary_lines.append("")
            summary_lines.append("ğŸ“Š í˜„ì¬ ìƒíƒœ:")
            summary_lines.append(f"  - ì—ëŸ¬ìœ¨: {recent_error_rate:.2f}% (ì„ê³„ì¹˜: {error_rate_threshold}%)")
            summary_lines.append(f"  - ì´ ë¡œê·¸: {int(recent_total):,}ê±´")
            summary_lines.append(f"  - ì—ëŸ¬: {recent_errors}ê±´")
            if recent_p95:
                summary_lines.append(f"  - P95 ì‘ë‹µì‹œê°„: {recent_p95:.0f}ms")
            summary_lines.append("")
            summary_lines.append("ğŸ”” **ì•Œë¦¼ ë¶ˆí•„ìš”**: ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")

        else:
            # ìµœê³  ì‹¬ê°ë„ ê³„ì‚°
            max_severity = max(severity_scores) if severity_scores else 0
            if max_severity >= 3:
                overall = "ğŸ”´ CRITICAL - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”"
            elif max_severity >= 2:
                overall = "ğŸŸ  HIGH - ê¸´ê¸‰ í™•ì¸ í•„ìš”"
            else:
                overall = "ğŸŸ¡ MEDIUM - ëª¨ë‹ˆí„°ë§ ê°•í™”"

            summary_lines.append(f"ğŸš¨ **ì•Œë¦¼ í•„ìš”**: {len(alerts)}ê°œ ì¡°ê±´ ìœ„ë°˜")
            summary_lines.append(f"**ì¢…í•© ì‹¬ê°ë„**: {overall}")
            summary_lines.append("")

            # ìœ„ë°˜ ì¡°ê±´ ìƒì„¸
            summary_lines.append("## ğŸ”” ìœ„ë°˜ëœ ì¡°ê±´:")
            summary_lines.append("")
            for i, alert in enumerate(alerts, 1):
                summary_lines.append(f"**{i}. {alert['condition']}** | {alert['severity']}")
                summary_lines.append(f"   í˜„ì¬ê°’: {alert['current']} (ì„ê³„ì¹˜: {alert['threshold']})")
                summary_lines.append(f"   {alert['message']}")
                summary_lines.append("")

            # í˜„ì¬ ìƒíƒœ
            summary_lines.append("## ğŸ“Š í˜„ì¬ ìƒíƒœ:")
            summary_lines.append("")
            summary_lines.append(f"| ë©”íŠ¸ë¦­ | í˜„ì¬ ({time_window_minutes}ë¶„) | ê¸°ì¤€ ({time_window_minutes * 2}ë¶„ ì „) |")
            summary_lines.append("|--------|---------|----------|")
            summary_lines.append(f"| ì´ ë¡œê·¸ | {int(recent_total):,}ê±´ | {int(baseline_total):,}ê±´ |")
            summary_lines.append(f"| ì—ëŸ¬ | {recent_errors}ê±´ | {baseline_errors}ê±´ |")
            summary_lines.append(f"| ì—ëŸ¬ìœ¨ | {recent_error_rate:.2f}% | {baseline_error_rate:.2f}% |")
            summary_lines.append(f"| CRITICAL ì—ëŸ¬ | {recent_critical}ê±´ | - |")
            if recent_p95:
                summary_lines.append(f"| P95 ì‘ë‹µì‹œê°„ | {recent_p95:.0f}ms | - |")
            summary_lines.append("")

            # ê¶Œì¥ ì¡°ì¹˜
            summary_lines.append("## ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:")
            summary_lines.append("")
            if max_severity >= 3:
                summary_lines.append("1. **ì¦‰ì‹œ** ì˜¨ì½œ ì—”ì§€ë‹ˆì–´ì—ê²Œ ì•Œë¦¼")
                summary_lines.append("2. ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ í™•ì¸ (get_recent_errors)")
                summary_lines.append("3. ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜ í™•ì¸ (get_affected_users_count)")
                summary_lines.append("4. ì„œë¹„ìŠ¤ í—¬ìŠ¤ ìƒíƒœ ì ê²€ (get_service_health_status)")
            elif max_severity >= 2:
                summary_lines.append("1. ì—ëŸ¬ ì›ì¸ íŒŒì•… ì‹œì‘")
                summary_lines.append("2. ì—ëŸ¬ ë¹ˆë„ ë¶„ì„ (get_error_frequency_ranking)")
                summary_lines.append("3. ì—°ì‡„ ì¥ì•  ê°€ëŠ¥ì„± ì ê²€ (detect_cascading_failures)")
            else:
                summary_lines.append("1. ëª¨ë‹ˆí„°ë§ ê°•í™”")
                summary_lines.append("2. ì¶”ì„¸ ê´€ì°° (get_error_rate_trend)")
                summary_lines.append("3. í•„ìš”ì‹œ ìš©ëŸ‰ ì ê²€")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì•Œë¦¼ ì¡°ê±´ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def detect_resource_issues(
    project_uuid: str,
    time_hours: int = 24,
    service_name: Optional[str] = None
) -> str:
    """
    ë¦¬ì†ŒìŠ¤ ê´€ë ¨ ì´ìŠˆë¥¼ ê°ì§€í•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - "ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ê°€ ìˆë‚˜ìš”?"
    - "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ê³ ê°ˆ íŒ¨í„´ì´ ìˆë‚˜ìš”?"
    - "íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ê°€ ì¦ê°€í•˜ê³  ìˆë‚˜ìš”?"
    - "ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì¦ìƒì„ ë³´ì´ëŠ” ì„œë¹„ìŠ¤ëŠ”?"

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)
        service_name: ì„œë¹„ìŠ¤ ì´ë¦„ í•„í„° (ì„ íƒ)

    ì°¸ê³ : project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        ë¦¬ì†ŒìŠ¤ ì´ìŠˆ íƒ€ì…, ë°œìƒ ë¹ˆë„, ì˜í–¥ë°›ì€ ì„œë¹„ìŠ¤
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # ë¦¬ì†ŒìŠ¤ ì´ìŠˆ í‚¤ì›Œë“œ íŒ¨í„´
    resource_patterns = {
        "memory": {
            "keywords": ["OutOfMemory", "OutOfMemoryError", "memory exhausted", "heap space"],
            "severity": "ğŸ”´ CRITICAL",
            "category": "ë©”ëª¨ë¦¬ ë¶€ì¡±"
        },
        "database": {
            "keywords": ["connection pool", "pool exhausted", "too many connections", "database timeout", "connection refused"],
            "severity": "ğŸ”´ CRITICAL",
            "category": "DB ì—°ê²° ë¬¸ì œ"
        },
        "timeout": {
            "keywords": ["timeout", "timed out", "TimeoutException", "execution timeout"],
            "severity": "ğŸŸ  HIGH",
            "category": "íƒ€ì„ì•„ì›ƒ"
        },
        "thread": {
            "keywords": ["thread pool", "ThreadPoolExecutor", "RejectedExecutionException", "too many threads"],
            "severity": "ğŸŸ  HIGH",
            "category": "ìŠ¤ë ˆë“œ í’€ ë¬¸ì œ"
        },
        "disk": {
            "keywords": ["disk full", "no space left", "DiskFullException", "storage exhausted"],
            "severity": "ğŸ”´ CRITICAL",
            "category": "ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±"
        },
        "cpu": {
            "keywords": ["StackOverflowError", "CPU", "high CPU usage"],
            "severity": "ğŸŸ¡ MEDIUM",
            "category": "CPU ê³¼ë¶€í•˜"
        }
    }

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}},
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # ê° ë¦¬ì†ŒìŠ¤ ì´ìŠˆ íƒ€ì…ë³„ ê²€ìƒ‰
        resource_issues = []

        for issue_type, config in resource_patterns.items():
            # í‚¤ì›Œë“œ ê²€ìƒ‰
            should_clauses = [
                {"match": {"message": keyword}} for keyword in config["keywords"]
            ] + [
                {"match": {"log_details.exception_type": keyword}} for keyword in config["keywords"]
            ]

            issue_results = opensearch_client.search(
                index=index_pattern,
                body={
                    "size": 0,
                    "query": {
                        "bool": {
                            "must": must_clauses,
                            "should": should_clauses,
                            "minimum_should_match": 1
                        }
                    },
                    "aggs": {
                        "by_service": {
                            "terms": {
                                "field": "service_name.keyword",
                                "size": 10
                            }
                        },
                        "over_time": {
                            "date_histogram": {
                                "field": "timestamp",
                                "fixed_interval": "1h",
                                "time_zone": "Asia/Seoul"
                            }
                        }
                    }
                }
            )

            total_count = issue_results.get("hits", {}).get("total", {}).get("value", 0)

            if total_count > 0:
                # ì„œë¹„ìŠ¤ë³„ ë¶„í¬
                service_buckets = issue_results.get("aggregations", {}).get("by_service", {}).get("buckets", [])
                affected_services = [{"name": sb["key"], "count": sb["doc_count"]} for sb in service_buckets]

                # ì‹œê°„ëŒ€ë³„ ì¶”ì„¸
                time_buckets = issue_results.get("aggregations", {}).get("over_time", {}).get("buckets", [])
                non_zero_periods = [b for b in time_buckets if b.get("doc_count", 0) > 0]

                # ì¶”ì„¸ íŒë‹¨ (ìµœê·¼ 3ê°œ vs ì´ì „ 3ê°œ)
                if len(non_zero_periods) >= 6:
                    recent_avg = sum([b["doc_count"] for b in non_zero_periods[-3:]]) / 3
                    previous_avg = sum([b["doc_count"] for b in non_zero_periods[-6:-3]]) / 3
                    trend = "ì¦ê°€" if recent_avg > previous_avg * 1.2 else "ê°ì†Œ" if recent_avg < previous_avg * 0.8 else "ì•ˆì •"
                else:
                    trend = "íŒë‹¨ ë¶ˆê°€"

                resource_issues.append({
                    "type": issue_type,
                    "category": config["category"],
                    "severity": config["severity"],
                    "count": total_count,
                    "affected_services": affected_services,
                    "trend": trend,
                    "keywords": config["keywords"]
                })

        if not resource_issues:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¦¬ì†ŒìŠ¤ ê´€ë ¨ ì´ìŠˆê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # ì‹¬ê°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        severity_order = {"ğŸ”´ CRITICAL": 1, "ğŸŸ  HIGH": 2, "ğŸŸ¡ MEDIUM": 3}
        resource_issues.sort(key=lambda x: (severity_order.get(x["severity"], 99), -x["count"]))

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ë¦¬ì†ŒìŠ¤ ì´ìŠˆ ê°ì§€ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ê°ì§€ëœ ì´ìŠˆ: {len(resource_issues)}ê°œ ìœ í˜•",
            ""
        ]

        if service_name:
            summary_lines.append(f"ğŸ”§ ì„œë¹„ìŠ¤ í•„í„°: {service_name}")
            summary_lines.append("")

        # ì´ìŠˆë³„ ìƒì„¸
        total_resource_errors = sum([issue["count"] for issue in resource_issues])
        summary_lines.append(f"**ì´ ë¦¬ì†ŒìŠ¤ ê´€ë ¨ ì—ëŸ¬**: {total_resource_errors:,}ê±´")
        summary_lines.append("")

        for i, issue in enumerate(resource_issues, 1):
            summary_lines.append(f"## {i}. {issue['severity']} {issue['category']}")
            summary_lines.append(f"   ë°œìƒ íšŸìˆ˜: {issue['count']}ê±´")
            summary_lines.append(f"   ì¶”ì„¸: {issue['trend']}")
            summary_lines.append("")

            # ì˜í–¥ë°›ì€ ì„œë¹„ìŠ¤
            if issue["affected_services"]:
                summary_lines.append("   **ì˜í–¥ë°›ì€ ì„œë¹„ìŠ¤:**")
                for svc in issue["affected_services"][:5]:
                    summary_lines.append(f"   - {svc['name']}: {svc['count']}ê±´")
                summary_lines.append("")

            # ê´€ë ¨ í‚¤ì›Œë“œ
            keywords_str = ", ".join(issue["keywords"][:3])
            summary_lines.append(f"   ê´€ë ¨ í‚¤ì›Œë“œ: {keywords_str}")
            summary_lines.append("")

        # ì‹¬ê°ë„ ìš”ì•½
        critical_issues = [i for i in resource_issues if "CRITICAL" in i["severity"]]
        high_issues = [i for i in resource_issues if "HIGH" in i["severity"]]

        summary_lines.append("## ğŸ“Š ì‹¬ê°ë„ë³„ ìš”ì•½:")
        summary_lines.append("")
        if critical_issues:
            summary_lines.append(f"ğŸ”´ CRITICAL: {len(critical_issues)}ê°œ ì´ìŠˆ")
            for issue in critical_issues:
                summary_lines.append(f"  - {issue['category']}: {issue['count']}ê±´")
        if high_issues:
            summary_lines.append(f"ğŸŸ  HIGH: {len(high_issues)}ê°œ ì´ìŠˆ")
            for issue in high_issues:
                summary_lines.append(f"  - {issue['category']}: {issue['count']}ê±´")
        summary_lines.append("")

        # ê¶Œì¥ ì¡°ì¹˜
        summary_lines.append("## ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:")
        summary_lines.append("")

        if critical_issues:
            summary_lines.append("ğŸš¨ **ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”**:")
            for issue in critical_issues:
                if issue["type"] == "memory":
                    summary_lines.append("  - ë©”ëª¨ë¦¬ í™ ë¤í”„ ë¶„ì„")
                    summary_lines.append("  - JVM í™ í¬ê¸° ì¦ì„¤ ê²€í† ")
                    summary_lines.append("  - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜(Memory Leak) ì ê²€")
                elif issue["type"] == "database":
                    summary_lines.append("  - DB ì—°ê²° í’€ í¬ê¸° ì¦ì„¤")
                    summary_lines.append("  - ì¥ì‹œê°„ ì‹¤í–‰ ì¿¼ë¦¬ ì ê²€")
                    summary_lines.append("  - Connection Leak ê°ì§€ í™œì„±í™”")
                elif issue["type"] == "disk":
                    summary_lines.append("  - ë””ìŠ¤í¬ ê³µê°„ í™•ë³´")
                    summary_lines.append("  - ë¡œê·¸ ë¡œí…Œì´ì…˜ ì ê²€")
                    summary_lines.append("  - ì„ì‹œ íŒŒì¼ ì •ë¦¬")
        else:
            summary_lines.append("âš ï¸ **ëª¨ë‹ˆí„°ë§ ê°•í™”**:")
            summary_lines.append("  - ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì´ ê´€ì°°")
            summary_lines.append("  - ìš©ëŸ‰ ê³„íš(Capacity Planning) ê²€í† ")
            summary_lines.append("  - ì•Œë¦¼ ì„ê³„ì¹˜ ì„¤ì •")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ë¦¬ì†ŒìŠ¤ ì´ìŠˆ ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
