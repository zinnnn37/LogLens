"""
ë°°í¬ ë¶„ì„ ë„êµ¬ (Deployment Tools) - P2 Priority
- ë°°í¬ ì˜í–¥ ë¶„ì„
"""

from typing import Optional
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


@tool
async def analyze_deployment_impact(
    project_uuid: str,
    deployment_time: str,  # ISO format: "2025-11-11T14:30:00"
    before_hours: int = 1,
    after_hours: int = 1,
    service_name: Optional[str] = None
) -> str:
    """
    ë°°í¬ ì „í›„ ì˜í–¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - "ìƒˆ ë°°í¬ ì´í›„ ì—ëŸ¬ê°€ ì¦ê°€í–ˆë‚˜ìš”?"
    - "ë°°í¬ ì‹œì  ì „í›„ ë¹„êµí•´ì¤˜"
    - "ë¡¤ë°±ì´ í•„ìš”í•œê°€ìš”?"
    - "ë°°í¬ê°€ ì„±ê³µì ì´ì—ˆë‚˜ìš”?"

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        deployment_time: ë°°í¬ ì‹œê° (ISO í˜•ì‹, ì˜ˆ: "2025-11-11T14:30:00")
        before_hours: ë°°í¬ ì „ ë¶„ì„ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 1ì‹œê°„)
        after_hours: ë°°í¬ í›„ ë¶„ì„ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 1ì‹œê°„)
        service_name: ì„œë¹„ìŠ¤ ì´ë¦„ í•„í„° (ì„ íƒ)

    ì°¸ê³ : project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        ë°°í¬ ì „í›„ ì—ëŸ¬/ì„±ëŠ¥ ë¹„êµ, ë¡¤ë°± ê¶Œì¥ ì—¬ë¶€
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ë°°í¬ ì‹œê°„ íŒŒì‹±
    try:
        deploy_dt = datetime.fromisoformat(deployment_time.replace("Z", ""))
    except ValueError:
        return f"ì˜ëª»ëœ ë°°í¬ ì‹œê°„ í˜•ì‹ì…ë‹ˆë‹¤. ISO í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: 2025-11-11T14:30:00)"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    before_start = deploy_dt - timedelta(hours=before_hours)
    before_end = deploy_dt
    after_start = deploy_dt
    after_end = deploy_dt + timedelta(hours=after_hours)

    # Query êµ¬ì„±
    must_clauses = []
    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # OpenSearch Aggregation Query (ë°°í¬ ì „/í›„ ë¹„êµ)
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}} if must_clauses else {"match_all": {}},
                "aggs": {
                    "before_deployment": {
                        "filter": {
                            "range": {
                                "timestamp": {
                                    "gte": before_start.isoformat() + "Z",
                                    "lt": before_end.isoformat() + "Z"
                                }
                            }
                        },
                        "aggs": {
                            "total_logs": {"value_count": {"field": "log_id"}},
                            "error_logs": {"filter": {"term": {"level": "ERROR"}}},
                            "warn_logs": {"filter": {"term": {"level": "WARN"}}},
                            "avg_response_time": {
                                "avg": {"field": "log_details.execution_time"}
                            },
                            "p95_response_time": {
                                "percentiles": {
                                    "field": "log_details.execution_time",
                                    "percents": [95]
                                }
                            },
                            "error_types": {
                                "terms": {
                                    "field": "log_details.exception_type.keyword",
                                    "size": 5
                                }
                            }
                        }
                    },
                    "after_deployment": {
                        "filter": {
                            "range": {
                                "timestamp": {
                                    "gte": after_start.isoformat() + "Z",
                                    "lte": after_end.isoformat() + "Z"
                                }
                            }
                        },
                        "aggs": {
                            "total_logs": {"value_count": {"field": "log_id"}},
                            "error_logs": {"filter": {"term": {"level": "ERROR"}}},
                            "warn_logs": {"filter": {"term": {"level": "WARN"}}},
                            "avg_response_time": {
                                "avg": {"field": "log_details.execution_time"}
                            },
                            "p95_response_time": {
                                "percentiles": {
                                    "field": "log_details.execution_time",
                                    "percents": [95]
                                }
                            },
                            "error_types": {
                                "terms": {
                                    "field": "log_details.exception_type.keyword",
                                    "size": 5
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        aggs = results.get("aggregations", {})
        before = aggs.get("before_deployment", {})
        after = aggs.get("after_deployment", {})

        # ë°°í¬ ì „ ë°ì´í„°
        before_total = before.get("total_logs", {}).get("value", 0)
        before_errors = before.get("error_logs", {}).get("doc_count", 0)
        before_warns = before.get("warn_logs", {}).get("doc_count", 0)
        before_error_rate = (before_errors / before_total * 100) if before_total > 0 else 0
        before_avg_time = before.get("avg_response_time", {}).get("value")
        before_p95_time = before.get("p95_response_time", {}).get("values", {}).get("95.0")
        before_error_types = {b["key"]: b["doc_count"] for b in before.get("error_types", {}).get("buckets", [])}

        # ë°°í¬ í›„ ë°ì´í„°
        after_total = after.get("total_logs", {}).get("value", 0)
        after_errors = after.get("error_logs", {}).get("doc_count", 0)
        after_warns = after.get("warn_logs", {}).get("doc_count", 0)
        after_error_rate = (after_errors / after_total * 100) if after_total > 0 else 0
        after_avg_time = after.get("avg_response_time", {}).get("value")
        after_p95_time = after.get("p95_response_time", {}).get("values", {}).get("95.0")
        after_error_types = {b["key"]: b["doc_count"] for b in after.get("error_types", {}).get("buckets", [])}

        if before_total == 0 and after_total == 0:
            return f"ë°°í¬ ì „í›„ ê¸°ê°„ì— ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ë³€í™”ìœ¨ ê³„ì‚°
        if before_error_rate > 0:
            error_rate_change = ((after_error_rate - before_error_rate) / before_error_rate * 100)
        else:
            error_rate_change = 100 if after_error_rate > 0 else 0

        error_count_change = after_errors - before_errors

        if before_avg_time and after_avg_time:
            avg_time_change_pct = ((after_avg_time - before_avg_time) / before_avg_time * 100)
        else:
            avg_time_change_pct = 0

        # ë¡¤ë°± ê¶Œì¥ íŒì •
        rollback_reasons = []
        rollback_score = 0

        # ì¡°ê±´ 1: ì—ëŸ¬ìœ¨ ê¸‰ì¦ (50% ì´ìƒ)
        if error_rate_change > 50:
            rollback_reasons.append(f"ì—ëŸ¬ìœ¨ ê¸‰ì¦ ({error_rate_change:+.1f}%)")
            rollback_score += 3

        # ì¡°ê±´ 2: ìƒˆë¡œìš´ ì—ëŸ¬ íƒ€ì… ë“±ì¥
        new_errors = set(after_error_types.keys()) - set(before_error_types.keys())
        if new_errors:
            rollback_reasons.append(f"ìƒˆë¡œìš´ ì—ëŸ¬ íƒ€ì… ë“±ì¥ ({len(new_errors)}ê°œ)")
            rollback_score += 2

        # ì¡°ê±´ 3: ì‘ë‹µ ì‹œê°„ í¬ê²Œ ì¦ê°€ (30% ì´ìƒ)
        if avg_time_change_pct > 30:
            rollback_reasons.append(f"ì‘ë‹µ ì‹œê°„ ê¸‰ì¦ ({avg_time_change_pct:+.1f}%)")
            rollback_score += 2

        # ì¡°ê±´ 4: ì—ëŸ¬ ê±´ìˆ˜ ê¸‰ì¦ (20ê±´ ì´ìƒ ì¦ê°€)
        if error_count_change > 20:
            rollback_reasons.append(f"ì—ëŸ¬ ê±´ìˆ˜ ê¸‰ì¦ ({error_count_change:+}ê±´)")
            rollback_score += 1

        # ë¡¤ë°± ê¶Œì¥ ì—¬ë¶€
        if rollback_score >= 4:
            rollback_decision = "ğŸ”´ **ì¦‰ì‹œ ë¡¤ë°± ê¶Œì¥**"
        elif rollback_score >= 2:
            rollback_decision = "ğŸŸ¡ **ë¡¤ë°± ê²€í†  í•„ìš”**"
        else:
            rollback_decision = "ğŸŸ¢ **ë¡¤ë°± ë¶ˆí•„ìš” (ì •ìƒ ë°°í¬)**"

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ë°°í¬ ì˜í–¥ ë¶„ì„ ===",
            ""
        ]

        if service_name:
            summary_lines.append(f"ğŸ”§ ì„œë¹„ìŠ¤: {service_name}")
            summary_lines.append("")

        # ë°°í¬ ì •ë³´
        summary_lines.append("ğŸ“… ë°°í¬ ì •ë³´:")
        summary_lines.append(f"  - ë°°í¬ ì‹œê°: {deploy_dt.strftime('%Y-%m-%d %H:%M:%S')}")
        summary_lines.append(f"  - ë¶„ì„ ê¸°ê°„: ë°°í¬ ì „ {before_hours}ì‹œê°„ vs ë°°í¬ í›„ {after_hours}ì‹œê°„")
        summary_lines.append("")

        # ì¢…í•© íŒì •
        summary_lines.append(f"## {rollback_decision}")
        if rollback_reasons:
            summary_lines.append("")
            summary_lines.append("**ë¡¤ë°± ê³ ë ¤ ì‚¬ìœ :**")
            for reason in rollback_reasons:
                summary_lines.append(f"  - {reason}")
        summary_lines.append("")

        # ìƒì„¸ ë¹„êµí‘œ
        summary_lines.append("## ğŸ“Š ë°°í¬ ì „/í›„ ë¹„êµ")
        summary_lines.append("")
        summary_lines.append(f"| ë©”íŠ¸ë¦­ | ë°°í¬ ì „ ({before_hours}h) | ë°°í¬ í›„ ({after_hours}h) | ë³€í™” |")
        summary_lines.append("|--------|---------|---------|------|")
        summary_lines.append(f"| ì´ ë¡œê·¸ | {int(before_total):,}ê±´ | {int(after_total):,}ê±´ | {int(after_total - before_total):+,}ê±´ |")
        summary_lines.append(f"| ì—ëŸ¬ | {before_errors}ê±´ | {after_errors}ê±´ | {error_count_change:+}ê±´ |")
        summary_lines.append(f"| ê²½ê³  | {before_warns}ê±´ | {after_warns}ê±´ | {after_warns - before_warns:+}ê±´ |")
        summary_lines.append(f"| ì—ëŸ¬ìœ¨ | {before_error_rate:.2f}% | {after_error_rate:.2f}% | {after_error_rate - before_error_rate:+.2f}%p |")

        if before_avg_time and after_avg_time:
            summary_lines.append(f"| í‰ê·  ì‘ë‹µì‹œê°„ | {before_avg_time:.0f}ms | {after_avg_time:.0f}ms | {after_avg_time - before_avg_time:+.0f}ms |")
        if before_p95_time and after_p95_time:
            summary_lines.append(f"| P95 ì‘ë‹µì‹œê°„ | {before_p95_time:.0f}ms | {after_p95_time:.0f}ms | {after_p95_time - before_p95_time:+.0f}ms |")

        summary_lines.append("")

        # ì—ëŸ¬ íƒ€ì… ë¹„êµ
        if before_error_types or after_error_types:
            summary_lines.append("## âŒ ì—ëŸ¬ íƒ€ì… ë¹„êµ")
            summary_lines.append("")

            all_error_types = set(before_error_types.keys()) | set(after_error_types.keys())

            if new_errors:
                summary_lines.append(f"ğŸ†• **ìƒˆë¡œ ë“±ì¥í•œ ì—ëŸ¬** ({len(new_errors)}ê°œ):")
                for error_type in new_errors:
                    count = after_error_types.get(error_type, 0)
                    summary_lines.append(f"  - {error_type}: {count}ê±´")
                summary_lines.append("")

            disappeared_errors = set(before_error_types.keys()) - set(after_error_types.keys())
            if disappeared_errors:
                summary_lines.append(f"âœ… **ì‚¬ë¼ì§„ ì—ëŸ¬** ({len(disappeared_errors)}ê°œ):")
                for error_type in disappeared_errors:
                    summary_lines.append(f"  - {error_type}")
                summary_lines.append("")

            common_errors = set(before_error_types.keys()) & set(after_error_types.keys())
            if common_errors:
                summary_lines.append("ğŸ“Š **ê¸°ì¡´ ì—ëŸ¬ ë³€í™”:**")
                for error_type in common_errors:
                    before_count = before_error_types.get(error_type, 0)
                    after_count = after_error_types.get(error_type, 0)
                    change = after_count - before_count
                    change_emoji = "ğŸ”´" if change > 0 else "ğŸŸ¢" if change < 0 else "âšª"
                    summary_lines.append(f"  {change_emoji} {error_type}: {before_count}ê±´ â†’ {after_count}ê±´ ({change:+})")
                summary_lines.append("")

        # ê¶Œì¥ ì¡°ì¹˜
        summary_lines.append("## ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜")
        summary_lines.append("")

        if rollback_score >= 4:
            summary_lines.append("ğŸš¨ **ì¦‰ì‹œ ë¡¤ë°±**:")
            summary_lines.append("  1. ì´ì „ ë²„ì „ìœ¼ë¡œ ì¦‰ì‹œ ë¡¤ë°±")
            summary_lines.append("  2. ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜ í™•ì¸ (get_affected_users_count)")
            summary_lines.append("  3. ë¡¤ë°± í›„ ì—ëŸ¬ ê°ì†Œ í™•ì¸")
            summary_lines.append("  4. ë°°í¬ ì „ í…ŒìŠ¤íŠ¸ í”„ë¡œì„¸ìŠ¤ ê°œì„ ")
        elif rollback_score >= 2:
            summary_lines.append("âš ï¸ **ë¡¤ë°± ê²€í† **:")
            summary_lines.append("  1. ì—ëŸ¬ ë¡œê·¸ ìƒì„¸ ë¶„ì„ (get_recent_errors)")
            summary_lines.append("  2. ì‚¬ìš©ì ì˜í–¥ë„ í‰ê°€ (get_affected_users_count)")
            summary_lines.append("  3. í•«í”½ìŠ¤ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨")
            summary_lines.append("  4. í•„ìš”ì‹œ ë¡¤ë°± ê²°ì •")
        else:
            summary_lines.append("âœ… **ë°°í¬ ì„±ê³µ**:")
            summary_lines.append("  1. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ìœ ì§€")
            summary_lines.append("  2. ì¶”ê°€ {after_hours}ì‹œê°„ ê´€ì°° ê¶Œì¥")
            if disappeared_errors:
                summary_lines.append(f"  3. í•´ê²°ëœ ì—ëŸ¬ {len(disappeared_errors)}ê°œ í™•ì¸ë¨ - ë°°í¬ íš¨ê³¼ ìˆìŒ")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ë°°í¬ ì˜í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
