"""
ëª¨ë‹ˆí„°ë§ ë„êµ¬ (Monitoring Tools) - P0 Priority
- ì—ëŸ¬ìœ¨ ì¶”ì„¸, ì„œë¹„ìŠ¤ í—¬ìŠ¤, ì—ëŸ¬ ë¹ˆë„, API ì—ëŸ¬ìœ¨, ì‚¬ìš©ì ì˜í–¥ë„ ë¶„ì„
"""

from typing import Optional
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


@tool
async def get_error_rate_trend(
    project_uuid: str,
    interval: str = "1h",  # 1h, 30m, 15m
    time_hours: int = 24,
    service_name: Optional[str] = None
) -> str:
    """ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ ì¶”ì„¸ ë¶„ì„. "ì—ëŸ¬ìœ¨ ì¦ê°€?", "24ì‹œê°„ ì—ëŸ¬ ì¶”ì„¸", "ì—ëŸ¬ìœ¨ ê¸‰ì¦" âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„, ìë™ ì¶”ì„¸/ê²½ê³  íŒì •"""
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # OpenSearch Aggregation Query
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}},
                "aggs": {
                    "error_rate_over_time": {
                        "date_histogram": {
                            "field": "timestamp",
                            "fixed_interval": interval,
                            "time_zone": "Asia/Seoul",
                            "min_doc_count": 0
                        },
                        "aggs": {
                            "total_logs": {
                                "value_count": {"field": "log_id"}
                            },
                            "error_logs": {
                                "filter": {"term": {"level": "ERROR"}}
                            },
                            "error_rate": {
                                "bucket_script": {
                                    "buckets_path": {
                                        "errors": "error_logs>_count",
                                        "total": "total_logs"
                                    },
                                    "script": "params.total > 0 ? (params.errors / params.total * 100) : 0"
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        buckets = results.get("aggregations", {}).get("error_rate_over_time", {}).get("buckets", [])
        total_hits = results.get("hits", {}).get("total", {}).get("value", 0)

        if not buckets:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì—ëŸ¬ìœ¨ ë°ì´í„° ì¶”ì¶œ
        error_rates = []
        for bucket in buckets:
            time_str = bucket.get("key_as_string", "")
            total_count = bucket.get("total_logs", {}).get("value", 0)
            error_count = bucket.get("error_logs", {}).get("doc_count", 0)
            error_rate = bucket.get("error_rate", {}).get("value", 0)

            if total_count > 0:  # ë°ì´í„°ê°€ ìˆëŠ” ì‹œê°„ëŒ€ë§Œ
                error_rates.append({
                    "time": time_str,
                    "total": int(total_count),
                    "errors": error_count,
                    "rate": error_rate
                })

        if not error_rates:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì¶”ì„¸ ë¶„ì„: ìµœê·¼ 3ê°œ vs ì´ì „ 3ê°œ ë¹„êµ
        recent_avg = sum([r["rate"] for r in error_rates[-3:]]) / min(3, len(error_rates[-3:]))
        if len(error_rates) >= 6:
            previous_avg = sum([r["rate"] for r in error_rates[-6:-3]]) / 3
            change_rate = ((recent_avg - previous_avg) / previous_avg * 100) if previous_avg > 0 else 0
        else:
            previous_avg = recent_avg
            change_rate = 0

        # ì¶”ì„¸ ë°©í–¥ íŒì •
        if abs(change_rate) < 5:
            trend = "ì•ˆì •"
            trend_emoji = "ğŸŸ¢"
        elif change_rate > 0:
            trend = "ì¦ê°€"
            trend_emoji = "ğŸ”´" if change_rate > 20 else "ğŸŸ¡"
        else:
            trend = "ê°ì†Œ"
            trend_emoji = "ğŸŸ¢"

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì—ëŸ¬ìœ¨ ì¶”ì„¸ ë¶„ì„ (ìµœê·¼ {time_hours}ì‹œê°„, ê°„ê²©: {interval}) ===",
            ""
        ]

        if service_name:
            summary_lines.append(f"ğŸ”§ ì„œë¹„ìŠ¤: {service_name}")
            summary_lines.append("")

        # í˜„ì¬ ìƒíƒœ
        current = error_rates[-1]
        summary_lines.append(f"**í˜„ì¬ ì—ëŸ¬ìœ¨**: {current['rate']:.2f}% (ìµœê·¼ {interval})")
        summary_lines.append(f"**ì¶”ì„¸**: {trend_emoji} {trend} ì¤‘ ({change_rate:+.1f}% vs {len(error_rates[-6:-3])}ê°œ ì´ì „ êµ¬ê°„)")
        summary_lines.append("")

        # í†µê³„
        total_requests = sum([r["total"] for r in error_rates])
        total_errors = sum([r["errors"] for r in error_rates])
        avg_error_rate = (total_errors / total_requests * 100) if total_requests > 0 else 0

        summary_lines.append(f"ğŸ“Š ì „ì²´ í†µê³„:")
        summary_lines.append(f"  - ì´ ìš”ì²­: {total_requests:,}ê±´")
        summary_lines.append(f"  - ì´ ì—ëŸ¬: {total_errors:,}ê±´")
        summary_lines.append(f"  - í‰ê·  ì—ëŸ¬ìœ¨: {avg_error_rate:.2f}%")
        summary_lines.append("")

        # ì‹œê°„ëŒ€ë³„ ìƒì„¸ (ìµœê·¼ 10ê°œë§Œ)
        summary_lines.append("â±ï¸  ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ (ìµœê·¼ 10ê°œ):")
        for er in error_rates[-10:]:
            time_display = er["time"][:16].replace("T", " ")
            rate_emoji = "âŒ" if er["rate"] > 5 else "âš ï¸" if er["rate"] > 2 else "âœ…"
            summary_lines.append(
                f"  {rate_emoji} {time_display} | {er['rate']:.2f}% ({er['errors']}/{er['total']})"
            )

        # ê²½ê³  ë©”ì‹œì§€
        if trend == "ì¦ê°€" and recent_avg > 3:
            summary_lines.append("")
            summary_lines.append(f"âš ï¸ **ê²½ê³ **: ì—ëŸ¬ìœ¨ì´ ì¦ê°€ ì¶”ì„¸ì´ë©° í˜„ì¬ {recent_avg:.1f}%ë¡œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif recent_avg > 5:
            summary_lines.append("")
            summary_lines.append(f"ğŸš¨ **ìœ„í—˜**: ì—ëŸ¬ìœ¨ì´ {recent_avg:.1f}%ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì—ëŸ¬ìœ¨ ì¶”ì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_service_health_status(
    project_uuid: str,
    time_hours: int = 24,
    service_name: Optional[str] = None
) -> str:
    """ì„œë¹„ìŠ¤ë³„ í—¬ìŠ¤ ìƒíƒœ ë¶„ì„. "ë¶ˆì•ˆì •í•œ ì„œë¹„ìŠ¤?", "ì„œë¹„ìŠ¤ ì—ëŸ¬ìœ¨ ìˆœìœ„", "í—¬ìŠ¤ ì²´í¬" âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„, ìë™ í—¬ìŠ¤ ë“±ê¸‰ íŒì •"""
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # OpenSearch Aggregation Query
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}},
                "aggs": {
                    "by_service": {
                        "terms": {
                            "field": "service_name",
                            "size": 50
                        },
                        "aggs": {
                            "total_logs": {
                                "value_count": {"field": "log_id"}
                            },
                            "error_logs": {
                                "filter": {"term": {"level": "ERROR"}}
                            },
                            "warn_logs": {
                                "filter": {"term": {"level": "WARN"}}
                            },
                            "error_rate": {
                                "bucket_script": {
                                    "buckets_path": {
                                        "errors": "error_logs>_count",
                                        "total": "total_logs"
                                    },
                                    "script": "params.total > 0 ? (params.errors / params.total * 100) : 0"
                                }
                            },
                            "success_rate": {
                                "bucket_script": {
                                    "buckets_path": {
                                        "errors": "error_logs>_count",
                                        "total": "total_logs"
                                    },
                                    "script": "params.total > 0 ? ((params.total - params.errors) / params.total * 100) : 100"
                                }
                            },
                            "avg_response_time": {
                                "avg": {"field": "log_details.execution_time"}
                            },
                            "p95_response_time": {
                                "percentiles": {
                                    "field": "log_details.execution_time",
                                    "percents": [95]
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        buckets = results.get("aggregations", {}).get("by_service", {}).get("buckets", [])

        if not buckets:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì„œë¹„ìŠ¤ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì„œë¹„ìŠ¤ë³„ í—¬ìŠ¤ ë°ì´í„° ì²˜ë¦¬
        services = []
        for bucket in buckets:
            service = bucket.get("key", "unknown")
            total = bucket.get("total_logs", {}).get("value", 0)
            errors = bucket.get("error_logs", {}).get("doc_count", 0)
            warns = bucket.get("warn_logs", {}).get("doc_count", 0)
            error_rate = bucket.get("error_rate", {}).get("value", 0)
            success_rate = bucket.get("success_rate", {}).get("value", 100)
            avg_time = bucket.get("avg_response_time", {}).get("value")
            p95_time = bucket.get("p95_response_time", {}).get("values", {}).get("95.0")

            # í—¬ìŠ¤ ì ìˆ˜ ê³„ì‚° (0-100)
            # ì„±ê³µë¥  70% + ì—ëŸ¬ìœ¨ í˜ë„í‹° 20% + ì‘ë‹µì‹œê°„ 10%
            health_score = success_rate * 0.7
            if error_rate > 10:
                health_score -= 20
            elif error_rate > 5:
                health_score -= 10
            elif error_rate > 2:
                health_score -= 5

            if avg_time and avg_time > 5000:  # 5ì´ˆ ì´ìƒ
                health_score -= 10
            elif avg_time and avg_time > 2000:  # 2ì´ˆ ì´ìƒ
                health_score -= 5

            # í—¬ìŠ¤ ë“±ê¸‰ íŒì •
            if health_score >= 90 and error_rate < 1:
                grade = "ğŸŸ¢ Healthy"
            elif health_score >= 75 and error_rate < 3:
                grade = "ğŸŸ¡ Degraded"
            elif health_score >= 60 and error_rate < 10:
                grade = "ğŸŸ  Warning"
            else:
                grade = "ğŸ”´ Down"

            services.append({
                "name": service,
                "total": int(total),
                "errors": errors,
                "warns": warns,
                "error_rate": error_rate,
                "success_rate": success_rate,
                "health_score": health_score,
                "grade": grade,
                "avg_time": avg_time,
                "p95_time": p95_time
            })

        # ì—ëŸ¬ìœ¨ ìˆœìœ¼ë¡œ ì •ë ¬
        services.sort(key=lambda s: s["error_rate"], reverse=True)

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì„œë¹„ìŠ¤ í—¬ìŠ¤ ìƒíƒœ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            ""
        ]

        if service_name:
            # íŠ¹ì • ì„œë¹„ìŠ¤ ìƒì„¸ ì •ë³´
            svc = services[0]
            summary_lines.append(f"## ğŸ¥ {svc['name']} í—¬ìŠ¤ ìƒíƒœ")
            summary_lines.append("")
            summary_lines.append(f"**ì¢…í•© ë“±ê¸‰**: {svc['grade']}")
            summary_lines.append(f"**í—¬ìŠ¤ ì ìˆ˜**: {svc['health_score']:.1f}/100")
            summary_lines.append("")
            summary_lines.append("**ë©”íŠ¸ë¦­**:")
            summary_lines.append(f"  - ì—ëŸ¬ìœ¨: {svc['error_rate']:.2f}%")
            summary_lines.append(f"  - ì„±ê³µë¥ : {svc['success_rate']:.2f}%")
            summary_lines.append(f"  - ì´ ìš”ì²­: {svc['total']:,}ê±´")
            summary_lines.append(f"  - ì—ëŸ¬: {svc['errors']}ê±´")
            summary_lines.append(f"  - ê²½ê³ : {svc['warns']}ê±´")

            if svc['avg_time']:
                summary_lines.append(f"  - í‰ê·  ì‘ë‹µì‹œê°„: {svc['avg_time']:.0f}ms")
            if svc['p95_time']:
                summary_lines.append(f"  - P95 ì‘ë‹µì‹œê°„: {svc['p95_time']:.0f}ms")

            # ê¶Œì¥ ì¡°ì¹˜
            if "Down" in svc['grade']:
                summary_lines.append("")
                summary_lines.append(f"ğŸš¨ **ê¸´ê¸‰**: {svc['name']}ì´(ê°€) ì‹¬ê°í•œ ìƒíƒœì…ë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            elif "Warning" in svc['grade']:
                summary_lines.append("")
                summary_lines.append(f"âš ï¸ **ì£¼ì˜**: {svc['name']}ì˜ ì—ëŸ¬ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ì„¸ìš”.")

        else:
            # ì „ì²´ ì„œë¹„ìŠ¤ ìš”ì•½
            summary_lines.append(f"ì´ {len(services)}ê°œ ì„œë¹„ìŠ¤ ë¶„ì„")
            summary_lines.append("")

            # ë“±ê¸‰ë³„ ë¶„í¬
            grade_counts = {}
            for svc in services:
                grade_key = svc['grade'].split()[1]  # "ğŸŸ¢ Healthy" -> "Healthy"
                grade_counts[grade_key] = grade_counts.get(grade_key, 0) + 1

            summary_lines.append("ë“±ê¸‰ë³„ ë¶„í¬:")
            for grade in ["Healthy", "Degraded", "Warning", "Down"]:
                count = grade_counts.get(grade, 0)
                if count > 0:
                    summary_lines.append(f"  - {grade}: {count}ê°œ ì„œë¹„ìŠ¤")
            summary_lines.append("")

            # ì„œë¹„ìŠ¤ ìƒì„¸ ë¦¬ìŠ¤íŠ¸
            summary_lines.append("ì„œë¹„ìŠ¤ë³„ ìƒì„¸:")
            for i, svc in enumerate(services, 1):
                summary_lines.append(f"{i}. **{svc['name']}** | {svc['grade']}")
                summary_lines.append(f"   ì—ëŸ¬ìœ¨: {svc['error_rate']:.2f}% | ì„±ê³µë¥ : {svc['success_rate']:.1f}% | ìš”ì²­: {svc['total']:,}ê±´")
                if svc['avg_time']:
                    summary_lines.append(f"   í‰ê·  ì‘ë‹µ: {svc['avg_time']:.0f}ms")
                summary_lines.append("")

            # ê²½ê³  ìš”ì•½
            critical_services = [s for s in services if "Down" in s['grade'] or "Warning" in s['grade']]
            if critical_services:
                summary_lines.append("âš ï¸ **ì£¼ì˜ê°€ í•„ìš”í•œ ì„œë¹„ìŠ¤**:")
                for svc in critical_services:
                    summary_lines.append(f"  - {svc['name']}: {svc['grade']} (ì—ëŸ¬ìœ¨ {svc['error_rate']:.1f}%)")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì„œë¹„ìŠ¤ í—¬ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_error_frequency_ranking(
    project_uuid: str,
    time_hours: int = 24,  # ê¸°ë³¸ 24ì‹œê°„ (ë‹¤ë¥¸ ë„êµ¬ë“¤ê³¼ í†µì¼)
    limit: int = 10,
    service_name: Optional[str] = None
) -> str:
    """ì—ëŸ¬ íƒ€ì…ë³„ ë°œìƒ ë¹ˆë„ ìˆœìœ„. "ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬?", "NullPointerException ëª‡ ë²ˆ?" âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„"""
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"term": {"level": "ERROR"}},
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # OpenSearch Aggregation Query
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}},
                "aggs": {
                    "by_exception_type": {
                        "terms": {
                            "field": "log_details.exception_type",
                            "size": limit,
                            "order": {"_count": "desc"}
                        },
                        "aggs": {
                            "first_occurrence": {
                                "min": {"field": "timestamp"}
                            },
                            "last_occurrence": {
                                "max": {"field": "timestamp"}
                            },
                            "affected_services": {
                                "terms": {
                                    "field": "service_name",
                                    "size": 10
                                }
                            },
                            "over_time": {
                                "date_histogram": {
                                    "field": "timestamp",
                                    "fixed_interval": "6h",
                                    "min_doc_count": 0
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        buckets = results.get("aggregations", {}).get("by_exception_type", {}).get("buckets", [])
        total_errors = results.get("hits", {}).get("total", {}).get("value", 0)

        if not buckets:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì—ëŸ¬ ë¹ˆë„ ìˆœìœ„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì´ {total_errors}ê±´ì˜ ì—ëŸ¬ ë°œìƒ, ìƒìœ„ {len(buckets)}ê°œ í‘œì‹œ",
            ""
        ]

        if service_name:
            summary_lines.append(f"ğŸ”§ ì„œë¹„ìŠ¤ í•„í„°: {service_name}")
            summary_lines.append("")

        # ì—ëŸ¬ íƒ€ì…ë³„ ìƒì„¸
        for i, bucket in enumerate(buckets, 1):
            error_type = bucket.get("key", "Unknown")
            count = bucket.get("doc_count", 0)
            first_time = bucket.get("first_occurrence", {}).get("value_as_string", "N/A")
            last_time = bucket.get("last_occurrence", {}).get("value_as_string", "N/A")

            # ì‹œê°„ ê³„ì‚°
            if first_time != "N/A" and last_time != "N/A":
                first_dt = datetime.fromisoformat(first_time.replace("Z", "+00:00"))
                last_dt = datetime.fromisoformat(last_time.replace("Z", "+00:00"))
                duration = last_dt - first_dt
                duration_hours = duration.total_seconds() / 3600

                first_display = first_time[:19].replace("T", " ")
                last_display = last_time[:19].replace("T", " ")
            else:
                duration_hours = 0
                first_display = "N/A"
                last_display = "N/A"

            # ì˜í–¥ë°›ì€ ì„œë¹„ìŠ¤
            service_buckets = bucket.get("affected_services", {}).get("buckets", [])
            affected_services = [sb["key"] for sb in service_buckets]

            # ë°˜ë³µ íŒ¨í„´ ê°ì§€
            time_buckets = bucket.get("over_time", {}).get("buckets", [])
            non_zero_periods = [b for b in time_buckets if b.get("doc_count", 0) > 0]
            is_recurring = len(non_zero_periods) >= 3  # 3ê°œ ì´ìƒ ì‹œê°„ëŒ€ì— ë°œìƒí•˜ë©´ ë°˜ë³µìœ¼ë¡œ ê°„ì£¼

            # ì¶œë ¥
            summary_lines.append(f"{i}. **{error_type}**")
            summary_lines.append(f"   ğŸ“Š ë°œìƒ íšŸìˆ˜: {count}ê±´")
            summary_lines.append(f"   â° ìµœì´ˆ ë°œìƒ: {first_display}")
            summary_lines.append(f"   â° ìµœê·¼ ë°œìƒ: {last_display}")

            if duration_hours > 1:
                summary_lines.append(f"   â±ï¸  ë°œìƒ ê¸°ê°„: {duration_hours:.1f}ì‹œê°„")

            if is_recurring:
                summary_lines.append(f"   ğŸ” íŒ¨í„´: ë°˜ë³µ ë°œìƒ ({len(non_zero_periods)}ê°œ ì‹œê°„ëŒ€)")
            else:
                summary_lines.append(f"   ğŸ“ íŒ¨í„´: ì¼ì‹œì  ë°œìƒ")

            if affected_services:
                services_str = ", ".join(affected_services[:3])
                if len(affected_services) > 3:
                    services_str += f" ì™¸ {len(affected_services) - 3}ê°œ"
                summary_lines.append(f"   ğŸ”§ ì˜í–¥ ì„œë¹„ìŠ¤: {services_str}")

            summary_lines.append("")

        # ìš”ì•½ í†µê³„
        total_unique_errors = len(buckets)
        recurring_errors = sum(1 for b in buckets if len([tb for tb in b.get("over_time", {}).get("buckets", []) if tb.get("doc_count", 0) > 0]) >= 3)

        summary_lines.append("ğŸ“ˆ ìš”ì•½:")
        summary_lines.append(f"  - ê³ ìœ  ì—ëŸ¬ íƒ€ì…: {total_unique_errors}ê°œ")
        summary_lines.append(f"  - ë°˜ë³µ ë°œìƒ ì—ëŸ¬: {recurring_errors}ê°œ")
        summary_lines.append(f"  - ì¼ì‹œì  ì—ëŸ¬: {total_unique_errors - recurring_errors}ê°œ")

        if recurring_errors > 0:
            summary_lines.append("")
            summary_lines.append(f"âš ï¸ **ì£¼ì˜**: {recurring_errors}ê°œì˜ ì—ëŸ¬ê°€ ë°˜ë³µì ìœ¼ë¡œ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¼ë³¸ ì›ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì—ëŸ¬ ë¹ˆë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_api_error_rates(
    project_uuid: str,
    time_hours: int = 24,
    limit: int = 10,
    min_requests: int = 10  # ìµœì†Œ ìš”ì²­ ìˆ˜ (ë„ˆë¬´ ì ì€ API ì œì™¸)
) -> str:
    """
    API ì—”ë“œí¬ì¸íŠ¸ë³„ ì—ëŸ¬ìœ¨ ë° ì‹¤íŒ¨ìœ¨ ë¹„êµ ë¶„ì„.
    ì‚¬ìš©: "ì—ëŸ¬ ë§ì€ APIëŠ”?", "APIë³„ ì—ëŸ¬ìœ¨ ë¹„êµ", "ì—”ë“œí¬ì¸íŠ¸ ì‹¤íŒ¨ìœ¨"
    Args: project_uuid (í•„ìˆ˜), time_hours (ê¸°ë³¸ 24), limit (ê¸°ë³¸ 10)
    âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"exists": {"field": "log_details.request_uri"}},
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    try:
        # OpenSearch Aggregation Query
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}},
                "aggs": {
                    "by_endpoint": {
                        "terms": {
                            "field": "log_details.request_uri",
                            "size": limit * 2,  # í•„í„°ë§ í›„ limitê°œ ë‚¨ê¸°ê¸° ìœ„í•´ ë” ê°€ì ¸ì˜´
                            "min_doc_count": min_requests
                        },
                        "aggs": {
                            "total_requests": {
                                "value_count": {"field": "log_id"}
                            },
                            "error_requests": {
                                "filter": {"term": {"level": "ERROR"}}
                            },
                            "error_rate": {
                                "bucket_script": {
                                    "buckets_path": {
                                        "errors": "error_requests>_count",
                                        "total": "total_requests"
                                    },
                                    "script": "params.total > 0 ? (params.errors / params.total * 100) : 0"
                                }
                            },
                            "by_status_code": {
                                "terms": {
                                    "field": "log_details.response_status",
                                    "size": 10
                                }
                            },
                            "by_http_method": {
                                "terms": {
                                    "field": "log_details.http_method",
                                    "size": 5
                                }
                            },
                            "by_service": {
                                "terms": {
                                    "field": "service_name",
                                    "size": 5
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        buckets = results.get("aggregations", {}).get("by_endpoint", {}).get("buckets", [])

        if not buckets:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ API ìš”ì²­ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì—ëŸ¬ìœ¨ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ limitê°œë§Œ
        buckets_sorted = sorted(buckets, key=lambda b: b.get("error_rate", {}).get("value", 0), reverse=True)[:limit]

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== API ì—”ë“œí¬ì¸íŠ¸ ì—ëŸ¬ìœ¨ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì—ëŸ¬ìœ¨ì´ ë†’ì€ ìƒìœ„ {len(buckets_sorted)}ê°œ API",
            ""
        ]

        # APIë³„ ìƒì„¸
        for i, bucket in enumerate(buckets_sorted, 1):
            endpoint = bucket.get("key", "N/A")
            total = bucket.get("total_requests", {}).get("value", 0)
            errors = bucket.get("error_requests", {}).get("doc_count", 0)
            error_rate = bucket.get("error_rate", {}).get("value", 0)
            success_rate = 100 - error_rate

            # HTTP ìƒíƒœ ì½”ë“œ ë¶„í¬
            status_buckets = bucket.get("by_status_code", {}).get("buckets", [])
            status_5xx = sum([sb["doc_count"] for sb in status_buckets if 500 <= sb.get("key", 0) < 600])
            status_4xx = sum([sb["doc_count"] for sb in status_buckets if 400 <= sb.get("key", 0) < 500])

            # HTTP ë©”ì„œë“œ
            method_buckets = bucket.get("by_http_method", {}).get("buckets", [])
            methods = [mb["key"] for mb in method_buckets]
            method_str = "/".join(methods) if methods else "N/A"

            # ì„œë¹„ìŠ¤
            service_buckets = bucket.get("by_service", {}).get("buckets", [])
            services = [sb["key"] for sb in service_buckets]
            service_str = ", ".join(services[:2])

            # ë“±ê¸‰ íŒì •
            if error_rate > 10:
                grade_emoji = "ğŸ”´"
                grade = "ë§¤ìš° ë¶ˆì•ˆì •"
            elif error_rate > 5:
                grade_emoji = "ğŸŸ "
                grade = "ë¶ˆì•ˆì •"
            elif error_rate > 2:
                grade_emoji = "ğŸŸ¡"
                grade = "ì£¼ì˜"
            else:
                grade_emoji = "ğŸŸ¢"
                grade = "ì •ìƒ"

            # ì¶œë ¥
            summary_lines.append(f"{i}. {grade_emoji} **{endpoint}**")
            summary_lines.append(f"   ë©”ì„œë“œ: {method_str} | ì„œë¹„ìŠ¤: {service_str}")
            summary_lines.append(f"   ğŸ“Š ì—ëŸ¬ìœ¨: {error_rate:.2f}% | ì„±ê³µë¥ : {success_rate:.2f}%")
            summary_lines.append(f"   ğŸ“ˆ ì´ ìš”ì²­: {int(total):,}ê±´ | ì—ëŸ¬: {errors}ê±´")

            if status_5xx > 0 or status_4xx > 0:
                summary_lines.append(f"   ğŸŒ HTTP: 5xx({status_5xx}ê±´) / 4xx({status_4xx}ê±´)")

            summary_lines.append(f"   ë“±ê¸‰: {grade}")
            summary_lines.append("")

        # ìš”ì•½ í†µê³„
        critical_apis = sum(1 for b in buckets_sorted if b.get("error_rate", {}).get("value", 0) > 10)
        warning_apis = sum(1 for b in buckets_sorted if 5 < b.get("error_rate", {}).get("value", 0) <= 10)

        summary_lines.append("ğŸ“Š ìš”ì•½:")
        summary_lines.append(f"  - ë§¤ìš° ë¶ˆì•ˆì • (>10%): {critical_apis}ê°œ API")
        summary_lines.append(f"  - ë¶ˆì•ˆì • (5-10%): {warning_apis}ê°œ API")
        summary_lines.append(f"  - ì£¼ì˜/ì •ìƒ (<5%): {len(buckets_sorted) - critical_apis - warning_apis}ê°œ API")

        if critical_apis > 0:
            summary_lines.append("")
            summary_lines.append(f"ğŸš¨ **ê¸´ê¸‰**: {critical_apis}ê°œ APIì˜ ì—ëŸ¬ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"API ì—ëŸ¬ìœ¨ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_affected_users_count(
    project_uuid: str,
    time_hours: int = 24,
    error_only: bool = True
) -> str:
    """
    ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ (requester_ip ê¸°ë°˜).

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… ê³ ìœ  ì‚¬ìš©ì ìˆ˜ ê³„ì‚° (IP ê¸°ë°˜)
    - âœ… ì „ì²´ ëŒ€ë¹„ ì˜í–¥ ë¹„ìœ¨ ì œê³µ
    - âœ… ì‚¬ìš©ìë³„ ì—ëŸ¬ ë¹ˆë„ ë¶„ì„
    - âœ… ë°˜ë³µ ì—ëŸ¬ ì‚¬ìš©ì ê°ì§€
    - âŒ íŠ¹ì • ì—ëŸ¬ íƒ€ì…ë³„ ë¶„ì„ ì•ˆ í•¨ (get_error_frequency_ranking ì‚¬ìš©)
    - âŒ ì‹œê°„ëŒ€ë³„ ì¶”ì„¸ëŠ” ì œê³µ ì•ˆ í•¨ (get_error_rate_trend ì‚¬ìš©)

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ëª‡ ëª…ì´ ì—ëŸ¬ë¥¼ ê²ªì—ˆì–´?"
    2. "ì‚¬ìš©ì ì˜í–¥ë„ëŠ”?"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - requester_ip í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        error_only: ì—ëŸ¬ë§Œ ë¶„ì„ (ê¸°ë³¸ True)

    ê´€ë ¨ ë„êµ¬:
    - get_error_frequency_ranking: ì—ëŸ¬ íƒ€ì…ë³„ ë¹ˆë„
    - get_service_health_status: ì„œë¹„ìŠ¤ë³„ í—¬ìŠ¤

    Returns:
        ê³ ìœ  ì‚¬ìš©ì ìˆ˜, ì˜í–¥ ë¹„ìœ¨, ì‚¬ìš©ìë³„ ë¹ˆë„
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"exists": {"field": "requester_ip"}},
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    try:
        # OpenSearch Aggregation Query
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,
                "query": {"bool": {"must": must_clauses}},
                "aggs": {
                    "total_unique_users": {
                        "cardinality": {
                            "script": {
                                "source": "doc['requester_ip'].value.toString()",
                                "lang": "painless"
                            }
                        }
                    },
                    "users_with_errors": {
                        "filter": {"term": {"level": "ERROR"}},
                        "aggs": {
                            "unique_error_users": {
                                "cardinality": {
                                    "script": {
                                        "source": "doc['requester_ip'].value.toString()",
                                        "lang": "painless"
                                    }
                                }
                            },
                            "top_affected_users": {
                                "terms": {
                                    "script": {
                                        "source": "doc['requester_ip'].value.toString()",
                                        "lang": "painless"
                                    },
                                    "size": 10,
                                    "order": {"_count": "desc"}
                                },
                                "aggs": {
                                    "error_types": {
                                        "terms": {
                                            "field": "log_details.exception_type",
                                            "size": 5
                                        }
                                    }
                                }
                            }
                        }
                    },
                    "total_error_count": {
                        "filter": {"term": {"level": "ERROR"}}
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        aggs = results.get("aggregations", {})
        total_unique_users = aggs.get("total_unique_users", {}).get("value", 0)
        users_with_errors_agg = aggs.get("users_with_errors", {})
        unique_error_users = users_with_errors_agg.get("unique_error_users", {}).get("value", 0)
        total_error_count = aggs.get("total_error_count", {}).get("doc_count", 0)

        if total_unique_users == 0:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì‚¬ìš©ì í™œë™ì´ ì—†ìŠµë‹ˆë‹¤ (requester_ip í•„ë“œ ì—†ìŒ)."

        # ì˜í–¥ ë¹„ìœ¨ ê³„ì‚°
        affected_percentage = (unique_error_users / total_unique_users * 100) if total_unique_users > 0 else 0

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì˜í–¥ë°›ì€ ì‚¬ìš©ì ë¶„ì„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            ""
        ]

        # ì „ì²´ í†µê³„
        summary_lines.append("ğŸ“Š ì „ì²´ ì‚¬ìš©ì:")
        summary_lines.append(f"  - ì´ ê³ ìœ  ì‚¬ìš©ì (IP): {total_unique_users:,}ëª…")
        summary_lines.append(f"  - ì—ëŸ¬ë¥¼ ê²½í—˜í•œ ì‚¬ìš©ì: {unique_error_users:,}ëª…")
        summary_lines.append(f"  - ì˜í–¥ ë¹„ìœ¨: {affected_percentage:.2f}%")
        summary_lines.append("")

        # ì—ëŸ¬ í†µê³„
        summary_lines.append("âŒ ì—ëŸ¬ í†µê³„:")
        summary_lines.append(f"  - ì´ ì—ëŸ¬ ë°œìƒ: {total_error_count:,}ê±´")
        if unique_error_users > 0:
            avg_errors_per_user = total_error_count / unique_error_users
            summary_lines.append(f"  - ì‚¬ìš©ìë‹¹ í‰ê·  ì—ëŸ¬: {avg_errors_per_user:.1f}ê±´")
        summary_lines.append("")

        # ê°€ì¥ ë§ì´ ì˜í–¥ë°›ì€ ì‚¬ìš©ì
        top_users_buckets = users_with_errors_agg.get("top_affected_users", {}).get("buckets", [])
        if top_users_buckets:
            summary_lines.append("ğŸ‘¥ ê°€ì¥ ë§ì´ ì˜í–¥ë°›ì€ ì‚¬ìš©ì (IP):")
            for i, bucket in enumerate(top_users_buckets, 1):
                user_ip = bucket.get("key", "N/A")
                error_count = bucket.get("doc_count", 0)

                # ì—ëŸ¬ íƒ€ì…
                error_type_buckets = bucket.get("error_types", {}).get("buckets", [])
                error_types = [etb["key"] for etb in error_type_buckets]
                error_types_str = ", ".join(error_types[:3])
                if len(error_types) > 3:
                    error_types_str += f" ì™¸ {len(error_types) - 3}ê°œ"

                summary_lines.append(f"  {i}. {user_ip}")
                summary_lines.append(f"     ì—ëŸ¬ ë°œìƒ: {error_count}ê±´")
                if error_types_str:
                    summary_lines.append(f"     ì—ëŸ¬ íƒ€ì…: {error_types_str}")

            summary_lines.append("")

        # ì‹¬ê°ë„ í‰ê°€
        if affected_percentage > 50:
            severity = "ğŸ”´ ë§¤ìš° ì‹¬ê°"
            message = f"ì „ì²´ ì‚¬ìš©ìì˜ ì ˆë°˜ ì´ìƒ({affected_percentage:.1f}%)ì´ ì—ëŸ¬ë¥¼ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. ê¸´ê¸‰ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif affected_percentage > 20:
            severity = "ğŸŸ  ì‹¬ê°"
            message = f"ì „ì²´ ì‚¬ìš©ìì˜ {affected_percentage:.1f}%ê°€ ì—ëŸ¬ë¥¼ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif affected_percentage > 5:
            severity = "ğŸŸ¡ ì£¼ì˜"
            message = f"ì¼ë¶€ ì‚¬ìš©ì({affected_percentage:.1f}%)ê°€ ì—ëŸ¬ë¥¼ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ì„¸ìš”."
        else:
            severity = "ğŸŸ¢ ì–‘í˜¸"
            message = f"ì†Œìˆ˜ ì‚¬ìš©ì({affected_percentage:.1f}%)ë§Œ ì—ëŸ¬ë¥¼ ê²½í—˜í–ˆìŠµë‹ˆë‹¤."

        summary_lines.append(f"**ì‹¬ê°ë„**: {severity}")
        summary_lines.append(f"**í‰ê°€**: {message}")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì‚¬ìš©ì ì˜í–¥ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def detect_anomalies(
    project_uuid: str,
    metric: str = "error_rate",
    time_hours: int = 168,
    sensitivity: float = 2.0
) -> str:
    """
    í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ (í‰ì†Œë³´ë‹¤ ë¹„ì •ìƒì ì¸ íŒ¨í„´ ê°ì§€)

    ê³¼ê±° ë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í˜„ì¬ ê°’ì´ ì´ìƒí•œì§€ íŒë‹¨

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        metric: ì¸¡ì • ì§€í‘œ (error_rate|traffic|latency)
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 7ì¼)
        sensitivity: ë¯¼ê°ë„ (í‘œì¤€í¸ì°¨ ë°°ìˆ˜, ê¸°ë³¸ 2.0)
            - 2.0 = Â±2Ïƒ ë²—ì–´ë‚˜ë©´ ì´ìƒ (95%)
            - 3.0 = Â±3Ïƒ ë²—ì–´ë‚˜ë©´ ì´ìƒ (99.7%, ë” ì—„ê²©)

    Returns:
        ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ + ì •ìƒ ë²”ìœ„ + í˜„ì¬ ê°’

    Examples:
        - "í‰ì†Œë³´ë‹¤ ì—ëŸ¬ê°€ ë§ë‚˜ìš”?"
        - "íŠ¸ë˜í”½ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ê¸‰ì¦í–ˆë‚˜ìš”?"
    """
    try:
        from app.core.opensearch import opensearch_client
        from datetime import datetime, timedelta
        import statistics

        client = opensearch_client
        index_name = f"logs_{project_uuid}"

        now = datetime.now()
        start_time = now - timedelta(hours=time_hours)

        summary_lines = [f"=== ì´ìƒ íƒì§€ ë¶„ì„ ({metric}) ===", ""]

        # ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜ì§‘ (1ì‹œê°„ ë‹¨ìœ„)
        interval = "1h"
        response = client.search(
            index=index_name,
            body={
                "size": 0,
                "query": {
                    "range": {
                        "timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": now.isoformat()
                        }
                    }
                },
                "aggs": {
                    "time_buckets": {
                        "date_histogram": {
                            "field": "timestamp",
                            "fixed_interval": interval
                        },
                        "aggs": {
                            "error_count": {
                                "filter": {"term": {"level.keyword": "ERROR"}}
                            }
                        }
                    }
                }
            }
        )

        buckets = response['aggregations']['time_buckets']['buckets']

        if len(buckets) < 10:
            from app.tools.response_templates import get_conditional_failure_message
            return get_conditional_failure_message(
                condition="ë°ì´í„° í¬ì¸íŠ¸",
                current_value=f"{len(buckets)}ê°œ",
                required_value="10ê°œ ì´ìƒ",
                tool_name="ì´ìƒ íƒì§€ ë¶„ì„"
            )

        # ë©”íŠ¸ë¦­ë³„ ê°’ ê³„ì‚°
        values = []
        timestamps = []

        for bucket in buckets:
            timestamp = bucket['key_as_string']
            total_count = bucket['doc_count']
            error_count = bucket['error_count']['doc_count']

            if metric == "error_rate":
                value = (error_count / total_count * 100) if total_count > 0 else 0.0
            elif metric == "traffic":
                value = total_count
            elif metric == "latency":
                # latencyëŠ” ë³„ë„ aggregation í•„ìš” (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”)
                value = 0.0  # ì‹¤ì œ êµ¬í˜„ ì‹œ avg ì‚¬ìš©
            else:
                return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” metric: {metric}"

            values.append(value)
            timestamps.append(timestamp)

        if len(values) < 10:
            from app.tools.response_templates import get_conditional_failure_message
            return get_conditional_failure_message(
                condition="ìœ íš¨í•œ ë°ì´í„° í¬ì¸íŠ¸",
                current_value=f"{len(values)}ê°œ",
                required_value="10ê°œ ì´ìƒ",
                tool_name="ì´ìƒ íƒì§€ ë¶„ì„"
            )

        # í†µê³„ ê³„ì‚°
        mean_value = statistics.mean(values)
        stdev_value = statistics.stdev(values) if len(values) > 1 else 0.0

        # ì •ìƒ ë²”ìœ„ ê³„ì‚° (í‰ê·  Â± sensitivityÃ—í‘œì¤€í¸ì°¨)
        upper_threshold = mean_value + (sensitivity * stdev_value)
        lower_threshold = max(0, mean_value - (sensitivity * stdev_value))

        # í˜„ì¬ ê°’ (ìµœê·¼ 1ì‹œê°„)
        current_value = values[-1] if values else 0.0
        current_time = timestamps[-1] if timestamps else "N/A"

        # ì´ìƒ ì—¬ë¶€ íŒë‹¨
        is_anomaly = current_value > upper_threshold or current_value < lower_threshold
        anomaly_type = ""
        if current_value > upper_threshold:
            anomaly_type = "ê¸‰ì¦ (ìƒí•œ ì´ˆê³¼)"
        elif current_value < lower_threshold:
            anomaly_type = "ê¸‰ê° (í•˜í•œ ë¯¸ë‹¬)"

        # ê²°ê³¼ ìš”ì•½
        summary_lines.append(f"**ë¶„ì„ ê¸°ê°„:** {time_hours}ì‹œê°„ ({len(buckets)}ê°œ ë°ì´í„° í¬ì¸íŠ¸)")
        summary_lines.append(f"**ì¸¡ì • ì§€í‘œ:** {metric}")
        summary_lines.append(f"**ë¯¼ê°ë„:** Â±{sensitivity}Ïƒ (í‘œì¤€í¸ì°¨)")
        summary_lines.append("")

        summary_lines.append("**ğŸ“Š í†µê³„ ì •ë³´:**")
        unit = "%" if metric == "error_rate" else "ê±´"
        summary_lines.append(f"- í‰ê· ê°’: **{mean_value:.2f}{unit}**")
        summary_lines.append(f"- í‘œì¤€í¸ì°¨: {stdev_value:.2f}{unit}")
        summary_lines.append(f"- ì •ìƒ ë²”ìœ„: **{lower_threshold:.2f} ~ {upper_threshold:.2f}{unit}**")
        summary_lines.append("")

        summary_lines.append("**ğŸ” í˜„ì¬ ìƒíƒœ:**")
        summary_lines.append(f"- ì‹œê°„: {current_time}")
        summary_lines.append(f"- í˜„ì¬ ê°’: **{current_value:.2f}{unit}**")
        summary_lines.append("")

        if is_anomaly:
            summary_lines.append(f"**ğŸš¨ ì´ìƒ íƒì§€: {anomaly_type}**")
            deviation = abs(current_value - mean_value) / stdev_value if stdev_value > 0 else 0
            summary_lines.append(f"- í‰ê·  ëŒ€ë¹„ {deviation:.1f}Ïƒ ë²—ì–´ë‚¨")
            summary_lines.append(f"- í‰ê·  ëŒ€ë¹„ {abs(current_value - mean_value):.2f}{unit} ì°¨ì´")
            summary_lines.append("")
            summary_lines.append("**ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:**")
            if current_value > upper_threshold:
                summary_lines.append("1. ìµœê·¼ ë°°í¬ë‚˜ ì„¤ì • ë³€ê²½ í™•ì¸")
                summary_lines.append("2. ì„œë¹„ìŠ¤ ë¡œê·¸ ìƒì„¸ ë¶„ì„")
                summary_lines.append("3. ì•Œë¦¼ ì„¤ì • ë° ëª¨ë‹ˆí„°ë§ ê°•í™”")
            else:
                summary_lines.append("1. ë°ì´í„° ìˆ˜ì§‘ ì •ìƒ ì‘ë™ í™•ì¸")
                summary_lines.append("2. íŠ¸ë˜í”½ ìœ ì… ê²½ë¡œ ì ê²€")
        else:
            summary_lines.append("**âœ… ì •ìƒ ë²”ìœ„ ë‚´**")
            summary_lines.append(f"- í‰ê·  ëŒ€ë¹„ ì •ìƒì ì¸ ìˆ˜ì¤€ì…ë‹ˆë‹¤")
            summary_lines.append(f"- í˜„ì¬ ê°’ì€ ì •ìƒ ë²”ìœ„ ({lower_threshold:.2f} ~ {upper_threshold:.2f}{unit}) ì•ˆì— ìˆìŠµë‹ˆë‹¤")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"ì´ìƒ íƒì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def compare_source_types(
    project_uuid: str,
    time_hours: int = 24
) -> str:
    """FE/BE/INFRA ì†ŒìŠ¤ë³„ ì—ëŸ¬ìœ¨ ë° ë¡œê·¸ ë¶„í¬ ë¹„êµ. ì‚¬ìš©: "í”„ë¡ íŠ¸ì—”ë“œ vs ë°±ì—”ë“œ ì—ëŸ¬ ë¹„êµ" âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„"""
    from datetime import datetime, timedelta
    from app.core.opensearch import opensearch_client
    
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)
    
    try:
        results = opensearch_client.search(index=index_pattern, body={
            "size": 0, "query": {"range": {"timestamp": {"gte": start_time.isoformat()+"Z", "lte": end_time.isoformat()+"Z"}}},
            "aggs": {"by_source": {"terms": {"field": "source_type", "size": 5}, 
                "aggs": {"error_count": {"filter": {"term": {"level": "ERROR"}}}}}}})
        
        buckets = results.get("aggregations", {}).get("by_source", {}).get("buckets", [])
        if not buckets: return f"ìµœê·¼ {time_hours}ì‹œê°„ ë¡œê·¸ ì—†ìŒ"
        
        lines = [f"## ğŸ“Š Source Typeë³„ ë¹„êµ (ìµœê·¼ {time_hours}ì‹œê°„)", ""]
        lines.extend(["| Source | ì´ ë¡œê·¸ | ì—ëŸ¬ | ì—ëŸ¬ìœ¨ |", "|--------|---------|------|--------|"])
        for b in buckets:
            src = b.get("key", "Unknown")
            total = b.get("doc_count", 0)
            errors = b.get("error_count", {}).get("doc_count", 0)
            rate = (errors/total*100) if total > 0 else 0
            lines.append(f"| {src} | {total} | {errors} | {rate:.1f}% |")
        return "\n".join(lines)
    except Exception as e:
        return f"Source Type ë¹„êµ ì¤‘ ì˜¤ë¥˜: {str(e)}"


@tool
async def analyze_logger_activity(
    project_uuid: str,
    time_hours: int = 24,
    top_n: int = 10
) -> str:
    """ë¡œê±°ë³„ í™œë™ëŸ‰ê³¼ ì—ëŸ¬ìœ¨ì„ ë¶„ì„. ì‚¬ìš©: "ì–´ë–¤ í´ë˜ìŠ¤ê°€ ë¡œê·¸ë¥¼ ë§ì´ ë‚¨ê²¨?", "ë¡œê·¸ ë…¸ì´ì¦ˆ ë§ì€ í´ë˜ìŠ¤" âš ï¸ 1íšŒ í˜¸ì¶œ ì¶©ë¶„"""
    from datetime import datetime, timedelta
    from app.core.opensearch import opensearch_client
    
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)
    
    try:
        results = opensearch_client.search(index=index_pattern, body={
            "size": 0, "query": {"range": {"timestamp": {"gte": start_time.isoformat()+"Z", "lte": end_time.isoformat()+"Z"}}},
            "aggs": {"by_logger": {"terms": {"field": "logger", "size": top_n}, 
                "aggs": {"errors": {"filter": {"term": {"level": "ERROR"}}}}}}})
        
        buckets = results.get("aggregations", {}).get("by_logger", {}).get("buckets", [])
        if not buckets: return f"ìµœê·¼ {time_hours}ì‹œê°„ ë¡œê±° ë°ì´í„° ì—†ìŒ"
        
        lines = [f"## ğŸ“ ë¡œê±° í™œë™ TOP {top_n}", ""]
        lines.extend(["| ë¡œê±° | ë¡œê·¸ ìˆ˜ | ì—ëŸ¬ | í‰ê°€ |", "|------|---------|------|------|"])
        for b in buckets:
            logger = b.get("key", "")[:50]
            count = b.get("doc_count", 0)
            errors = b.get("errors", {}).get("doc_count", 0)
            noise = "ğŸ”Š ë†’ìŒ" if count > 1000 else "ë‚®ìŒ"
            lines.append(f"| {logger} | {count} | {errors} | {noise} |")
        return "\n".join(lines)
    except Exception as e:
        return f"ë¡œê±° í™œë™ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"

