"""
Vector í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ íŒ¨í„´ ìë™ ë°œê²¬ ë„êµ¬

ë³µì¡í•œ GROUP BY ì¿¼ë¦¬ë¥¼ Vector í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ìë™ìœ¼ë¡œ íŒ¨í„´ ë°œê²¬
"""

from typing import Dict, Any, List
from datetime import datetime, timedelta
from langchain_core.tools import tool
import numpy as np

from app.core.opensearch import opensearch_client


@tool
async def discover_error_patterns(
    project_uuid: str,
    time_range: str = "24h",
    min_cluster_size: int = 5,
    max_clusters: int = 10
) -> str:
    """
    Vector í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ERROR íŒ¨í„´ ìë™ ë°œê²¬

    ì „í†µì  ë°©ì‹:
    SELECT
      SUBSTRING(message, 1, 50) as pattern,
      COUNT(*) as count,
      MIN(timestamp) as first_seen
    FROM logs
    WHERE level = 'ERROR'
    GROUP BY pattern
    HAVING count > 10

    Vector ë°©ì‹:
    ìë™ìœ¼ë¡œ ìœ ì‚¬ ì—ëŸ¬ í´ëŸ¬ìŠ¤í„°ë§

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        time_range: ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸: 24h)
        min_cluster_size: ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (ê¸°ë³¸: 5)
        max_clusters: ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ê¸°ë³¸: 10)

    Returns:
        ë°œê²¬ëœ ì—ëŸ¬ íŒ¨í„´ ìš”ì•½
    """
    # ERROR ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
    error_logs = await _get_error_logs_with_vectors(project_uuid, time_range)

    if not error_logs:
        return f"âŒ {time_range} ë™ì•ˆ ë²¡í„°í™”ëœ ERROR ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    if len(error_logs) < min_cluster_size:
        return f"âš ï¸ ERROR ë¡œê·¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(error_logs)}ê°œ). ìµœì†Œ {min_cluster_size}ê°œ í•„ìš”."

    # Vector í´ëŸ¬ìŠ¤í„°ë§
    clusters = _cluster_error_logs(error_logs, min_cluster_size, max_clusters)

    if not clusters:
        return "âŒ ìœ ì˜ë¯¸í•œ íŒ¨í„´ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ê²°ê³¼ í¬ë§·íŒ…
    return _format_pattern_results(clusters, len(error_logs))


@tool
async def analyze_error_trends(
    project_uuid: str,
    time_range: str = "7d",
    interval: str = "1d"
) -> str:
    """
    ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ íŒ¨í„´ íŠ¸ë Œë“œ ë¶„ì„

    ì „í†µì  ë°©ì‹:
    SELECT
      DATE_TRUNC('day', timestamp) as date,
      CASE
        WHEN message LIKE '%timeout%' THEN 'timeout'
        WHEN message LIKE '%null%' THEN 'null_error'
        ELSE 'other'
      END as error_type,
      COUNT(*) as count
    FROM logs
    WHERE level = 'ERROR'
    GROUP BY date, error_type

    Vector ë°©ì‹:
    í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ íŒ¨í„´ì„ ìë™ ì‹ë³„í•˜ê³  ì‹œê°„ëŒ€ë³„ ì¶”ì´ ë¶„ì„

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        time_range: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸: 7d)
        interval: ì§‘ê³„ ê°„ê²© (ê¸°ë³¸: 1d)

    Returns:
        ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ íŒ¨í„´ íŠ¸ë Œë“œ
    """
    # ì‹œê°„ëŒ€ë³„ ERROR ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
    time_buckets = _parse_interval(interval)
    all_patterns = {}

    for bucket in time_buckets:
        bucket_logs = await _get_error_logs_with_vectors(
            project_uuid,
            time_range=f"{bucket['hours']}h",
            start_time=bucket['start']
        )

        if bucket_logs:
            patterns = _cluster_error_logs(bucket_logs, min_cluster_size=3)
            all_patterns[bucket['label']] = patterns

    return _format_trend_results(all_patterns)


async def _get_error_logs_with_vectors(
    project_uuid: str,
    time_range: str,
    start_time: datetime = None
) -> List[Dict[str, Any]]:
    """
    log_vectorê°€ ìˆëŠ” ERROR ë¡œê·¸ ì¡°íšŒ

    Args:
        project_uuid: í”„ë¡œì íŠ¸ UUID
        time_range: ì‹œê°„ ë²”ìœ„ (ì˜ˆ: "24h")
        start_time: ì‹œì‘ ì‹œê°„ (ì„ íƒì )

    Returns:
        ERROR ë¡œê·¸ ë¦¬ìŠ¤íŠ¸ (log_vector í¬í•¨)
    """
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ íŒŒì‹±
    if not start_time:
        unit = time_range[-1]
        value = int(time_range[:-1])

        if unit == 'h':
            delta = timedelta(hours=value)
        elif unit == 'd':
            delta = timedelta(days=value)
        else:
            delta = timedelta(hours=24)

        start_time = datetime.utcnow() - delta

    query_body = {
        "size": 1000,  # ìµœëŒ€ 1000ê°œ
        "query": {
            "bool": {
                "must": [
                    {"term": {"level": "ERROR"}},
                    {"exists": {"field": "log_vector"}},
                    {
                        "range": {
                            "timestamp": {
                                "gte": start_time.isoformat() + "Z"
                            }
                        }
                    }
                ]
            }
        },
        "_source": ["log_id", "message", "timestamp", "service_name", "log_vector"]
    }

    try:
        results = opensearch_client.search(index=index_pattern, body=query_body)
        hits = results.get("hits", {}).get("hits", [])

        logs = []
        for hit in hits:
            source = hit["_source"]
            if "log_vector" in source:
                logs.append(source)

        return logs

    except Exception as e:
        print(f"Error querying ERROR logs: {e}")
        return []


def _cluster_error_logs(
    logs: List[Dict[str, Any]],
    min_cluster_size: int = 5,
    max_clusters: int = 10
) -> List[Dict[str, Any]]:
    """
    DBSCANì„ ì‚¬ìš©í•˜ì—¬ ERROR ë¡œê·¸ í´ëŸ¬ìŠ¤í„°ë§

    Args:
        logs: ERROR ë¡œê·¸ ë¦¬ìŠ¤íŠ¸ (log_vector í¬í•¨)
        min_cluster_size: ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        max_clusters: ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜

    Returns:
        í´ëŸ¬ìŠ¤í„° ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from sklearn.cluster import DBSCAN
        from sklearn.metrics.pairwise import cosine_distances
    except ImportError:
        return [{"error": "sklearnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install scikit-learn"}]

    # Vector ì¶”ì¶œ
    vectors = np.array([log["log_vector"] for log in logs])

    # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (ì½”ì‚¬ì¸ ê±°ë¦¬ ì‚¬ìš©)
    # eps=0.15: 15% ì´ë‚´ì˜ ê±°ë¦¬ë¥¼ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¡œ ê°„ì£¼
    clustering = DBSCAN(
        eps=0.15,
        min_samples=min_cluster_size,
        metric='cosine'
    )
    labels = clustering.fit_predict(vectors)

    # í´ëŸ¬ìŠ¤í„°ë³„ ë¡œê·¸ ê·¸ë£¹í™”
    clusters = {}
    for i, label in enumerate(labels):
        if label == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
            continue

        if label not in clusters:
            clusters[label] = []

        clusters[label].append(logs[i])

    # í´ëŸ¬ìŠ¤í„° ì •ë³´ ìƒì„±
    cluster_info = []
    for label, cluster_logs in sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)[:max_clusters]:
        # ëŒ€í‘œ ë¡œê·¸ ì„ íƒ (ì²« ë²ˆì§¸ ë¡œê·¸)
        representative = cluster_logs[0]

        # ì²« ë°œìƒ ì‹œê°„
        timestamps = [log.get("timestamp", "") for log in cluster_logs]
        first_seen = min(timestamps) if timestamps else "Unknown"

        # ì˜í–¥ë°›ì€ ì„œë¹„ìŠ¤
        services = list(set(log.get("service_name", "unknown") for log in cluster_logs))

        cluster_info.append({
            "pattern_id": f"pattern_{label}",
            "count": len(cluster_logs),
            "representative_message": representative.get("message", "")[:150],
            "first_seen": first_seen,
            "affected_services": services[:5],  # ìµœëŒ€ 5ê°œ
            "example_log_ids": [log.get("log_id") for log in cluster_logs[:3]]
        })

    return cluster_info


def _format_pattern_results(clusters: List[Dict[str, Any]], total_errors: int) -> str:
    """
    íŒ¨í„´ ë°œê²¬ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í¬ë§·íŒ…

    Args:
        clusters: í´ëŸ¬ìŠ¤í„° ì •ë³´ ë¦¬ìŠ¤íŠ¸
        total_errors: ì´ ERROR ë¡œê·¸ ìˆ˜

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¬¸ìì—´
    """
    result = f"""ğŸ” ERROR íŒ¨í„´ ìë™ ë°œê²¬ ê²°ê³¼

ğŸ“Š ì´ ERROR ë¡œê·¸: {total_errors}ê°œ
ğŸ¯ ë°œê²¬ëœ ì£¼ìš” íŒ¨í„´: {len(clusters)}ê°œ
ğŸ“ˆ í´ëŸ¬ìŠ¤í„°ë§ ì»¤ë²„ë¦¬ì§€: {sum(c['count'] for c in clusters) / total_errors * 100:.1f}%

---

"""

    for i, cluster in enumerate(clusters, 1):
        result += f"""### íŒ¨í„´ #{i}: {cluster['pattern_id']}

- **ë°œìƒ íšŸìˆ˜**: {cluster['count']}íšŒ
- **ì²« ë°œìƒ**: {cluster['first_seen'][:19] if cluster['first_seen'] != 'Unknown' else 'Unknown'}
- **ì˜í–¥ ì„œë¹„ìŠ¤**: {', '.join(cluster['affected_services'])}
- **ëŒ€í‘œ ë©”ì‹œì§€**: `{cluster['representative_message']}`
- **ì˜ˆì‹œ ë¡œê·¸ ID**: {', '.join(cluster['example_log_ids'])}

"""

    result += """---

âœ… Vector í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë³µì¡í•œ GROUP BY ì—†ì´ ìë™ìœ¼ë¡œ íŒ¨í„´ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
"""

    return result


def _format_trend_results(all_patterns: Dict[str, List[Dict[str, Any]]]) -> str:
    """
    íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…

    Args:
        all_patterns: ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë”•ì…”ë„ˆë¦¬

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¬¸ìì—´
    """
    result = "ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ERROR íŒ¨í„´ íŠ¸ë Œë“œ\n\n"

    for time_label, patterns in all_patterns.items():
        result += f"**{time_label}**\n"
        if patterns:
            for pattern in patterns[:3]:  # ìƒìœ„ 3ê°œë§Œ
                result += f"  - {pattern['pattern_id']}: {pattern['count']}íšŒ\n"
        else:
            result += "  - íŒ¨í„´ ì—†ìŒ\n"
        result += "\n"

    return result


def _parse_interval(interval: str) -> List[Dict[str, Any]]:
    """
    ê°„ê²© ë¬¸ìì—´ì„ ì‹œê°„ ë²„í‚·ìœ¼ë¡œ ë³€í™˜

    Args:
        interval: ê°„ê²© (ì˜ˆ: "1d", "6h")

    Returns:
        ì‹œê°„ ë²„í‚· ë¦¬ìŠ¤íŠ¸
    """
    # ê°„ë‹¨í•œ êµ¬í˜„: 1ì¼ ê°„ê²© ê¸°ë³¸
    return [
        {"label": "ì˜¤ëŠ˜", "hours": 24, "start": datetime.utcnow() - timedelta(days=1)},
        {"label": "ì–´ì œ", "hours": 24, "start": datetime.utcnow() - timedelta(days=2)},
        {"label": "ê·¸ì €ê»˜", "hours": 24, "start": datetime.utcnow() - timedelta(days=3)}
    ]
