"""
ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ (Performance Analysis Tools)
- API ì‘ë‹µ ì‹œê°„ ë¶„ì„, íŠ¸ë˜í”½ ë¶„í¬ ë¶„ì„
"""

from typing import Optional
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client


@tool
async def get_slowest_apis(
    project_uuid: str,
    limit: int = 10,
    time_hours: int = 168,  # ê¸°ë³¸ 7ì¼
    service_name: Optional[str] = None,
    min_execution_time: Optional[int] = None
) -> str:
    """
    ì‘ë‹µ ì‹œê°„ì´ ê°€ì¥ ëŠë¦° API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - "ì‘ë‹µ ì‹œê°„ì´ ê°€ì¥ ëŠë¦° APIëŠ”?"
    - "í‰ê·  ì‘ë‹µ ì‹œê°„ì´ ê¸´ ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„"
    - "ì„±ëŠ¥ ë³‘ëª© ì§€ì  íŒŒì•…"

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        limit: ì¡°íšŒí•  API ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        time_hours: ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 168ì‹œê°„=7ì¼)
        service_name: ì„œë¹„ìŠ¤ ì´ë¦„ í•„í„° (ì„ íƒ)
        min_execution_time: ìµœì†Œ ì‹¤í–‰ ì‹œê°„ í•„í„° (ë°€ë¦¬ì´ˆ, ì„ íƒ)

    ì°¸ê³ : project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        ì‘ë‹µ ì‹œê°„ì´ ëŠë¦° API ëª©ë¡ (í‰ê· /ìµœëŒ€/P95 í¬í•¨)
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    # í‰ë©´ êµ¬ì¡°ì™€ nested êµ¬ì¡° ëª¨ë‘ ì§€ì› (execution_time, duration, log_details.execution_time)
    must_clauses = [
        {
            "bool": {
                "should": [
                    {"exists": {"field": "log_details.execution_time"}},
                    {"exists": {"field": "execution_time"}},
                    {"exists": {"field": "duration"}}
                ],
                "minimum_should_match": 1
            }
        },
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }},
        # ì‹¤ì œ HTTP APIë§Œ í•„í„°ë§ (Controller ë ˆì´ì–´)
        {
            "bool": {
                "should": [
                    {"exists": {"field": "log_details.request_uri"}},
                    {"exists": {"field": "log_details.request_uri.keyword"}}
                ],
                "minimum_should_match": 1
            }
        }
    ]

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    if min_execution_time:
        # ëª¨ë“  execution_time í•„ë“œì— ëŒ€í•´ í•„í„° ì ìš©
        must_clauses.append({
            "bool": {
                "should": [
                    {"range": {"log_details.execution_time": {"gte": min_execution_time}}},
                    {"range": {"execution_time": {"gte": min_execution_time}}},
                    {"range": {"duration": {"gte": min_execution_time}}}
                ],
                "minimum_should_match": 1
            }
        })

    try:
        # OpenSearch Aggregation Query
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,  # ì§‘ê³„ë§Œ ìˆ˜í–‰
                "query": {
                    "bool": {"must": must_clauses}
                },
                "aggs": {
                    "by_api": {
                        "terms": {
                            # HTTP ë©”ì„œë“œ + request_uri ì¡°í•©ìœ¼ë¡œ API ì‹ë³„
                            "script": {
                                "source": """
                                    String httpMethod = 'N/A';
                                    String requestUri = 'unknown';

                                    if (doc.containsKey('log_details.http_method.keyword') &&
                                        doc['log_details.http_method.keyword'].size() > 0) {
                                        httpMethod = doc['log_details.http_method.keyword'].value;
                                    }

                                    if (doc.containsKey('log_details.request_uri.keyword') &&
                                        doc['log_details.request_uri.keyword'].size() > 0) {
                                        requestUri = doc['log_details.request_uri.keyword'].value;
                                    }

                                    if (requestUri != 'unknown') {
                                        return httpMethod + ' ' + requestUri;
                                    } else {
                                        return 'unknown';
                                    }
                                """
                            },
                            "size": limit,
                            "order": {"avg_time": "desc"}  # í‰ê·  ì‹œê°„ ë‚´ë¦¼ì°¨ìˆœ
                        },
                        "aggs": {
                            "avg_time": {
                                "avg": {
                                    # í‰ë©´/nested êµ¬ì¡° ëª¨ë‘ ì§€ì›
                                    "script": {
                                        "source": """
                                            if (doc.containsKey('log_details.execution_time') &&
                                                doc['log_details.execution_time'].size() > 0) {
                                                return doc['log_details.execution_time'].value;
                                            } else if (doc.containsKey('execution_time') &&
                                                       doc['execution_time'].size() > 0) {
                                                return doc['execution_time'].value;
                                            } else if (doc.containsKey('duration') &&
                                                       doc['duration'].size() > 0) {
                                                return doc['duration'].value;
                                            } else {
                                                return null;
                                            }
                                        """
                                    }
                                }
                            },
                            "max_time": {
                                "max": {
                                    "script": {
                                        "source": """
                                            if (doc.containsKey('log_details.execution_time') &&
                                                doc['log_details.execution_time'].size() > 0) {
                                                return doc['log_details.execution_time'].value;
                                            } else if (doc.containsKey('execution_time') &&
                                                       doc['execution_time'].size() > 0) {
                                                return doc['execution_time'].value;
                                            } else if (doc.containsKey('duration') &&
                                                       doc['duration'].size() > 0) {
                                                return doc['duration'].value;
                                            } else {
                                                return null;
                                            }
                                        """
                                    }
                                }
                            },
                            "min_time": {
                                "min": {
                                    "script": {
                                        "source": """
                                            if (doc.containsKey('log_details.execution_time') &&
                                                doc['log_details.execution_time'].size() > 0) {
                                                return doc['log_details.execution_time'].value;
                                            } else if (doc.containsKey('execution_time') &&
                                                       doc['execution_time'].size() > 0) {
                                                return doc['execution_time'].value;
                                            } else if (doc.containsKey('duration') &&
                                                       doc['duration'].size() > 0) {
                                                return doc['duration'].value;
                                            } else {
                                                return null;
                                            }
                                        """
                                    }
                                }
                            },
                            "percentiles": {
                                "percentiles": {
                                    "script": {
                                        "source": """
                                            if (doc.containsKey('log_details.execution_time') &&
                                                doc['log_details.execution_time'].size() > 0) {
                                                return doc['log_details.execution_time'].value;
                                            } else if (doc.containsKey('execution_time') &&
                                                       doc['execution_time'].size() > 0) {
                                                return doc['execution_time'].value;
                                            } else if (doc.containsKey('duration') &&
                                                       doc['duration'].size() > 0) {
                                                return doc['duration'].value;
                                            } else {
                                                return null;
                                            }
                                        """
                                    },
                                    "percents": [50, 95, 99]
                                }
                            },
                            "by_http_method": {
                                "terms": {
                                    "field": "log_details.http_method.keyword",
                                    "size": 5,
                                    "missing": "N/A"  # í•„ë“œê°€ ì—†ìœ¼ë©´ N/Aë¡œ í‘œì‹œ
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        buckets = results.get("aggregations", {}).get("by_api", {}).get("buckets", [])
        total_hits = results.get("hits", {}).get("total", {}).get("value", 0)

        # 'unknown' API ì œì™¸ (request_uriê°€ ì—†ëŠ” ë‚´ë¶€ ë©”ì„œë“œ)
        buckets = [b for b in buckets if b.get("key", "unknown") != "unknown"]

        if not buckets:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì‘ë‹µ ì‹œê°„ ë°ì´í„°ê°€ ìˆëŠ” APIê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì‘ë‹µ ì‹œê°„ì´ ëŠë¦° API ë¶„ì„ (ìµœê·¼ {time_hours}ì‹œê°„) ===",
            f"ì´ {total_hits}ê±´ì˜ ìš”ì²­ ë¶„ì„, ìƒìœ„ {len(buckets)}ê°œ API í‘œì‹œ",
            ""
        ]

        # API ëª©ë¡
        for i, bucket in enumerate(buckets, 1):
            api_path = bucket.get("key", "N/A")
            doc_count = bucket.get("doc_count", 0)
            avg_time = bucket.get("avg_time", {}).get("value", 0)
            max_time = bucket.get("max_time", {}).get("value", 0)
            min_time = bucket.get("min_time", {}).get("value", 0)
            percentiles = bucket.get("percentiles", {}).get("values", {})
            p50 = percentiles.get("50.0", 0)
            p95 = percentiles.get("95.0", 0)
            p99 = percentiles.get("99.0", 0)

            summary_lines.append(f"{i}. {api_path}")
            summary_lines.append(f"   ğŸ“Š ìš”ì²­ ìˆ˜: {doc_count}ê±´")
            summary_lines.append(f"   â±ï¸  í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.0f}ms")
            summary_lines.append(f"   â±ï¸  ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max_time:.0f}ms")
            summary_lines.append(f"   â±ï¸  ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min_time:.0f}ms")
            summary_lines.append(f"   ğŸ“ˆ P50 (ì¤‘ì•™ê°’): {p50:.0f}ms")
            summary_lines.append(f"   ğŸ“ˆ P95: {p95:.0f}ms")
            summary_lines.append(f"   ğŸ“ˆ P99: {p99:.0f}ms")

            # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
            if avg_time > 5000:
                grade = "ğŸ”´ ë§¤ìš° ëŠë¦¼ (5ì´ˆ ì´ìƒ)"
            elif avg_time > 2000:
                grade = "ğŸŸ  ëŠë¦¼ (2-5ì´ˆ)"
            elif avg_time > 1000:
                grade = "ğŸŸ¡ ë³´í†µ (1-2ì´ˆ)"
            else:
                grade = "ğŸŸ¢ ë¹ ë¦„ (1ì´ˆ ì´í•˜)"
            summary_lines.append(f"   ë“±ê¸‰: {grade}")
            summary_lines.append("")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"API ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def get_traffic_by_time(
    project_uuid: str,
    interval: str = "1h",  # 1h, 30m, 1d ë“±
    time_hours: int = 168,  # ê¸°ë³¸ 7ì¼
    level: Optional[str] = None,
    service_name: Optional[str] = None
) -> str:
    """
    ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„í¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - "íŠ¸ë˜í”½ì´ ê°€ì¥ ë§ì€ ì‹œê°„ëŒ€ëŠ”?"
    - "ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ë°œìƒ íŒ¨í„´ ë¶„ì„"
    - "í”¼í¬ íƒ€ì„ íŒŒì•…"

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        interval: ì§‘ê³„ ê°„ê²© (1h=1ì‹œê°„, 30m=30ë¶„, 1d=1ì¼, ê¸°ë³¸ 1h)
        time_hours: ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 168ì‹œê°„=7ì¼)
        level: ë¡œê·¸ ë ˆë²¨ í•„í„° (ERROR, WARN, INFO, DEBUG ì¤‘ í•˜ë‚˜, ì„ íƒ)
        service_name: ì„œë¹„ìŠ¤ ì´ë¦„ í•„í„° (ì„ íƒ)

    ì°¸ê³ : project_uuidëŠ” ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ ì „ë‹¬í•˜ì§€ ë§ˆì„¸ìš”.

    Returns:
        ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„í¬ (ìš”ì²­ ìˆ˜, ë ˆë²¨ë³„ ë¶„í¬ í¬í•¨)
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´ (UUIDì˜ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜)
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    if level:
        must_clauses.append({"term": {"level": level.upper()}})

    if service_name:
        must_clauses.append({"term": {"service_name": service_name}})

    try:
        # OpenSearch Date Histogram Aggregation
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 0,  # ì§‘ê³„ë§Œ ìˆ˜í–‰
                "query": {
                    "bool": {"must": must_clauses}
                },
                "aggs": {
                    "traffic_over_time": {
                        "date_histogram": {
                            "field": "timestamp",
                            "fixed_interval": interval,
                            "time_zone": "Asia/Seoul",  # KST ì‹œê°„ëŒ€
                            "min_doc_count": 0  # ë°ì´í„° ì—†ëŠ” ì‹œê°„ëŒ€ë„ í‘œì‹œ
                        },
                        "aggs": {
                            "by_level": {
                                "terms": {
                                    "field": "level.keyword",
                                    "size": 10
                                }
                            },
                            "by_service": {
                                "terms": {
                                    "field": "service_name.keyword",
                                    "size": 5
                                }
                            }
                        }
                    }
                }
            }
        )

        # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
        buckets = results.get("aggregations", {}).get("traffic_over_time", {}).get("buckets", [])
        total_hits = results.get("hits", {}).get("total", {}).get("value", 0)

        if not buckets:
            return f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ íŠ¸ë˜í”½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê²°ê³¼ í¬ë§·íŒ…
        summary_lines = [
            f"=== ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„ì„ (ìµœê·¼ {time_hours}ì‹œê°„, ê°„ê²©: {interval}) ===",
            f"ì´ {total_hits}ê±´ì˜ ë¡œê·¸ ë¶„ì„, {len(buckets)}ê°œ ì‹œê°„ëŒ€",
            ""
        ]

        # í”¼í¬ íƒ€ì„ ì°¾ê¸°
        peak_bucket = max(buckets, key=lambda b: b.get("doc_count", 0))
        peak_time = peak_bucket.get("key_as_string", "N/A")
        peak_count = peak_bucket.get("doc_count", 0)

        summary_lines.append(f"ğŸ”¥ í”¼í¬ íƒ€ì„: {peak_time[:16].replace('T', ' ')} ({peak_count}ê±´)")
        summary_lines.append("")

        # ì‹œê°„ëŒ€ë³„ ìƒì„¸ ì •ë³´ (ìƒìœ„ 10ê°œ ë˜ëŠ” ì „ì²´)
        summary_lines.append("ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½:")
        display_buckets = sorted(buckets, key=lambda b: b.get("doc_count", 0), reverse=True)[:15]

        for bucket in display_buckets:
            time_str = bucket.get("key_as_string", "")[:16].replace("T", " ")  # YYYY-MM-DD HH:MM
            doc_count = bucket.get("doc_count", 0)

            # ë ˆë²¨ë³„ ë¶„í¬
            level_buckets = bucket.get("by_level", {}).get("buckets", [])
            level_dist = {lb["key"]: lb["doc_count"] for lb in level_buckets}

            # ì„œë¹„ìŠ¤ë³„ ë¶„í¬
            service_buckets = bucket.get("by_service", {}).get("buckets", [])
            top_service = service_buckets[0]["key"] if service_buckets else "N/A"

            # ê¸°ë³¸ ì •ë³´
            summary_lines.append(f"  ğŸ“… {time_str} | ì´ {doc_count}ê±´")

            # ë ˆë²¨ë³„ ë¶„í¬ (ERRORê°€ ìˆìœ¼ë©´ ê°•ì¡°)
            if level_dist:
                level_str_parts = []
                for lvl in ["ERROR", "WARN", "INFO", "DEBUG"]:
                    if lvl in level_dist:
                        count = level_dist[lvl]
                        emoji = "âŒ" if lvl == "ERROR" else "âš ï¸" if lvl == "WARN" else "â„¹ï¸" if lvl == "INFO" else "ğŸ›"
                        level_str_parts.append(f"{emoji}{lvl}({count})")
                if level_str_parts:
                    summary_lines.append(f"     ë ˆë²¨: {', '.join(level_str_parts)}")

            # ì£¼ìš” ì„œë¹„ìŠ¤
            if top_service != "N/A":
                summary_lines.append(f"     ì£¼ìš” ì„œë¹„ìŠ¤: {top_service}")

            summary_lines.append("")

        # ì „ì²´ ë ˆë²¨ ë¶„í¬ ìš”ì•½
        summary_lines.append("ì „ì²´ ë ˆë²¨ ë¶„í¬:")
        all_level_counts = {}
        for bucket in buckets:
            for lb in bucket.get("by_level", {}).get("buckets", []):
                level_key = lb["key"]
                all_level_counts[level_key] = all_level_counts.get(level_key, 0) + lb["doc_count"]

        for lvl in ["ERROR", "WARN", "INFO", "DEBUG"]:
            if lvl in all_level_counts:
                count = all_level_counts[lvl]
                percentage = (count / total_hits * 100) if total_hits > 0 else 0
                summary_lines.append(f"  - {lvl}: {count}ê±´ ({percentage:.1f}%)")

        return "\n".join(summary_lines)

    except Exception as e:
        return f"íŠ¸ë˜í”½ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
