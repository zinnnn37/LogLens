"""
V2 LangGraph API - Statistics Comparison Endpoints

AI vs DB í†µê³„ ë¹„êµë¥¼ í†µí•œ LLM ì—­ëŸ‰ ê²€ì¦ API
"""

import logging
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime

from app.tools.statistics_comparison_tools import (
    _get_db_statistics,
    _get_log_samples,
    _get_stratified_log_samples,
    _llm_estimate_statistics,
    _calculate_accuracy,
    _get_db_error_statistics,
    _sample_errors_with_vector,
    _llm_estimate_error_statistics,
    _calculate_error_accuracy
)
from app.tools.sampling_strategies import sample_two_stage_rare_aware

logger = logging.getLogger(__name__)


router = APIRouter(prefix="/v2-langgraph", tags=["Statistics Comparison"])


# Response Models
class DBStatistics(BaseModel):
    """DB ì§ì ‘ ì¡°íšŒ í†µê³„"""
    total_logs: int = Field(..., description="ì´ ë¡œê·¸ ìˆ˜")
    error_count: int = Field(..., description="ERROR ë¡œê·¸ ìˆ˜")
    warn_count: int = Field(..., description="WARN ë¡œê·¸ ìˆ˜")
    info_count: int = Field(..., description="INFO ë¡œê·¸ ìˆ˜")
    error_rate: float = Field(..., description="ì—ëŸ¬ìœ¨ (%)")
    peak_hour: str = Field(..., description="í”¼í¬ ì‹œê°„")
    peak_count: int = Field(..., description="í”¼í¬ ì‹œê°„ ë¡œê·¸ ìˆ˜")


class AIStatistics(BaseModel):
    """AI(LLM) ì¶”ë¡  í†µê³„"""
    estimated_total_logs: int = Field(..., description="ì¶”ë¡ í•œ ì´ ë¡œê·¸ ìˆ˜")
    estimated_error_count: int = Field(..., description="ì¶”ë¡ í•œ ERROR ë¡œê·¸ ìˆ˜")
    estimated_warn_count: int = Field(..., description="ì¶”ë¡ í•œ WARN ë¡œê·¸ ìˆ˜")
    estimated_info_count: int = Field(..., description="ì¶”ë¡ í•œ INFO ë¡œê·¸ ìˆ˜")
    estimated_error_rate: float = Field(..., description="ì¶”ë¡ í•œ ì—ëŸ¬ìœ¨ (%)")
    confidence_score: int = Field(..., description="AI ì‹ ë¢°ë„ (0-100)")
    reasoning: str = Field(..., description="ì¶”ë¡  ê·¼ê±°")


class AccuracyMetrics(BaseModel):
    """ì •í™•ë„ ì§€í‘œ"""
    total_logs_accuracy: float = Field(..., description="ì´ ë¡œê·¸ ìˆ˜ ì¼ì¹˜ìœ¨ (%)")
    error_count_accuracy: float = Field(..., description="ERROR ìˆ˜ ì¼ì¹˜ìœ¨ (%)")
    warn_count_accuracy: float = Field(..., description="WARN ìˆ˜ ì¼ì¹˜ìœ¨ (%)")
    info_count_accuracy: float = Field(..., description="INFO ìˆ˜ ì¼ì¹˜ìœ¨ (%)")
    error_rate_accuracy: float = Field(..., description="ì—ëŸ¬ìœ¨ ì •í™•ë„ (%)")
    overall_accuracy: float = Field(..., description="ì¢…í•© ì •í™•ë„ (%)")
    ai_confidence: int = Field(..., description="AI ìì²´ ì‹ ë¢°ë„")


class ComparisonVerdict(BaseModel):
    """ê²€ì¦ ê²°ë¡ """
    grade: str = Field(..., description="ë“±ê¸‰ (ë§¤ìš° ìš°ìˆ˜/ìš°ìˆ˜/ì–‘í˜¸/ë³´í†µ/ë¯¸í¡)")
    can_replace_db: bool = Field(..., description="DB ëŒ€ì²´ ê°€ëŠ¥ ì—¬ë¶€")
    explanation: str = Field(..., description="ì„¤ëª…")
    recommendations: List[str] = Field(..., description="ê¶Œì¥ ì‚¬í•­")


class AIvsDBComparisonResponse(BaseModel):
    """AI vs DB í†µê³„ ë¹„êµ ì‘ë‹µ"""
    project_uuid: str = Field(..., description="í”„ë¡œì íŠ¸ UUID")
    analysis_period_hours: int = Field(..., description="ë¶„ì„ ê¸°ê°„ (ì‹œê°„)")
    sample_size: int = Field(..., description="ì‚¬ìš©ëœ ìƒ˜í”Œ í¬ê¸°")
    analyzed_at: datetime = Field(..., description="ë¶„ì„ ì‹œì ")

    db_statistics: DBStatistics = Field(..., description="DB ì§ì ‘ ì¡°íšŒ ê²°ê³¼")
    ai_statistics: AIStatistics = Field(..., description="AI ì¶”ë¡  ê²°ê³¼")
    accuracy_metrics: AccuracyMetrics = Field(..., description="ì •í™•ë„ ì§€í‘œ")
    verdict: ComparisonVerdict = Field(..., description="ê²€ì¦ ê²°ë¡ ")

    technical_highlights: List[str] = Field(..., description="ê¸°ìˆ ì  ì–´í•„ í¬ì¸íŠ¸")


# ERROR ë¹„êµ ì „ìš© Response Models
class DBErrorStats(BaseModel):
    """DBì—ì„œ ì¡°íšŒí•œ ERROR í†µê³„"""
    total_errors: int = Field(..., description="ERROR ë¡œê·¸ ìˆ˜")
    error_rate: float = Field(..., description="ERROR ë¹„ìœ¨ (%)")
    peak_error_hour: Optional[str] = Field(None, description="ERROR ìµœë‹¤ ë°œìƒ ì‹œê°„")
    peak_error_count: Optional[int] = Field(None, description="ìµœë‹¤ ì‹œê°„ ERROR ìˆ˜")


class AIErrorStats(BaseModel):
    """AI ì¶”ì • ERROR í†µê³„"""
    estimated_total_errors: int = Field(..., description="ì¶”ì • ERROR ë¡œê·¸ ìˆ˜")
    estimated_error_rate: float = Field(..., description="ì¶”ì • ERROR ë¹„ìœ¨ (%)")
    confidence_score: int = Field(..., description="AI ì‹ ë¢°ë„ (0-100)")
    reasoning: str = Field(..., description="ì¶”ë¡  ê·¼ê±°")


class ErrorAccuracyMetrics(BaseModel):
    """ERROR ì •í™•ë„ ë©”íŠ¸ë¦­"""
    error_count_accuracy: float = Field(..., description="ERROR ìˆ˜ ì¼ì¹˜ìœ¨ (%)")
    error_rate_accuracy: float = Field(..., description="ERROR ë¹„ìœ¨ ì •í™•ë„ (%)")
    overall_accuracy: float = Field(..., description="ì¢…í•© ì •í™•ë„ (%)")


class VectorGroupingInfo(BaseModel):
    """Vector KNN ê·¸ë£¹í•‘ ì •ë³´"""
    vectorized_error_count: int = Field(..., description="log_vector ìˆëŠ” ERROR ê°œìˆ˜")
    vectorization_rate: float = Field(..., description="ë²¡í„°í™”ìœ¨ (%)")
    sampling_method: str = Field(..., description="ìƒ˜í”Œë§ ë°©ë²• (vector_knn ë˜ëŠ” random_fallback)")
    sample_distribution: str = Field(..., description="ìƒ˜í”Œ ë¶„í¬ ì„¤ëª…")


class ErrorComparisonResponse(BaseModel):
    """ERROR ë¡œê·¸ ë¹„êµ ê²°ê³¼"""
    project_uuid: str = Field(..., description="í”„ë¡œì íŠ¸ UUID")
    analysis_period_hours: int = Field(..., description="ë¶„ì„ ê¸°ê°„ (ì‹œê°„)")
    sample_size: int = Field(..., description="ì‚¬ìš©ëœ ìƒ˜í”Œ í¬ê¸°")
    analyzed_at: datetime = Field(..., description="ë¶„ì„ ì‹œì ")
    db_error_stats: DBErrorStats = Field(..., description="DB ERROR í†µê³„")
    ai_error_stats: AIErrorStats = Field(..., description="AI ERROR ì¶”ì •")
    accuracy_metrics: ErrorAccuracyMetrics = Field(..., description="ì •í™•ë„ ì§€í‘œ")
    vector_analysis: Optional[VectorGroupingInfo] = Field(None, description="Vector ìƒ˜í”Œë§ ì •ë³´")


class HourlyDataPoint(BaseModel):
    """ì‹œê°„ëŒ€ë³„ ë°ì´í„° í¬ì¸íŠ¸"""
    hour: str = Field(..., description="ì‹œê°„ (ISO format)")
    total: int = Field(..., description="ì´ ë¡œê·¸ ìˆ˜")
    error: int = Field(..., description="ERROR ìˆ˜")
    warn: int = Field(..., description="WARN ìˆ˜")
    info: int = Field(..., description="INFO ìˆ˜")


class HourlyStatisticsResponse(BaseModel):
    """ì‹œê°„ëŒ€ë³„ í†µê³„ ì‘ë‹µ"""
    project_uuid: str = Field(..., description="í”„ë¡œì íŠ¸ UUID")
    analysis_period_hours: int = Field(..., description="ë¶„ì„ ê¸°ê°„")
    total_logs: int = Field(..., description="ì´ ë¡œê·¸ ìˆ˜")
    error_rate: float = Field(..., description="ì—ëŸ¬ìœ¨ (%)")
    peak_hour: str = Field(..., description="í”¼í¬ ì‹œê°„")
    peak_count: int = Field(..., description="í”¼í¬ ì‹œê°„ ë¡œê·¸ ìˆ˜")
    hourly_data: List[HourlyDataPoint] = Field(..., description="ì‹œê°„ëŒ€ë³„ ë°ì´í„°")


@router.get("/statistics/compare", response_model=AIvsDBComparisonResponse)
async def compare_ai_vs_db(
    project_uuid: str = Query(..., description="í”„ë¡œì íŠ¸ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="ë¶„ì„ ê¸°ê°„ (ì‹œê°„, ê¸°ë³¸ 24ì‹œê°„)"),
    sample_size: int = Query(100, ge=10, le=500, description="AI ë¶„ì„ìš© ìƒ˜í”Œ í¬ê¸°")
) -> AIvsDBComparisonResponse:
    """
    AI vs DB í†µê³„ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ì—¬ LLMì˜ DB ëŒ€ì²´ ì—­ëŸ‰ì„ ê²€ì¦í•©ë‹ˆë‹¤.

    **ê²€ì¦ ëª©í‘œ:**
    - LLMì´ ë¡œê·¸ ìƒ˜í”Œë§Œìœ¼ë¡œ ì „ì²´ í†µê³„ë¥¼ ì •í™•íˆ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?
    - AIê°€ ì‚¬ëŒ(ë˜ëŠ” DB ì¿¼ë¦¬)ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ë³´ìœ í–ˆëŠ”ê°€?

    **ê¸°ìˆ ì  ì–´í•„ í¬ì¸íŠ¸:**
    - Temperature 0.1ë¡œ ì¼ê´€ëœ ì¶”ë¡  ë³´ì¥
    - Structured Outputìœ¼ë¡œ í˜•ì‹ ì˜¤ë¥˜ ë°©ì§€
    - ìë™í™”ëœ ì •í™•ë„ ì¸¡ì •ìœ¼ë¡œ ì‹ ë¢°ì„± ê²€ì¦

    **ë°˜í™˜ ë°ì´í„°:**
    - DB ì§ì ‘ ì¡°íšŒ ê²°ê³¼ (Ground Truth)
    - AI ì¶”ë¡  ê²°ê³¼
    - ì •í™•ë„ ì§€í‘œ (ì˜¤ì°¨ìœ¨)
    - ê²€ì¦ ê²°ë¡  (DB ëŒ€ì²´ ê°€ëŠ¥ ì—¬ë¶€)

    **ì •í™•ë„ ê¸°ì¤€:**
    - 95% ì´ìƒ: ë§¤ìš° ìš°ìˆ˜ (í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥)
    - 90% ì´ìƒ: ìš°ìˆ˜ (ëŒ€ë¶€ë¶„ ì—…ë¬´ì— í™œìš© ê°€ëŠ¥)
    - 80% ì´ìƒ: ì–‘í˜¸ (ë³´ì¡° ë„êµ¬ë¡œ ìœ ìš©)
    - 70% ì´ìƒ: ë³´í†µ (ê°œì„  í•„ìš”)
    - 70% ë¯¸ë§Œ: ë¯¸í¡ (ì¬ê²€í†  í•„ìš”)
    """
    logger.info(f"ğŸ¤– AI vs DB í†µê³„ ë¹„êµ ì‹œì‘: project_uuid={project_uuid}, time_hours={time_hours}, sample_size={sample_size}")

    try:
        # 1. DBì—ì„œ ì§ì ‘ í†µê³„ ì¡°íšŒ
        logger.debug(f"1ë‹¨ê³„: DB í†µê³„ ì¡°íšŒ ì‹œì‘")
        db_stats = _get_db_statistics(project_uuid, time_hours)
        logger.info(f"âœ… DB í†µê³„ ì¡°íšŒ ì™„ë£Œ: total_logs={db_stats.get('total_logs', 0)}")

        if db_stats["total_logs"] == 0:
            logger.warning(f"âš ï¸ ë¡œê·¸ ë°ì´í„° ì—†ìŒ: project_uuid={project_uuid}, time_hours={time_hours}")
            raise HTTPException(
                status_code=404,
                detail=f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        # 2. ë¡œê·¸ ìƒ˜í”Œ ì¶”ì¶œ (2ë‹¨ê³„ í¬ì†Œ ì´ë²¤íŠ¸ ì¸ì‹ ìƒ˜í”Œë§ + IPW ê°€ì¤‘ì¹˜)
        logger.debug(f"2ë‹¨ê³„: 2ë‹¨ê³„ í¬ì†Œ ì´ë²¤íŠ¸ ì¸ì‹ ìƒ˜í”Œë§ ì‹œì‘")
        sample_metadata = None
        try:
            log_samples, sample_metadata = await sample_two_stage_rare_aware(
                project_uuid=project_uuid,
                total_k=sample_size,
                time_hours=time_hours,
                rare_threshold=100  # 100ê°œ ë¯¸ë§Œì´ë©´ í¬ì†Œ ì´ë²¤íŠ¸ë¡œ ê°„ì£¼
            )
            sampling_method = sample_metadata.get('sampling_method', 'unknown')
            logger.info(
                f"âœ… 2ë‹¨ê³„ ìƒ˜í”Œë§ ì™„ë£Œ: sample_count={len(log_samples)}, "
                f"method={sampling_method}, "
                f"weights={sample_metadata.get('weights', {})}, "
                f"rare_levels={sample_metadata.get('rare_levels', [])}"
            )
        except Exception as e:
            logger.warning(f"âš ï¸ 2ë‹¨ê³„ ìƒ˜í”Œë§ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±: {type(e).__name__}: {str(e)}")
            # í´ë°±: ê¸°ì¡´ ìƒ˜í”Œë§ ë°©ì‹ ì‚¬ìš©
            level_counts = {
                "ERROR": db_stats["error_count"],
                "WARN": db_stats["warn_count"],
                "INFO": db_stats["info_count"]
            }
            log_samples = await _get_stratified_log_samples(project_uuid, time_hours, sample_size, level_counts)
            sample_metadata = None  # ê¸°ì¡´ ë°©ì‹ì€ ë©”íƒ€ë°ì´í„° ì—†ìŒ
            logger.info(f"âœ… í´ë°± ìƒ˜í”Œë§ ì™„ë£Œ: sample_count={len(log_samples)}")

        if not log_samples:
            error_msg = f"ë¡œê·¸ ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: project_uuid={project_uuid}"
            if sample_metadata:
                error_detail = sample_metadata.get("error", "Unknown error")
                level_counts = sample_metadata.get("level_counts", {})
                logger.error(
                    f"ğŸ”´ {error_msg}, error={error_detail}, level_counts={level_counts}"
                )

                # ì§„ë‹¨ ë©”ì‹œì§€ ìƒì„±
                if not level_counts or sum(level_counts.values()) == 0:
                    detail = f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."
                else:
                    detail = f"ë¡œê·¸ëŠ” ì¡´ì¬í•˜ì§€ë§Œ (ERROR: {level_counts.get('ERROR', 0)}, " \
                             f"WARN: {level_counts.get('WARN', 0)}, INFO: {level_counts.get('INFO', 0)}), " \
                             f"Vector ìƒ˜í”Œë§ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            else:
                logger.error(f"ğŸ”´ {error_msg}")
                detail = f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            raise HTTPException(
                status_code=500,
                detail=detail
            )

        # 3. LLM ê¸°ë°˜ í†µê³„ ì¶”ë¡  (ìƒ˜í”Œ + IPW ê°€ì¤‘ì¹˜ ë©”íƒ€ë°ì´í„°)
        logger.debug(f"3ë‹¨ê³„: LLM í†µê³„ ì¶”ë¡  ì‹œì‘ (IPW ê°€ì¤‘ì¹˜ í¬í•¨)")
        ai_stats = _llm_estimate_statistics(
            log_samples,
            db_stats["total_logs"],
            time_hours,
            None,  # Inference Mode: DB íŒíŠ¸ ì—†ìŒ
            sample_metadata  # IPW ê°€ì¤‘ì¹˜ ë©”íƒ€ë°ì´í„° ì „ë‹¬
        )
        logger.info(f"âœ… LLM ì¶”ë¡  ì™„ë£Œ: estimated_total={ai_stats.get('estimated_total_logs', 0)}, confidence={ai_stats.get('confidence_score', 0)}")

        # 4. ì •í™•ë„ ê³„ì‚°
        logger.debug(f"4ë‹¨ê³„: ì •í™•ë„ ê³„ì‚° ì‹œì‘")
        accuracy_metrics = _calculate_accuracy(db_stats, ai_stats)
        logger.info(f"âœ… ì •í™•ë„ ê³„ì‚° ì™„ë£Œ: overall_accuracy={accuracy_metrics.get('overall_accuracy', 0)}%")

        # 5. ê²€ì¦ ê²°ë¡  ìƒì„±
        overall = accuracy_metrics["overall_accuracy"]
        if overall >= 95:
            grade = "ë§¤ìš° ìš°ìˆ˜"
            can_replace = True
            explanation = "ì˜¤ì°¨ìœ¨ 5% ë¯¸ë§Œìœ¼ë¡œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‹ ë¢°ì„± ìˆê²Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
            recommendations = [
                "í”„ë¡œë•ì…˜ í™˜ê²½ ì ìš© ê¶Œì¥",
                "ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ AI ê¸°ë°˜ ë¶„ì„ ë„ì… ê°€ëŠ¥",
                "DB ë¶€í•˜ ê°ì†Œë¥¼ ìœ„í•œ AI ìºì‹± ë ˆì´ì–´ êµ¬ì¶•"
            ]
        elif overall >= 90:
            grade = "ìš°ìˆ˜"
            can_replace = True
            explanation = "ì˜¤ì°¨ìœ¨ 10% ë¯¸ë§Œìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ë¶„ì„ ì—…ë¬´ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
            recommendations = [
                "ë³´ì¡° ë¶„ì„ ë„êµ¬ë¡œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥",
                "ë¹„ì‹¤ì‹œê°„ ë¦¬í¬íŠ¸ ìƒì„±ì— AI í™œìš©",
                "ì¶”ê°€ í”„ë¡¬í”„íŠ¸ íŠœë‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ ê°€ëŠ¥"
            ]
        elif overall >= 80:
            grade = "ì–‘í˜¸"
            can_replace = False
            explanation = "ì˜¤ì°¨ìœ¨ 20% ë¯¸ë§Œìœ¼ë¡œ íŠ¸ë Œë“œ ë¶„ì„ê³¼ ì´ìƒ íƒì§€ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
            recommendations = [
                "íŠ¸ë Œë“œ ë¶„ì„ ë³´ì¡° ë„êµ¬ë¡œ í™œìš©",
                "ì´ìƒ íƒì§€ ì•Œë¦¼ì— AI ì¶”ë¡  ê²°í•©",
                "ìƒ˜í”Œ í¬ê¸° ì¦ê°€ë¡œ ì •í™•ë„ í–¥ìƒ"
            ]
        elif overall >= 70:
            grade = "ë³´í†µ"
            can_replace = False
            explanation = "ì˜¤ì°¨ìœ¨ì´ ë†’ì•„ ì¶”ê°€ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤."
            recommendations = [
                "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„  í•„ìš”",
                "ìƒ˜í”Œë§ ì „ëµ ì¬ê²€í† ",
                "Few-shot ì˜ˆì œ ì¶”ê°€ ê³ ë ¤"
            ]
        else:
            grade = "ë¯¸í¡"
            can_replace = False
            explanation = "ì •í™•ë„ê°€ ë‚®ì•„ ê·¼ë³¸ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
            recommendations = [
                "LLM ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ ê³ ë ¤",
                "ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ ê°œì„ ",
                "RAG íŒ¨í„´ ë„ì… ê²€í† "
            ]

        verdict = ComparisonVerdict(
            grade=grade,
            can_replace_db=can_replace,
            explanation=explanation,
            recommendations=recommendations
        )

        # 6. ê¸°ìˆ ì  ì–´í•„ í¬ì¸íŠ¸
        technical_highlights = [
            f"Temperature 0.1ë¡œ ì¼ê´€ëœ ì¶”ë¡  (ê¸°ë³¸ê°’ ëŒ€ë¹„ 7ë°° ë‚®ìŒ)",
            f"ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ í¬ì†Œ ì´ë²¤íŠ¸(ERROR/WARN) í¬ì°© ë³´ì¥",
            f"ìƒ˜í”Œ {len(log_samples)}ê°œë¡œ {db_stats['total_logs']:,}ê°œ í†µê³„ ì¶”ë¡ ",
            f"ì¢…í•© ì •í™•ë„ {overall:.1f}% ë‹¬ì„±",
            "Structured Outputìœ¼ë¡œ JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ",
            "ìë™í™”ëœ ë‹¤ì°¨ì› ì •í™•ë„ ê²€ì¦",
            "MCP/ë©€í‹°ëª¨ë‹¬ ì—†ì´ ë‹¨ì¼ LLMìœ¼ë¡œ êµ¬í˜„"
        ]

        # 7. ì‘ë‹µ êµ¬ì„±
        return AIvsDBComparisonResponse(
            project_uuid=project_uuid,
            analysis_period_hours=time_hours,
            sample_size=len(log_samples),
            analyzed_at=datetime.utcnow(),
            db_statistics=DBStatistics(
                total_logs=db_stats["total_logs"],
                error_count=db_stats["error_count"],
                warn_count=db_stats["warn_count"],
                info_count=db_stats["info_count"],
                error_rate=db_stats["error_rate"],
                peak_hour=db_stats["peak_hour"],
                peak_count=db_stats["peak_count"]
            ),
            ai_statistics=AIStatistics(
                estimated_total_logs=ai_stats.get("estimated_total_logs", 0),
                estimated_error_count=ai_stats.get("estimated_error_count", 0),
                estimated_warn_count=ai_stats.get("estimated_warn_count", 0),
                estimated_info_count=ai_stats.get("estimated_info_count", 0),
                estimated_error_rate=ai_stats.get("estimated_error_rate", 0.0),
                confidence_score=ai_stats.get("confidence_score", 0),
                reasoning=ai_stats.get("reasoning", "N/A")
            ),
            accuracy_metrics=AccuracyMetrics(
                total_logs_accuracy=accuracy_metrics["total_logs_accuracy"],
                error_count_accuracy=accuracy_metrics["error_count_accuracy"],
                warn_count_accuracy=accuracy_metrics["warn_count_accuracy"],
                info_count_accuracy=accuracy_metrics["info_count_accuracy"],
                error_rate_accuracy=accuracy_metrics["error_rate_accuracy"],
                overall_accuracy=accuracy_metrics["overall_accuracy"],
                ai_confidence=accuracy_metrics["ai_confidence"]
            ),
            verdict=verdict,
            technical_highlights=technical_highlights
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ”´ AI vs DB í†µê³„ ë¹„êµ ì¤‘ ì˜ˆì™¸ ë°œìƒ: project_uuid={project_uuid}, error={str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"AI vs DB í†µê³„ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@router.get("/statistics/hourly", response_model=HourlyStatisticsResponse)
async def get_hourly_statistics(
    project_uuid: str = Query(..., description="í”„ë¡œì íŠ¸ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="ë¶„ì„ ê¸°ê°„ (ì‹œê°„)")
) -> HourlyStatisticsResponse:
    """
    ì‹œê°„ëŒ€ë³„ ë¡œê·¸ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    1ì‹œê°„ ë‹¨ìœ„ë¡œ ë¡œê·¸ ë¶„í¬ë¥¼ í™•ì¸í•˜ì—¬ íŠ¸ë Œë“œì™€ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    try:
        db_stats = _get_db_statistics(project_uuid, time_hours)

        if db_stats["total_logs"] == 0:
            raise HTTPException(
                status_code=404,
                detail=f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        hourly_data = [
            HourlyDataPoint(
                hour=h["hour"],
                total=h["total"],
                error=h["error"],
                warn=h["warn"],
                info=h["info"]
            )
            for h in db_stats["hourly_data"]
        ]

        return HourlyStatisticsResponse(
            project_uuid=project_uuid,
            analysis_period_hours=time_hours,
            total_logs=db_stats["total_logs"],
            error_rate=db_stats["error_rate"],
            peak_hour=db_stats["peak_hour"],
            peak_count=db_stats["peak_count"],
            hourly_data=hourly_data
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì‹œê°„ëŒ€ë³„ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@router.get("/statistics/compare-errors", response_model=ErrorComparisonResponse)
async def compare_error_statistics(
    project_uuid: str = Query(..., description="í”„ë¡œì íŠ¸ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="ë¶„ì„ ê¸°ê°„ (ì‹œê°„, ê¸°ë³¸ 24ì‹œê°„)"),
    sample_size: int = Query(100, ge=10, le=500, description="AI ë¶„ì„ìš© ìƒ˜í”Œ í¬ê¸°")
) -> ErrorComparisonResponse:
    """
    ERROR ë¡œê·¸ ì „ìš© DB vs AI í†µê³„ ë¹„êµ

    Vector KNN ìƒ˜í”Œë§ì„ í™œìš©í•˜ì—¬ ìœ ì‚¬í•œ ERROR íŒ¨í„´ì„ ê·¸ë£¹í•‘í•˜ê³ ,
    LLMìœ¼ë¡œ ERROR í†µê³„ë¥¼ ì¶”ì •í•˜ì—¬ ì •í™•ë„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

    **íŠ¹ì§•:**
    - ERROR ë¡œê·¸ë§Œ ì§‘ì¤‘ ë¶„ì„
    - Vector KNNìœ¼ë¡œ ìœ ì‚¬ ERROR ê·¸ë£¹í•‘
    - log_vector í•„ë“œ í™œìš© (ë²¡í„°í™”ëœ ERRORë§Œ)
    - ë²¡í„°í™”ìœ¨ 50% ë¯¸ë§Œ ì‹œ ìë™ ëœë¤ ìƒ˜í”Œë§ í´ë°±

    **ì •í™•ë„ ì§€í‘œ:**
    - ERROR ê°œìˆ˜ ì •í™•ë„
    - ERROR ë¹„ìœ¨ ì •í™•ë„
    - ì¢…í•© ì •í™•ë„ (ê°€ì¤‘ í‰ê· )
    """
    logger.info(f"ERROR ë¹„êµ ì‹œì‘: project={project_uuid}, hours={time_hours}, sample_size={sample_size}")

    try:
        # Step 1: DBì—ì„œ ERROR í†µê³„ ì¡°íšŒ
        db_stats = await _get_db_error_statistics(project_uuid, time_hours)
        logger.info(f"âœ… DB ERROR í†µê³„: total_errors={db_stats['total_errors']}, rate={db_stats['error_rate']}%")

        if db_stats["total_errors"] == 0:
            logger.warning(f"âš ï¸ ERROR ë¡œê·¸ ì—†ìŒ: project_uuid={project_uuid}")
            raise HTTPException(
                status_code=404,
                detail=f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ERROR ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        # Step 2: Vector ìƒ˜í”Œë§ (ERRORë§Œ)
        error_samples, vector_info = await _sample_errors_with_vector(
            project_uuid, time_hours, sample_size
        )
        logger.info(
            f"âœ… ERROR ìƒ˜í”Œë§ ì™„ë£Œ: count={len(error_samples)}, "
            f"method={vector_info['sampling_method']}, "
            f"vectorization_rate={vector_info['vectorization_rate']}%"
        )

        if not error_samples:
            raise HTTPException(
                status_code=500,
                detail="ERROR ë¡œê·¸ ìƒ˜í”Œë§ ì‹¤íŒ¨"
            )

        # Step 3: LLMìœ¼ë¡œ ERROR ì¶”ì •
        ai_stats = _llm_estimate_error_statistics(
            error_samples,
            db_stats["total_logs"],
            time_hours
        )
        logger.info(
            f"âœ… AI ERROR ì¶”ì •: estimated_errors={ai_stats['estimated_total_errors']}, "
            f"confidence={ai_stats['confidence_score']}"
        )

        # Step 4: ì •í™•ë„ ê³„ì‚°
        accuracy = _calculate_error_accuracy(db_stats, ai_stats)
        logger.info(f"âœ… ì •í™•ë„ ê³„ì‚°: overall={accuracy['overall_accuracy']:.1f}%")

        # Step 5: ì‘ë‹µ êµ¬ì„±
        response = ErrorComparisonResponse(
            project_uuid=project_uuid,
            analysis_period_hours=time_hours,
            sample_size=len(error_samples),
            analyzed_at=datetime.utcnow(),
            db_error_stats=DBErrorStats(
                total_errors=db_stats["total_errors"],
                error_rate=db_stats["error_rate"],
                peak_error_hour=db_stats["peak_error_hour"],
                peak_error_count=db_stats["peak_error_count"]
            ),
            ai_error_stats=AIErrorStats(
                estimated_total_errors=ai_stats["estimated_total_errors"],
                estimated_error_rate=ai_stats["estimated_error_rate"],
                confidence_score=ai_stats["confidence_score"],
                reasoning=ai_stats["reasoning"]
            ),
            accuracy_metrics=ErrorAccuracyMetrics(
                error_count_accuracy=accuracy["error_count_accuracy"],
                error_rate_accuracy=accuracy["error_rate_accuracy"],
                overall_accuracy=accuracy["overall_accuracy"]
            ),
            vector_analysis=VectorGroupingInfo(
                vectorized_error_count=vector_info["vectorized_error_count"],
                vectorization_rate=vector_info["vectorization_rate"],
                sampling_method=vector_info["sampling_method"],
                sample_distribution=vector_info["sample_distribution"]
            ) if vector_info else None
        )

        logger.info(f"âœ… ERROR ë¹„êµ ì™„ë£Œ: accuracy={accuracy['overall_accuracy']:.1f}%")
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ”´ ERROR ë¹„êµ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ERROR ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )
