"""
V2 LangGraph API - Statistics Comparison Endpoints

AI vs DB ÌÜµÍ≥Ñ ÎπÑÍµêÎ•º ÌÜµÌïú LLM Ïó≠Îüâ Í≤ÄÏ¶ù API
"""

import logging
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime

from app.tools.statistics_comparison_tools import (
    _get_db_statistics,
    _get_log_samples,
    _get_stratified_log_samples,
    _llm_estimate_statistics,
    _calculate_accuracy,
    _get_db_error_statistics,
    _sample_errors_with_vector,
    _vector_estimate_error_count,
    _calculate_error_accuracy
)
from app.tools.sampling_strategies import sample_two_stage_rare_aware

logger = logging.getLogger(__name__)


router = APIRouter(prefix="/v2-langgraph", tags=["Statistics Comparison"])


# Response Models
class DBStatistics(BaseModel):
    """DB ÏßÅÏ†ë Ï°∞Ìöå ÌÜµÍ≥Ñ"""
    total_logs: int = Field(..., description="Ï¥ù Î°úÍ∑∏ Ïàò")
    error_count: int = Field(..., description="ERROR Î°úÍ∑∏ Ïàò")
    warn_count: int = Field(..., description="WARN Î°úÍ∑∏ Ïàò")
    info_count: int = Field(..., description="INFO Î°úÍ∑∏ Ïàò")
    error_rate: float = Field(..., description="ÏóêÎü¨Ïú® (%)")
    peak_hour: str = Field(..., description="ÌîºÌÅ¨ ÏãúÍ∞Ñ")
    peak_count: int = Field(..., description="ÌîºÌÅ¨ ÏãúÍ∞Ñ Î°úÍ∑∏ Ïàò")


class AIStatistics(BaseModel):
    """AI(LLM) Ï∂îÎ°† ÌÜµÍ≥Ñ"""
    estimated_total_logs: int = Field(..., description="Ï∂îÎ°†Ìïú Ï¥ù Î°úÍ∑∏ Ïàò")
    estimated_error_count: int = Field(..., description="Ï∂îÎ°†Ìïú ERROR Î°úÍ∑∏ Ïàò")
    estimated_warn_count: int = Field(..., description="Ï∂îÎ°†Ìïú WARN Î°úÍ∑∏ Ïàò")
    estimated_info_count: int = Field(..., description="Ï∂îÎ°†Ìïú INFO Î°úÍ∑∏ Ïàò")
    estimated_error_rate: float = Field(..., description="Ï∂îÎ°†Ìïú ÏóêÎü¨Ïú® (%)")
    confidence_score: int = Field(..., description="AI Ïã†Î¢∞ÎèÑ (0-100)")
    reasoning: str = Field(..., description="Ï∂îÎ°† Í∑ºÍ±∞")


class AccuracyMetrics(BaseModel):
    """Ï†ïÌôïÎèÑ ÏßÄÌëú"""
    total_logs_accuracy: float = Field(..., description="Ï¥ù Î°úÍ∑∏ Ïàò ÏùºÏπòÏú® (%)")
    error_count_accuracy: float = Field(..., description="ERROR Ïàò ÏùºÏπòÏú® (%)")
    warn_count_accuracy: float = Field(..., description="WARN Ïàò ÏùºÏπòÏú® (%)")
    info_count_accuracy: float = Field(..., description="INFO Ïàò ÏùºÏπòÏú® (%)")
    error_rate_accuracy: float = Field(..., description="ÏóêÎü¨Ïú® Ï†ïÌôïÎèÑ (%)")
    overall_accuracy: float = Field(..., description="Ï¢ÖÌï© Ï†ïÌôïÎèÑ (%)")
    ai_confidence: int = Field(..., description="AI ÏûêÏ≤¥ Ïã†Î¢∞ÎèÑ")


class ComparisonVerdict(BaseModel):
    """Í≤ÄÏ¶ù Í≤∞Î°†"""
    grade: str = Field(..., description="Îì±Í∏â (Îß§Ïö∞ Ïö∞Ïàò/Ïö∞Ïàò/ÏñëÌò∏/Î≥¥ÌÜµ/ÎØ∏Ìù°)")
    can_replace_db: bool = Field(..., description="DB ÎåÄÏ≤¥ Í∞ÄÎä• Ïó¨Î∂Ä")
    explanation: str = Field(..., description="ÏÑ§Î™Ö")
    recommendations: List[str] = Field(..., description="Í∂åÏû• ÏÇ¨Ìï≠")


class AIvsDBComparisonResponse(BaseModel):
    """AI vs DB ÌÜµÍ≥Ñ ÎπÑÍµê ÏùëÎãµ"""
    project_uuid: str = Field(..., description="ÌîÑÎ°úÏ†ùÌä∏ UUID")
    analysis_period_hours: int = Field(..., description="Î∂ÑÏÑù Í∏∞Í∞Ñ (ÏãúÍ∞Ñ)")
    sample_size: int = Field(..., description="ÏÇ¨Ïö©Îêú ÏÉòÌîå ÌÅ¨Í∏∞")
    analyzed_at: datetime = Field(..., description="Î∂ÑÏÑù ÏãúÏ†ê")

    db_statistics: DBStatistics = Field(..., description="DB ÏßÅÏ†ë Ï°∞Ìöå Í≤∞Í≥º")
    ai_statistics: AIStatistics = Field(..., description="AI Ï∂îÎ°† Í≤∞Í≥º")
    accuracy_metrics: AccuracyMetrics = Field(..., description="Ï†ïÌôïÎèÑ ÏßÄÌëú")
    verdict: ComparisonVerdict = Field(..., description="Í≤ÄÏ¶ù Í≤∞Î°†")

    technical_highlights: List[str] = Field(..., description="Í∏∞Ïà†Ï†Å Ïñ¥ÌïÑ Ìè¨Ïù∏Ìä∏")


# ERROR ÎπÑÍµê Ï†ÑÏö© Response Models
class DBErrorStats(BaseModel):
    """DBÏóêÏÑú Ï°∞ÌöåÌïú ERROR ÌÜµÍ≥Ñ"""
    total_errors: int = Field(..., description="ERROR Î°úÍ∑∏ Ïàò")
    error_rate: float = Field(..., description="ERROR ÎπÑÏú® (%)")
    peak_error_hour: Optional[str] = Field(None, description="ERROR ÏµúÎã§ Î∞úÏÉù ÏãúÍ∞Ñ")
    peak_error_count: Optional[int] = Field(None, description="ÏµúÎã§ ÏãúÍ∞Ñ ERROR Ïàò")


class AIErrorStats(BaseModel):
    """AI Ï∂îÏ†ï ERROR ÌÜµÍ≥Ñ"""
    estimated_total_errors: int = Field(..., description="Ï∂îÏ†ï ERROR Î°úÍ∑∏ Ïàò")
    estimated_error_rate: float = Field(..., description="Ï∂îÏ†ï ERROR ÎπÑÏú® (%)")
    confidence_score: int = Field(..., description="AI Ïã†Î¢∞ÎèÑ (0-100)")
    reasoning: str = Field(..., description="Ï∂îÎ°† Í∑ºÍ±∞")


class ErrorAccuracyMetrics(BaseModel):
    """ERROR Ï†ïÌôïÎèÑ Î©îÌä∏Î¶≠"""
    error_count_accuracy: float = Field(..., description="ERROR Ïàò ÏùºÏπòÏú® (%)")
    error_rate_accuracy: float = Field(..., description="ERROR ÎπÑÏú® Ï†ïÌôïÎèÑ (%)")
    overall_accuracy: float = Field(..., description="Ï¢ÖÌï© Ï†ïÌôïÎèÑ (%)")


class VectorGroupingInfo(BaseModel):
    """Vector KNN Í∑∏Î£πÌïë Ï†ïÎ≥¥"""
    vectorized_error_count: int = Field(..., description="log_vector ÏûàÎäî ERROR Í∞úÏàò")
    vectorization_rate: float = Field(..., description="Î≤°ÌÑ∞ÌôîÏú® (%)")
    sampling_method: str = Field(..., description="ÏÉòÌîåÎßÅ Î∞©Î≤ï (vector_knn ÎòêÎäî random_fallback)")
    sample_distribution: str = Field(..., description="ÏÉòÌîå Î∂ÑÌè¨ ÏÑ§Î™Ö")


class ErrorComparisonResponse(BaseModel):
    """ERROR Î°úÍ∑∏ ÎπÑÍµê Í≤∞Í≥º"""
    project_uuid: str = Field(..., description="ÌîÑÎ°úÏ†ùÌä∏ UUID")
    analysis_period_hours: int = Field(..., description="Î∂ÑÏÑù Í∏∞Í∞Ñ (ÏãúÍ∞Ñ)")
    sample_size: int = Field(..., description="ÏÇ¨Ïö©Îêú ÏÉòÌîå ÌÅ¨Í∏∞")
    analyzed_at: datetime = Field(..., description="Î∂ÑÏÑù ÏãúÏ†ê")
    db_error_stats: DBErrorStats = Field(..., description="DB ERROR ÌÜµÍ≥Ñ")
    ai_error_stats: AIErrorStats = Field(..., description="AI ERROR Ï∂îÏ†ï")
    accuracy_metrics: ErrorAccuracyMetrics = Field(..., description="Ï†ïÌôïÎèÑ ÏßÄÌëú")
    vector_analysis: Optional[VectorGroupingInfo] = Field(None, description="Vector ÏÉòÌîåÎßÅ Ï†ïÎ≥¥")


class HourlyDataPoint(BaseModel):
    """ÏãúÍ∞ÑÎåÄÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏"""
    hour: str = Field(..., description="ÏãúÍ∞Ñ (ISO format)")
    total: int = Field(..., description="Ï¥ù Î°úÍ∑∏ Ïàò")
    error: int = Field(..., description="ERROR Ïàò")
    warn: int = Field(..., description="WARN Ïàò")
    info: int = Field(..., description="INFO Ïàò")


class HourlyStatisticsResponse(BaseModel):
    """ÏãúÍ∞ÑÎåÄÎ≥Ñ ÌÜµÍ≥Ñ ÏùëÎãµ"""
    project_uuid: str = Field(..., description="ÌîÑÎ°úÏ†ùÌä∏ UUID")
    analysis_period_hours: int = Field(..., description="Î∂ÑÏÑù Í∏∞Í∞Ñ")
    total_logs: int = Field(..., description="Ï¥ù Î°úÍ∑∏ Ïàò")
    error_rate: float = Field(..., description="ÏóêÎü¨Ïú® (%)")
    peak_hour: str = Field(..., description="ÌîºÌÅ¨ ÏãúÍ∞Ñ")
    peak_count: int = Field(..., description="ÌîºÌÅ¨ ÏãúÍ∞Ñ Î°úÍ∑∏ Ïàò")
    hourly_data: List[HourlyDataPoint] = Field(..., description="ÏãúÍ∞ÑÎåÄÎ≥Ñ Îç∞Ïù¥ÌÑ∞")


@router.get("/statistics/compare", response_model=AIvsDBComparisonResponse)
async def compare_ai_vs_db(
    project_uuid: str = Query(..., description="ÌîÑÎ°úÏ†ùÌä∏ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="Î∂ÑÏÑù Í∏∞Í∞Ñ (ÏãúÍ∞Ñ, Í∏∞Î≥∏ 24ÏãúÍ∞Ñ)"),
    sample_size: int = Query(100, ge=10, le=500, description="AI Î∂ÑÏÑùÏö© ÏÉòÌîå ÌÅ¨Í∏∞")
) -> AIvsDBComparisonResponse:
    """
    AI vs DB ÌÜµÍ≥Ñ ÎπÑÍµêÎ•º ÏàòÌñâÌïòÏó¨ LLMÏùò DB ÎåÄÏ≤¥ Ïó≠ÎüâÏùÑ Í≤ÄÏ¶ùÌï©ÎãàÎã§.

    **Í≤ÄÏ¶ù Î™©Ìëú:**
    - LLMÏù¥ Î°úÍ∑∏ ÏÉòÌîåÎßåÏúºÎ°ú Ï†ÑÏ≤¥ ÌÜµÍ≥ÑÎ•º Ï†ïÌôïÌûà Ï∂îÎ°†Ìï† Ïàò ÏûàÎäîÍ∞Ä?
    - AIÍ∞Ä ÏÇ¨Îûå(ÎòêÎäî DB ÏøºÎ¶¨)ÏùÑ ÎåÄÏ≤¥Ìï† Ïàò ÏûàÎäî Ïó≠ÎüâÏùÑ Î≥¥Ïú†ÌñàÎäîÍ∞Ä?

    **Í∏∞Ïà†Ï†Å Ïñ¥ÌïÑ Ìè¨Ïù∏Ìä∏:**
    - Temperature 0.1Î°ú ÏùºÍ¥ÄÎêú Ï∂îÎ°† Î≥¥Ïû•
    - Structured OutputÏúºÎ°ú ÌòïÏãù Ïò§Î•ò Î∞©ÏßÄ
    - ÏûêÎèôÌôîÎêú Ï†ïÌôïÎèÑ Ï∏°Ï†ïÏúºÎ°ú Ïã†Î¢∞ÏÑ± Í≤ÄÏ¶ù

    **Î∞òÌôò Îç∞Ïù¥ÌÑ∞:**
    - DB ÏßÅÏ†ë Ï°∞Ìöå Í≤∞Í≥º (Ground Truth)
    - AI Ï∂îÎ°† Í≤∞Í≥º
    - Ï†ïÌôïÎèÑ ÏßÄÌëú (Ïò§Ï∞®Ïú®)
    - Í≤ÄÏ¶ù Í≤∞Î°† (DB ÎåÄÏ≤¥ Í∞ÄÎä• Ïó¨Î∂Ä)

    **Ï†ïÌôïÎèÑ Í∏∞Ï§Ä:**
    - 95% Ïù¥ÏÉÅ: Îß§Ïö∞ Ïö∞Ïàò (ÌîÑÎ°úÎçïÏÖò ÏÇ¨Ïö© Í∞ÄÎä•)
    - 90% Ïù¥ÏÉÅ: Ïö∞Ïàò (ÎåÄÎ∂ÄÎ∂Ñ ÏóÖÎ¨¥Ïóê ÌôúÏö© Í∞ÄÎä•)
    - 80% Ïù¥ÏÉÅ: ÏñëÌò∏ (Î≥¥Ï°∞ ÎèÑÍµ¨Î°ú Ïú†Ïö©)
    - 70% Ïù¥ÏÉÅ: Î≥¥ÌÜµ (Í∞úÏÑ† ÌïÑÏöî)
    - 70% ÎØ∏Îßå: ÎØ∏Ìù° (Ïû¨Í≤ÄÌÜ† ÌïÑÏöî)
    """
    logger.info(f"ü§ñ AI vs DB ÌÜµÍ≥Ñ ÎπÑÍµê ÏãúÏûë: project_uuid={project_uuid}, time_hours={time_hours}, sample_size={sample_size}")

    try:
        # 1. DBÏóêÏÑú ÏßÅÏ†ë ÌÜµÍ≥Ñ Ï°∞Ìöå
        logger.debug(f"1Îã®Í≥Ñ: DB ÌÜµÍ≥Ñ Ï°∞Ìöå ÏãúÏûë")
        db_stats = _get_db_statistics(project_uuid, time_hours)
        logger.info(f"‚úÖ DB ÌÜµÍ≥Ñ Ï°∞Ìöå ÏôÑÎ£å: total_logs={db_stats.get('total_logs', 0)}")

        if db_stats["total_logs"] == 0:
            logger.warning(f"‚ö†Ô∏è Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå: project_uuid={project_uuid}, time_hours={time_hours}")
            raise HTTPException(
                status_code=404,
                detail=f"ÏµúÍ∑º {time_hours}ÏãúÍ∞Ñ ÎèôÏïà Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§."
            )

        # 2. Î°úÍ∑∏ ÏÉòÌîå Ï∂îÏ∂ú (2Îã®Í≥Ñ Ìù¨ÏÜå Ïù¥Î≤§Ìä∏ Ïù∏Ïãù ÏÉòÌîåÎßÅ + IPW Í∞ÄÏ§ëÏπò)
        logger.debug(f"2Îã®Í≥Ñ: 2Îã®Í≥Ñ Ìù¨ÏÜå Ïù¥Î≤§Ìä∏ Ïù∏Ïãù ÏÉòÌîåÎßÅ ÏãúÏûë")
        sample_metadata = None
        try:
            log_samples, sample_metadata = await sample_two_stage_rare_aware(
                project_uuid=project_uuid,
                total_k=sample_size,
                time_hours=time_hours,
                rare_threshold=100  # 100Í∞ú ÎØ∏ÎßåÏù¥Î©¥ Ìù¨ÏÜå Ïù¥Î≤§Ìä∏Î°ú Í∞ÑÏ£º
            )
            sampling_method = sample_metadata.get('sampling_method', 'unknown')
            logger.info(
                f"‚úÖ 2Îã®Í≥Ñ ÏÉòÌîåÎßÅ ÏôÑÎ£å: sample_count={len(log_samples)}, "
                f"method={sampling_method}, "
                f"weights={sample_metadata.get('weights', {})}, "
                f"rare_levels={sample_metadata.get('rare_levels', [])}"
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è 2Îã®Í≥Ñ ÏÉòÌîåÎßÅ Ïã§Ìå®, Í∏∞Ï°¥ Î∞©ÏãùÏúºÎ°ú Ìè¥Î∞±: {type(e).__name__}: {str(e)}")
            # Ìè¥Î∞±: Í∏∞Ï°¥ ÏÉòÌîåÎßÅ Î∞©Ïãù ÏÇ¨Ïö©
            level_counts = {
                "ERROR": db_stats["error_count"],
                "WARN": db_stats["warn_count"],
                "INFO": db_stats["info_count"]
            }
            log_samples = await _get_stratified_log_samples(project_uuid, time_hours, sample_size, level_counts)
            sample_metadata = None  # Í∏∞Ï°¥ Î∞©ÏãùÏùÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÜÏùå
            logger.info(f"‚úÖ Ìè¥Î∞± ÏÉòÌîåÎßÅ ÏôÑÎ£å: sample_count={len(log_samples)}")

        if not log_samples:
            error_msg = f"Î°úÍ∑∏ ÏÉòÌîå Ï∂îÏ∂ú Ïã§Ìå®: project_uuid={project_uuid}"
            if sample_metadata:
                error_detail = sample_metadata.get("error", "Unknown error")
                level_counts = sample_metadata.get("level_counts", {})
                logger.error(
                    f"üî¥ {error_msg}, error={error_detail}, level_counts={level_counts}"
                )

                # ÏßÑÎã® Î©îÏãúÏßÄ ÏÉùÏÑ±
                if not level_counts or sum(level_counts.values()) == 0:
                    detail = f"ÏµúÍ∑º {time_hours}ÏãúÍ∞Ñ ÎèôÏïà Î°úÍ∑∏Í∞Ä ÏóÜÏäµÎãàÎã§."
                else:
                    detail = f"Î°úÍ∑∏Îäî Ï°¥Ïû¨ÌïòÏßÄÎßå (ERROR: {level_counts.get('ERROR', 0)}, " \
                             f"WARN: {level_counts.get('WARN', 0)}, INFO: {level_counts.get('INFO', 0)}), " \
                             f"Vector ÏÉòÌîåÎßÅÏù¥ Ïã§Ìå®ÌñàÏäµÎãàÎã§."
            else:
                logger.error(f"üî¥ {error_msg}")
                detail = f"ÏµúÍ∑º {time_hours}ÏãúÍ∞Ñ ÎèôÏïà Î°úÍ∑∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            raise HTTPException(
                status_code=500,
                detail=detail
            )

        # 3. LLM Í∏∞Î∞ò ÌÜµÍ≥Ñ Ï∂îÎ°† (ÏÉòÌîå + IPW Í∞ÄÏ§ëÏπò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞)
        logger.debug(f"3Îã®Í≥Ñ: LLM ÌÜµÍ≥Ñ Ï∂îÎ°† ÏãúÏûë (IPW Í∞ÄÏ§ëÏπò Ìè¨Ìï®)")
        ai_stats = _llm_estimate_statistics(
            log_samples,
            db_stats["total_logs"],
            time_hours,
            None,  # Inference Mode: DB ÌûåÌä∏ ÏóÜÏùå
            sample_metadata  # IPW Í∞ÄÏ§ëÏπò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨
        )
        logger.info(f"‚úÖ LLM Ï∂îÎ°† ÏôÑÎ£å: estimated_total={ai_stats.get('estimated_total_logs', 0)}, confidence={ai_stats.get('confidence_score', 0)}")

        # 4. Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
        logger.debug(f"4Îã®Í≥Ñ: Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞ ÏãúÏûë")
        accuracy_metrics = _calculate_accuracy(db_stats, ai_stats)
        logger.info(f"‚úÖ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞ ÏôÑÎ£å: overall_accuracy={accuracy_metrics.get('overall_accuracy', 0)}%")

        # 5. Í≤ÄÏ¶ù Í≤∞Î°† ÏÉùÏÑ±
        overall = accuracy_metrics["overall_accuracy"]
        if overall >= 95:
            grade = "Îß§Ïö∞ Ïö∞Ïàò"
            can_replace = True
            explanation = "Ïò§Ï∞®Ïú® 5% ÎØ∏ÎßåÏúºÎ°ú ÌîÑÎ°úÎçïÏÖò ÌôòÍ≤ΩÏóêÏÑú Ïã†Î¢∞ÏÑ± ÏûàÍ≤å ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§."
            recommendations = [
                "ÌîÑÎ°úÎçïÏÖò ÌôòÍ≤Ω Ï†ÅÏö© Í∂åÏû•",
                "Ïã§ÏãúÍ∞Ñ ÎåÄÏãúÎ≥¥Îìú AI Í∏∞Î∞ò Î∂ÑÏÑù ÎèÑÏûÖ Í∞ÄÎä•",
                "DB Î∂ÄÌïò Í∞êÏÜåÎ•º ÏúÑÌïú AI Ï∫êÏã± Î†àÏù¥Ïñ¥ Íµ¨Ï∂ï"
            ]
        elif overall >= 90:
            grade = "Ïö∞Ïàò"
            can_replace = True
            explanation = "Ïò§Ï∞®Ïú® 10% ÎØ∏ÎßåÏúºÎ°ú ÎåÄÎ∂ÄÎ∂ÑÏùò Î∂ÑÏÑù ÏóÖÎ¨¥Ïóê ÌôúÏö© Í∞ÄÎä•Ìï©ÎãàÎã§."
            recommendations = [
                "Î≥¥Ï°∞ Î∂ÑÏÑù ÎèÑÍµ¨Î°ú Ï¶âÏãú ÌôúÏö© Í∞ÄÎä•",
                "ÎπÑÏã§ÏãúÍ∞Ñ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±Ïóê AI ÌôúÏö©",
                "Ï∂îÍ∞Ä ÌîÑÎ°¨ÌîÑÌä∏ ÌäúÎãùÏúºÎ°ú Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ Í∞ÄÎä•"
            ]
        elif overall >= 80:
            grade = "ÏñëÌò∏"
            can_replace = False
            explanation = "Ïò§Ï∞®Ïú® 20% ÎØ∏ÎßåÏúºÎ°ú Ìä∏Î†åÎìú Î∂ÑÏÑùÍ≥º Ïù¥ÏÉÅ ÌÉêÏßÄÏóê ÌôúÏö© Í∞ÄÎä•Ìï©ÎãàÎã§."
            recommendations = [
                "Ìä∏Î†åÎìú Î∂ÑÏÑù Î≥¥Ï°∞ ÎèÑÍµ¨Î°ú ÌôúÏö©",
                "Ïù¥ÏÉÅ ÌÉêÏßÄ ÏïåÎ¶ºÏóê AI Ï∂îÎ°† Í≤∞Ìï©",
                "ÏÉòÌîå ÌÅ¨Í∏∞ Ï¶ùÍ∞ÄÎ°ú Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ"
            ]
        elif overall >= 70:
            grade = "Î≥¥ÌÜµ"
            can_replace = False
            explanation = "Ïò§Ï∞®Ïú®Ïù¥ ÎÜíÏïÑ Ï∂îÍ∞Ä ÌäúÎãùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§."
            recommendations = [
                "ÌîÑÎ°¨ÌîÑÌä∏ ÏóîÏßÄÎãàÏñ¥ÎßÅ Í∞úÏÑ† ÌïÑÏöî",
                "ÏÉòÌîåÎßÅ Ï†ÑÎûµ Ïû¨Í≤ÄÌÜ†",
                "Few-shot ÏòàÏ†ú Ï∂îÍ∞Ä Í≥†Î†§"
            ]
        else:
            grade = "ÎØ∏Ìù°"
            can_replace = False
            explanation = "Ï†ïÌôïÎèÑÍ∞Ä ÎÇÆÏïÑ Í∑ºÎ≥∏Ï†ÅÏù∏ Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§."
            recommendations = [
                "LLM Î™®Îç∏ ÏóÖÍ∑∏Î†àÏù¥Îìú Í≥†Î†§",
                "Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î°úÏßÅ Í∞úÏÑ†",
                "RAG Ìå®ÌÑ¥ ÎèÑÏûÖ Í≤ÄÌÜ†"
            ]

        verdict = ComparisonVerdict(
            grade=grade,
            can_replace_db=can_replace,
            explanation=explanation,
            recommendations=recommendations
        )

        # 6. Í∏∞Ïà†Ï†Å Ïñ¥ÌïÑ Ìè¨Ïù∏Ìä∏
        technical_highlights = [
            f"Temperature 0.1Î°ú ÏùºÍ¥ÄÎêú Ï∂îÎ°† (Í∏∞Î≥∏Í∞í ÎåÄÎπÑ 7Î∞∞ ÎÇÆÏùå)",
            f"Ï∏µÌôî ÏÉòÌîåÎßÅÏúºÎ°ú Ìù¨ÏÜå Ïù¥Î≤§Ìä∏(ERROR/WARN) Ìè¨Ï∞© Î≥¥Ïû•",
            f"ÏÉòÌîå {len(log_samples)}Í∞úÎ°ú {db_stats['total_logs']:,}Í∞ú ÌÜµÍ≥Ñ Ï∂îÎ°†",
            f"Ï¢ÖÌï© Ï†ïÌôïÎèÑ {overall:.1f}% Îã¨ÏÑ±",
            "Structured OutputÏúºÎ°ú JSON Ïä§ÌÇ§Îßà Í∞ïÏ†ú",
            "ÏûêÎèôÌôîÎêú Îã§Ï∞®Ïõê Ï†ïÌôïÎèÑ Í≤ÄÏ¶ù",
            "MCP/Î©ÄÌã∞Î™®Îã¨ ÏóÜÏù¥ Îã®Ïùº LLMÏúºÎ°ú Íµ¨ÌòÑ"
        ]

        # 7. ÏùëÎãµ Íµ¨ÏÑ±
        return AIvsDBComparisonResponse(
            project_uuid=project_uuid,
            analysis_period_hours=time_hours,
            sample_size=len(log_samples),
            analyzed_at=datetime.utcnow(),
            db_statistics=DBStatistics(
                total_logs=db_stats["total_logs"],
                error_count=db_stats["error_count"],
                warn_count=db_stats["warn_count"],
                info_count=db_stats["info_count"],
                error_rate=db_stats["error_rate"],
                peak_hour=db_stats["peak_hour"],
                peak_count=db_stats["peak_count"]
            ),
            ai_statistics=AIStatistics(
                estimated_total_logs=ai_stats.get("estimated_total_logs", 0),
                estimated_error_count=ai_stats.get("estimated_error_count", 0),
                estimated_warn_count=ai_stats.get("estimated_warn_count", 0),
                estimated_info_count=ai_stats.get("estimated_info_count", 0),
                estimated_error_rate=ai_stats.get("estimated_error_rate", 0.0),
                confidence_score=ai_stats.get("confidence_score", 0),
                reasoning=ai_stats.get("reasoning", "N/A")
            ),
            accuracy_metrics=AccuracyMetrics(
                total_logs_accuracy=accuracy_metrics["total_logs_accuracy"],
                error_count_accuracy=accuracy_metrics["error_count_accuracy"],
                warn_count_accuracy=accuracy_metrics["warn_count_accuracy"],
                info_count_accuracy=accuracy_metrics["info_count_accuracy"],
                error_rate_accuracy=accuracy_metrics["error_rate_accuracy"],
                overall_accuracy=accuracy_metrics["overall_accuracy"],
                ai_confidence=accuracy_metrics["ai_confidence"]
            ),
            verdict=verdict,
            technical_highlights=technical_highlights
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"üî¥ AI vs DB ÌÜµÍ≥Ñ ÎπÑÍµê Ï§ë ÏòàÏô∏ Î∞úÏÉù: project_uuid={project_uuid}, error={str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"AI vs DB ÌÜµÍ≥Ñ ÎπÑÍµê Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        )


@router.get("/statistics/hourly", response_model=HourlyStatisticsResponse)
async def get_hourly_statistics(
    project_uuid: str = Query(..., description="ÌîÑÎ°úÏ†ùÌä∏ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="Î∂ÑÏÑù Í∏∞Í∞Ñ (ÏãúÍ∞Ñ)")
) -> HourlyStatisticsResponse:
    """
    ÏãúÍ∞ÑÎåÄÎ≥Ñ Î°úÍ∑∏ ÌÜµÍ≥ÑÎ•º Ï°∞ÌöåÌï©ÎãàÎã§.

    1ÏãúÍ∞Ñ Îã®ÏúÑÎ°ú Î°úÍ∑∏ Î∂ÑÌè¨Î•º ÌôïÏù∏ÌïòÏó¨ Ìä∏Î†åÎìúÏôÄ Ìå®ÌÑ¥ÏùÑ Î∂ÑÏÑùÌï©ÎãàÎã§.
    """
    try:
        db_stats = _get_db_statistics(project_uuid, time_hours)

        if db_stats["total_logs"] == 0:
            raise HTTPException(
                status_code=404,
                detail=f"ÏµúÍ∑º {time_hours}ÏãúÍ∞Ñ ÎèôÏïà Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§."
            )

        hourly_data = [
            HourlyDataPoint(
                hour=h["hour"],
                total=h["total"],
                error=h["error"],
                warn=h["warn"],
                info=h["info"]
            )
            for h in db_stats["hourly_data"]
        ]

        return HourlyStatisticsResponse(
            project_uuid=project_uuid,
            analysis_period_hours=time_hours,
            total_logs=db_stats["total_logs"],
            error_rate=db_stats["error_rate"],
            peak_hour=db_stats["peak_hour"],
            peak_count=db_stats["peak_count"],
            hourly_data=hourly_data
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ÏãúÍ∞ÑÎåÄÎ≥Ñ ÌÜµÍ≥Ñ Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        )


@router.get("/statistics/compare-errors", response_model=ErrorComparisonResponse)
async def compare_error_statistics(
    project_uuid: str = Query(..., description="ÌîÑÎ°úÏ†ùÌä∏ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="Î∂ÑÏÑù Í∏∞Í∞Ñ (ÏãúÍ∞Ñ, Í∏∞Î≥∏ 24ÏãúÍ∞Ñ)"),
    sample_size: int = Query(100, ge=10, le=500, description="AI Î∂ÑÏÑùÏö© ÏÉòÌîå ÌÅ¨Í∏∞")
) -> ErrorComparisonResponse:
    """
    ERROR Î°úÍ∑∏ Ï†ÑÏö© DB vs AI ÌÜµÍ≥Ñ ÎπÑÍµê

    Vector KNN ÏÉòÌîåÎßÅÏùÑ ÌôúÏö©ÌïòÏó¨ Ïú†ÏÇ¨Ìïú ERROR Ìå®ÌÑ¥ÏùÑ Í∑∏Î£πÌïëÌïòÍ≥†,
    LLMÏúºÎ°ú ERROR ÌÜµÍ≥ÑÎ•º Ï∂îÏ†ïÌïòÏó¨ Ï†ïÌôïÎèÑÎ•º Í≤ÄÏ¶ùÌï©ÎãàÎã§.

    **ÌäπÏßï:**
    - ERROR Î°úÍ∑∏Îßå ÏßëÏ§ë Î∂ÑÏÑù
    - Vector KNNÏúºÎ°ú Ïú†ÏÇ¨ ERROR Í∑∏Î£πÌïë
    - log_vector ÌïÑÎìú ÌôúÏö© (Î≤°ÌÑ∞ÌôîÎêú ERRORÎßå)
    - Î≤°ÌÑ∞ÌôîÏú® 50% ÎØ∏Îßå Ïãú ÏûêÎèô ÎûúÎç§ ÏÉòÌîåÎßÅ Ìè¥Î∞±

    **Ï†ïÌôïÎèÑ ÏßÄÌëú:**
    - ERROR Í∞úÏàò Ï†ïÌôïÎèÑ
    - ERROR ÎπÑÏú® Ï†ïÌôïÎèÑ
    - Ï¢ÖÌï© Ï†ïÌôïÎèÑ (Í∞ÄÏ§ë ÌèâÍ∑†)
    """
    logger.info(f"ERROR ÎπÑÍµê ÏãúÏûë: project={project_uuid}, hours={time_hours}, sample_size={sample_size}")

    try:
        # Step 1: DBÏóêÏÑú ERROR ÌÜµÍ≥Ñ Ï°∞Ìöå
        db_stats = await _get_db_error_statistics(project_uuid, time_hours)
        logger.info(f"‚úÖ DB ERROR ÌÜµÍ≥Ñ: total_errors={db_stats['total_errors']}, rate={db_stats['error_rate']}%")

        if db_stats["total_errors"] == 0:
            logger.warning(f"‚ö†Ô∏è ERROR Î°úÍ∑∏ ÏóÜÏùå: project_uuid={project_uuid}")
            raise HTTPException(
                status_code=404,
                detail=f"ÏµúÍ∑º {time_hours}ÏãúÍ∞Ñ ÎèôÏïà ERROR Î°úÍ∑∏Í∞Ä ÏóÜÏäµÎãàÎã§."
            )

        # Step 2: Vector ÏÉòÌîåÎßÅ (ERRORÎßå)
        error_samples, vector_info = await _sample_errors_with_vector(
            project_uuid, time_hours, sample_size
        )
        logger.info(
            f"‚úÖ ERROR ÏÉòÌîåÎßÅ ÏôÑÎ£å: count={len(error_samples)}, "
            f"method={vector_info['sampling_method']}, "
            f"vectorization_rate={vector_info['vectorization_rate']}%"
        )

        if not error_samples:
            raise HTTPException(
                status_code=500,
                detail="ERROR Î°úÍ∑∏ ÏÉòÌîåÎßÅ Ïã§Ìå®"
            )

        # Step 3: Vector Ïú†ÏÇ¨ÎèÑÎ°ú ERROR Ï∂îÏ†ï (ÎèôÏ†Å threshold)
        ai_stats = await _vector_estimate_error_count(
            error_samples,
            project_uuid,
            time_hours,
            db_stats["total_logs"]
            # similarity_threshold=None -> ÎèôÏ†Å Í≥ÑÏÇ∞
        )
        logger.info(
            f"‚úÖ AI ERROR Ï∂îÏ†ï (Vector): estimated_errors={ai_stats['estimated_total_errors']}, "
            f"confidence={ai_stats['confidence_score']}"
        )

        # Step 4: Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
        accuracy = _calculate_error_accuracy(db_stats, ai_stats)
        logger.info(f"‚úÖ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞: overall={accuracy['overall_accuracy']:.1f}%")

        # Step 5: ÏùëÎãµ Íµ¨ÏÑ±
        response = ErrorComparisonResponse(
            project_uuid=project_uuid,
            analysis_period_hours=time_hours,
            sample_size=len(error_samples),
            analyzed_at=datetime.utcnow(),
            db_error_stats=DBErrorStats(
                total_errors=db_stats["total_errors"],
                error_rate=db_stats["error_rate"],
                peak_error_hour=db_stats["peak_error_hour"],
                peak_error_count=db_stats["peak_error_count"]
            ),
            ai_error_stats=AIErrorStats(
                estimated_total_errors=ai_stats["estimated_total_errors"],
                estimated_error_rate=ai_stats["estimated_error_rate"],
                confidence_score=ai_stats["confidence_score"],
                reasoning=ai_stats["reasoning"]
            ),
            accuracy_metrics=ErrorAccuracyMetrics(
                error_count_accuracy=accuracy["error_count_accuracy"],
                error_rate_accuracy=accuracy["error_rate_accuracy"],
                overall_accuracy=accuracy["overall_accuracy"]
            ),
            vector_analysis=VectorGroupingInfo(
                vectorized_error_count=vector_info["vectorized_error_count"],
                vectorization_rate=vector_info["vectorization_rate"],
                sampling_method=vector_info["sampling_method"],
                sample_distribution=vector_info["sample_distribution"]
            ) if vector_info else None
        )

        logger.info(f"‚úÖ ERROR ÎπÑÍµê ÏôÑÎ£å: accuracy={accuracy['overall_accuracy']:.1f}%")
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"üî¥ ERROR ÎπÑÍµê Ïã§Ìå®: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ERROR ÎπÑÍµê Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        )
