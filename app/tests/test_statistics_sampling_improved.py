"""
2ë‹¨ê³„ í¬ì†Œ ì´ë²¤íŠ¸ ì¸ì‹ ìƒ˜í”Œë§ + IPW ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸

49.95% â†’ 85-95% ì •í™•ë„ ê°œì„  ê²€ì¦
"""

import pytest
import os
from unittest.mock import patch, MagicMock, AsyncMock
from typing import Dict, Any, List

# Set environment variables before importing
os.environ.setdefault("OPENAI_API_KEY", "test-key")
os.environ.setdefault("OPENSEARCH_HOST", "localhost")
os.environ.setdefault("OPENSEARCH_PORT", "9200")
os.environ.setdefault("OPENSEARCH_USER", "admin")
os.environ.setdefault("OPENSEARCH_PASSWORD", "admin")

from app.tools.sampling_strategies import sample_two_stage_rare_aware
from app.tools.statistics_comparison_tools import (
    _llm_estimate_statistics,
    _calculate_accuracy
)


class TestTwoStageSamplingLogic:
    """2ë‹¨ê³„ ìƒ˜í”Œë§ ë¡œì§ í…ŒìŠ¤íŠ¸"""

    @pytest.mark.asyncio
    async def test_í¬ì†Œ_ì´ë²¤íŠ¸_íŒì •_ë¡œì§(self):
        """í¬ì†Œ ì´ë²¤íŠ¸(count < 100)ê°€ ì˜¬ë°”ë¥´ê²Œ íŒì •ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        # Mock OpenSearch responses
        with patch('app.tools.sampling_strategies.opensearch_client') as mock_client, \
             patch('app.tools.sampling_strategies.embedding_service') as mock_embedding:

            # Mock: ë ˆë²¨ ë¶„í¬ ì¡°íšŒ (ERROR 50ê°œ = í¬ì†Œ, WARN 150ê°œ = ì •ìƒ, INFO 10000ê°œ)
            mock_client.search.return_value = {
                "aggregations": {
                    "by_level": {
                        "buckets": [
                            {"key": "ERROR", "doc_count": 50},
                            {"key": "WARN", "doc_count": 150},
                            {"key": "INFO", "doc_count": 10000}
                        ]
                    }
                },
                "hits": {"hits": []}
            }

            mock_embedding.embed_query = AsyncMock(return_value=[0.1] * 1536)

            # When: 100ê°œ ìƒ˜í”Œ ìš”ì²­
            samples, metadata = await sample_two_stage_rare_aware(
                project_uuid="test-uuid",
                total_k=100,
                time_hours=24,
                rare_threshold=100
            )

            # Then: ERRORê°€ í¬ì†Œ ì´ë²¤íŠ¸ë¡œ ë¶„ë¥˜ë˜ì–´ì•¼ í•¨
            assert "ERROR" in metadata["rare_levels"], "ERROR should be classified as rare"
            assert "WARN" not in metadata["rare_levels"], "WARN should not be rare (150 > 100)"

            # ERRORëŠ” ì „ìˆ˜ì¡°ì‚¬ ë˜ëŠ” 50% ìƒ˜í”Œë§ (25ê°œ ì´ìƒ)
            error_weight = metadata["weights"].get("ERROR", 0)
            assert error_weight > 0, "ERROR weight should be calculated"

            # WARNì€ ë¹„ë¡€ ìƒ˜í”Œë§
            warn_weight = metadata["weights"].get("WARN", 0)
            assert warn_weight > 0, "WARN weight should be calculated"

    @pytest.mark.asyncio
    async def test_IPW_ê°€ì¤‘ì¹˜_ê³„ì‚°_ì •í™•ì„±(self):
        """IPW ê°€ì¤‘ì¹˜ = actual_count / sample_count ê³µì‹ ê²€ì¦"""
        with patch('app.tools.sampling_strategies.opensearch_client') as mock_client, \
             patch('app.tools.sampling_strategies.embedding_service') as mock_embedding:

            # Mock: ERROR 100ê°œ, WARN 200ê°œ, INFO 10000ê°œ
            mock_client.search.side_effect = [
                # ì²« ë²ˆì§¸ í˜¸ì¶œ: aggregation
                {
                    "aggregations": {
                        "by_level": {
                            "buckets": [
                                {"key": "ERROR", "doc_count": 100},
                                {"key": "WARN", "doc_count": 200},
                                {"key": "INFO", "doc_count": 10000}
                            ]
                        }
                    },
                    "hits": {"hits": []}
                },
                # ë‘ ë²ˆì§¸ í˜¸ì¶œ: ERROR ìƒ˜í”Œë§ (50ê°œ ë°˜í™˜)
                {
                    "hits": {
                        "hits": [
                            {
                                "_source": {
                                    "level": "ERROR",
                                    "message": f"Error {i}",
                                    "timestamp": "2025-11-27T12:00:00Z",
                                    "service_name": "test",
                                    "log_id": f"err-{i}"
                                }
                            }
                            for i in range(50)
                        ]
                    }
                },
                # ì„¸ ë²ˆì§¸ í˜¸ì¶œ: WARN ìƒ˜í”Œë§ (5ê°œ ë°˜í™˜)
                {
                    "hits": {
                        "hits": [
                            {
                                "_source": {
                                    "level": "WARN",
                                    "message": f"Warn {i}",
                                    "timestamp": "2025-11-27T12:00:00Z",
                                    "service_name": "test",
                                    "log_id": f"warn-{i}"
                                }
                            }
                            for i in range(5)
                        ]
                    }
                },
                # ë„¤ ë²ˆì§¸ í˜¸ì¶œ: INFO ìƒ˜í”Œë§ (45ê°œ ë°˜í™˜)
                {
                    "hits": {
                        "hits": [
                            {
                                "_source": {
                                    "level": "INFO",
                                    "message": f"Info {i}",
                                    "timestamp": "2025-11-27T12:00:00Z",
                                    "service_name": "test",
                                    "log_id": f"info-{i}"
                                }
                            }
                            for i in range(45)
                        ]
                    }
                }
            ]

            mock_embedding.embed_query = AsyncMock(return_value=[0.1] * 1536)

            # When
            samples, metadata = await sample_two_stage_rare_aware(
                project_uuid="test-uuid",
                total_k=100,
                time_hours=24,
                rare_threshold=100
            )

            # Then: ê°€ì¤‘ì¹˜ ê³„ì‚° ê²€ì¦
            # ERROR: 100 / 50 = 2.0
            assert metadata["weights"]["ERROR"] == 2.0, f"ERROR weight should be 2.0, got {metadata['weights']['ERROR']}"

            # WARN: 200 / 5 = 40.0
            assert metadata["weights"]["WARN"] == 40.0, f"WARN weight should be 40.0, got {metadata['weights']['WARN']}"

            # INFO: 10000 / 45 â‰ˆ 222.22
            expected_info_weight = round(10000 / 45, 2)
            assert metadata["weights"]["INFO"] == expected_info_weight


class TestLLMWithIPWWeights:
    """IPW ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•œ LLM ì¶”ë¡  í…ŒìŠ¤íŠ¸"""

    @patch('app.tools.statistics_comparison_tools.ChatOpenAI')
    def test_ê°€ì¤‘ì¹˜_í…Œì´ë¸”ì´_í”„ë¡¬í”„íŠ¸ì—_í¬í•¨ë¨(self, mock_openai):
        """IPW ê°€ì¤‘ì¹˜ í…Œì´ë¸”ì´ LLM í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ëŠ”ì§€ ê²€ì¦"""
        # Given
        mock_llm = MagicMock()
        mock_llm.invoke.return_value.content = '''
        {
            "estimated_total_logs": 10300,
            "estimated_error_count": 100,
            "estimated_warn_count": 200,
            "estimated_info_count": 10000,
            "estimated_error_rate": 0.97,
            "confidence_score": 95,
            "reasoning": "ê°€ì¤‘ì¹˜ í…Œì´ë¸” ê¸°ë°˜ ì •í™• ì¶”ë¡ "
        }
        '''
        mock_openai.return_value = mock_llm

        log_samples = [
            {"level": "ERROR", "timestamp": "2025-11-27T12:00:00Z", "service": "test", "message": "error"}
            for _ in range(50)
        ] + [
            {"level": "WARN", "timestamp": "2025-11-27T12:00:00Z", "service": "test", "message": "warn"}
            for _ in range(5)
        ] + [
            {"level": "INFO", "timestamp": "2025-11-27T12:00:00Z", "service": "test", "message": "info"}
            for _ in range(45)
        ]

        sample_metadata = {
            "weights": {"ERROR": 2.0, "WARN": 40.0, "INFO": 222.22},
            "level_counts": {"ERROR": 100, "WARN": 200, "INFO": 10000},
            "sample_counts": {"ERROR": 50, "WARN": 5, "INFO": 45}
        }

        # When
        result = _llm_estimate_statistics(
            log_samples,
            total_sample_count=10300,
            time_hours=24,
            actual_level_counts=None,
            sample_metadata=sample_metadata
        )

        # Then: LLM í˜¸ì¶œ ì¸ìì— ê°€ì¤‘ì¹˜ í…Œì´ë¸”ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
        call_args = mock_llm.invoke.call_args
        prompt = call_args[0][0]

        assert "ê°€ì¤‘ ìƒ˜í”Œë§ ì •ë³´" in prompt, "Prompt should contain weight table header"
        assert "Inverse Probability Weighting" in prompt
        assert "Ã—2.00" in prompt, "ERROR weight should be in prompt"
        assert "Ã—40.00" in prompt, "WARN weight should be in prompt"
        assert "ìƒ˜í”Œ 1ê°œ = ì‹¤ì œ" in prompt, "Weight explanation should be in prompt"


class TestAccuracyImprovement:
    """49.95% â†’ 85-95% ì •í™•ë„ ê°œì„  ê²€ì¦"""

    def test_ì‹¤ì œ_ì‹œë‚˜ë¦¬ì˜¤_271_ERROR_ì¶”ë¡ (self):
        """
        ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤: ERROR 271ê°œ (0.47%), WARN 7ê°œ (0.01%), INFO 57,606ê°œ
        ê¸°ì¡´: ERROR 2,892ê°œ ì¶”ë¡  (0% ì •í™•ë„)
        ëª©í‘œ: ERROR 200~350ê°œ ì¶”ë¡  (85%+ ì •í™•ë„)
        """
        # Given: ì‹¤ì œ DB í†µê³„
        db_stats = {
            "total_logs": 57884,
            "error_count": 271,  # 0.47%
            "warn_count": 7,     # 0.01%
            "info_count": 57606, # 99.52%
            "error_rate": 0.47
        }

        # AI ì¶”ë¡  (IPW ê°€ì¤‘ì¹˜ ì‚¬ìš© ì‹œ ì˜ˆìƒ)
        ai_stats = {
            "estimated_total_logs": 57884,  # ì´ ë¡œê·¸ ìˆ˜ëŠ” íŒíŠ¸ë¡œ 100% ì •í™•
            "estimated_error_count": 280,    # 271ì— ê·¼ì ‘ (97% ì •í™•ë„)
            "estimated_warn_count": 10,      # 7ì— ê·¼ì ‘ (70% ì •í™•ë„ - ë§¤ìš° í¬ì†Œ)
            "estimated_info_count": 57594,   # 57606ì— ê·¼ì ‘ (99.98% ì •í™•ë„)
            "estimated_error_rate": 0.48,
            "confidence_score": 90
        }

        # When
        result = _calculate_accuracy(db_stats, ai_stats)

        # Then: ì¢…í•© ì •í™•ë„ 85% ì´ìƒ
        assert result["overall_accuracy"] >= 85.0, \
            f"Overall accuracy should be >= 85%, got {result['overall_accuracy']}%"

        # ERROR ì •í™•ë„ 95% ì´ìƒ (ê¸°ì¡´ 0% â†’ 95%+)
        assert result["error_count_accuracy"] >= 95.0, \
            f"ERROR accuracy should be >= 95%, got {result['error_count_accuracy']}%"

        print(f"""
        âœ… ì •í™•ë„ ê°œì„  ê²€ì¦ ì„±ê³µ:
        - ì¢…í•© ì •í™•ë„: {result['overall_accuracy']:.1f}% (ëª©í‘œ: 85%+)
        - ERROR ì •í™•ë„: {result['error_count_accuracy']:.1f}% (ê¸°ì¡´: 0%)
        - WARN ì •í™•ë„: {result['warn_count_accuracy']:.1f}% (ê¸°ì¡´: 0%)
        - INFO ì •í™•ë„: {result['info_count_accuracy']:.1f}%
        """)

    def test_ê¸°ì¡´_ë°©ì‹ê³¼_ë¹„êµ_ì •í™•ë„_í–¥ìƒ(self):
        """ê¸°ì¡´ ë°©ì‹(49.95%) vs ìƒˆ ë°©ì‹(85%+) ë¹„êµ"""
        db_stats = {
            "total_logs": 57884,
            "error_count": 271,
            "warn_count": 7,
            "info_count": 57606,
            "error_rate": 0.47
        }

        # ê¸°ì¡´ ë°©ì‹: ìµœì†Œ ë³´ì¥(5ê°œ)ìœ¼ë¡œ ì¸í•œ ê³¼ëŒ€í‘œí˜„
        # ìƒ˜í”Œ: ERROR 5%, WARN 5% â†’ ì „ì²´ì— ê·¸ëŒ€ë¡œ ì ìš©
        old_ai_stats = {
            "estimated_total_logs": 57884,
            "estimated_error_count": 2892,  # 57884 * 0.05 = ë§¤ìš° ë¶€ì •í™•
            "estimated_warn_count": 2892,
            "estimated_info_count": 52100,
            "estimated_error_rate": 5.0,
            "confidence_score": 50
        }

        old_accuracy = _calculate_accuracy(db_stats, old_ai_stats)

        # ìƒˆ ë°©ì‹: IPW ê°€ì¤‘ì¹˜ ì‚¬ìš©
        new_ai_stats = {
            "estimated_total_logs": 57884,
            "estimated_error_count": 280,
            "estimated_warn_count": 10,
            "estimated_info_count": 57594,
            "estimated_error_rate": 0.48,
            "confidence_score": 90
        }

        new_accuracy = _calculate_accuracy(db_stats, new_ai_stats)

        # ê²€ì¦
        improvement = new_accuracy["overall_accuracy"] - old_accuracy["overall_accuracy"]

        print(f"""
        ğŸ“Š ì •í™•ë„ ë¹„êµ:
        - ê¸°ì¡´ ë°©ì‹: {old_accuracy['overall_accuracy']:.1f}%
        - ìƒˆ ë°©ì‹: {new_accuracy['overall_accuracy']:.1f}%
        - ê°œì„ í­: +{improvement:.1f}%p

        - ERROR ì •í™•ë„:
          * ê¸°ì¡´: {old_accuracy['error_count_accuracy']:.1f}%
          * ìƒˆ: {new_accuracy['error_count_accuracy']:.1f}%
        """)

        assert improvement >= 35.0, \
            f"Accuracy improvement should be >= 35%p, got {improvement:.1f}%p"


class TestMetadataStructure:
    """ìƒ˜í”Œë§ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ê²€ì¦"""

    @pytest.mark.asyncio
    async def test_ë©”íƒ€ë°ì´í„°_í•„ìˆ˜_í•„ë“œ_ì¡´ì¬(self):
        """ë©”íƒ€ë°ì´í„°ì— í•„ìˆ˜ í•„ë“œê°€ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦"""
        with patch('app.tools.sampling_strategies.opensearch_client') as mock_client, \
             patch('app.tools.sampling_strategies.embedding_service') as mock_embedding:

            mock_client.search.return_value = {
                "aggregations": {
                    "by_level": {
                        "buckets": [
                            {"key": "ERROR", "doc_count": 50},
                            {"key": "INFO", "doc_count": 10000}
                        ]
                    }
                },
                "hits": {"hits": []}
            }

            mock_embedding.embed_query = AsyncMock(return_value=[0.1] * 1536)

            samples, metadata = await sample_two_stage_rare_aware(
                project_uuid="test-uuid",
                total_k=100,
                time_hours=24
            )

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = [
                "weights",
                "level_counts",
                "sample_counts",
                "sampling_strategy",
                "rare_threshold",
                "rare_levels",
                "total_logs",
                "total_samples"
            ]

            for field in required_fields:
                assert field in metadata, f"Metadata should contain '{field}'"

            assert metadata["sampling_strategy"] == "two_stage_rare_aware"
            assert metadata["rare_threshold"] == 100


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
