"""
ìƒ˜í”Œë§ ë¬¸ì œ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
python3 diagnose_sampling.py 831776ac-2d47-3e23-83b9-7619972f0cbf
"""

import asyncio
import sys
import os
from datetime import datetime, timedelta

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ.setdefault("OPENAI_API_KEY", "dummy-key-for-diagnosis")
os.environ.setdefault("OPENSEARCH_HOST", "localhost")

from app.core.opensearch import opensearch_client
from app.tools.sampling_strategies import _get_level_distribution


async def diagnose(project_uuid: str):
    """í”„ë¡œì íŠ¸ì˜ ë¡œê·¸ ìƒíƒœ ì§„ë‹¨"""
    print(f"ğŸ” í”„ë¡œì íŠ¸ ì§„ë‹¨ ì‹œì‘: {project_uuid}\n")

    # 1. ì¸ë±ìŠ¤ íŒ¨í„´ í™•ì¸
    index_pattern = f"{project_uuid.replace('-', '_')}_*"
    print(f"1ï¸âƒ£ ì¸ë±ìŠ¤ íŒ¨í„´: {index_pattern}")

    # 2. ìµœê·¼ 24ì‹œê°„ ë¡œê·¸ ê°œìˆ˜ í™•ì¸
    try:
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=24)

        query_body = {
            "size": 0,
            "query": {
                "range": {
                    "timestamp": {
                        "gte": start_time.isoformat() + "Z",
                        "lte": end_time.isoformat() + "Z"
                    }
                }
            },
            "aggs": {
                "by_level": {
                    "terms": {"field": "level", "size": 10}
                }
            }
        }

        results = await asyncio.to_thread(
            opensearch_client.search,
            index=index_pattern,
            body=query_body
        )

        total = results.get("hits", {}).get("total", {}).get("value", 0)
        print(f"2ï¸âƒ£ ìµœê·¼ 24ì‹œê°„ ë¡œê·¸ ì´ ê°œìˆ˜: {total:,}ê°œ")

        buckets = results.get("aggregations", {}).get("by_level", {}).get("buckets", [])
        if buckets:
            print("\n   ë ˆë²¨ë³„ ë¶„í¬:")
            for bucket in buckets:
                print(f"   - {bucket['key']}: {bucket['doc_count']:,}ê°œ")
        else:
            print("   âš ï¸ ë ˆë²¨ë³„ ë¶„í¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        return

    # 3. log_vector í•„ë“œê°€ ìˆëŠ” ë¡œê·¸ ê°œìˆ˜ í™•ì¸
    try:
        query_body = {
            "size": 0,
            "query": {
                "bool": {
                    "must": [
                        {
                            "exists": {"field": "log_vector"}
                        },
                        {
                            "range": {
                                "timestamp": {
                                    "gte": start_time.isoformat() + "Z",
                                    "lte": end_time.isoformat() + "Z"
                                }
                            }
                        }
                    ]
                }
            },
            "aggs": {
                "by_level": {
                    "terms": {"field": "level", "size": 10}
                }
            }
        }

        results = await asyncio.to_thread(
            opensearch_client.search,
            index=index_pattern,
            body=query_body
        )

        total_vectorized = results.get("hits", {}).get("total", {}).get("value", 0)
        print(f"\n3ï¸âƒ£ log_vector í•„ë“œê°€ ìˆëŠ” ë¡œê·¸: {total_vectorized:,}ê°œ")

        if total > 0:
            vectorization_rate = (total_vectorized / total) * 100
            print(f"   ë²¡í„°í™”ìœ¨: {vectorization_rate:.1f}%")

        buckets = results.get("aggregations", {}).get("by_level", {}).get("buckets", [])
        if buckets:
            print("\n   ë²¡í„°í™”ëœ ë¡œê·¸ì˜ ë ˆë²¨ë³„ ë¶„í¬:")
            for bucket in buckets:
                print(f"   - {bucket['key']}: {bucket['doc_count']:,}ê°œ")
        else:
            print("   âš ï¸ ë²¡í„°í™”ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")

    # 4. ì§„ë‹¨ ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ì§„ë‹¨ ê²°ê³¼ ìš”ì•½")
    print("="*60)

    if total == 0:
        print("âŒ ë¬¸ì œ: ìµœê·¼ 24ì‹œê°„ ë™ì•ˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. ë¡œê·¸ê°€ OpenSearchì— ìˆ˜ì§‘ë˜ê³  ìˆëŠ”ì§€ í™•ì¸")
        print("2. ì¸ë±ìŠ¤ íŒ¨í„´ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("3. timestamp í•„ë“œê°€ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸")

    elif total_vectorized == 0:
        print("âŒ ë¬¸ì œ: ë¡œê·¸ëŠ” ìˆì§€ë§Œ ë²¡í„°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. ë²¡í„°í™” ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸:")
        print("   - ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")
        print("   - periodic_enrichment_scheduler.py ë¡œê·¸ í™•ì¸")
        print("2. ERROR ë¡œê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ERRORë§Œ ë²¡í„°í™”)")
        print("3. OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")

    elif vectorization_rate < 10:
        print(f"âš ï¸ ì£¼ì˜: ë²¡í„°í™”ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤ ({vectorization_rate:.1f}%)")
        print("\nê¶Œì¥ ì‚¬í•­:")
        print("1. ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ERROR ë¡œê·¸ë¥¼ ë²¡í„°í™”í•˜ê³  ìˆëŠ”ì§€ í™•ì¸")
        print("2. ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸ (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” 10ì´ˆë§ˆë‹¤ ì‹¤í–‰)")

    else:
        print(f"âœ… ë¡œê·¸ ìƒíƒœ ì–‘í˜¸: ì´ {total:,}ê°œ, ë²¡í„°í™” {total_vectorized:,}ê°œ ({vectorization_rate:.1f}%)")
        print("\nVector ìƒ˜í”Œë§ì´ ê°€ëŠ¥í•œ ìƒíƒœì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python3 diagnose_sampling.py <project_uuid>")
        sys.exit(1)

    project_uuid = sys.argv[1]
    asyncio.run(diagnose(project_uuid))
