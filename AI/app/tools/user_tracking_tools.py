"""
ì‚¬ìš©ì ì¶”ì  ë„êµ¬ (User Tracking Tools)
- IP ê¸°ë°˜ ì‚¬ìš©ì ì„¸ì…˜ ì¶”ì , íŒŒë¼ë¯¸í„° ë¶„í¬, ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ
"""

from typing import Optional
from datetime import datetime, timedelta
from langchain_core.tools import tool

from app.core.opensearch import opensearch_client
from app.tools.common_fields import BASE_FIELDS, LOG_DETAILS_FIELDS


@tool
async def trace_user_session(
    requester_ip: str,
    project_uuid: str,
    time_hours: int = 24,
    include_info: bool = False
) -> str:
    """
    íŠ¹ì • ì‚¬ìš©ìì˜ í™œë™ ë¡œê·¸ë¥¼ IP ê¸°ë°˜ìœ¼ë¡œ ì¶”ì í•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… requester_ip í•„ë“œë¡œ ì‚¬ìš©ì ì‹ë³„ ë° í™œë™ ì¶”ì 
    - âœ… ì‹œê°„ìˆœ ì •ë ¬ë¡œ ì‚¬ìš©ì í–‰ë™ ìˆœì„œ íŒŒì•…
    - âœ… trace_idë¡œ ìš”ì²­ ê·¸ë£¹í•‘
    - âœ… í˜¸ì¶œí•œ API ëª©ë¡ ë° ë¹ˆë„ ì§‘ê³„
    - âœ… ì—ëŸ¬ ë°œìƒ íŒ¨í„´ ë¶„ì„
    - âŒ í¬ë¡œìŠ¤ ì„œë¹„ìŠ¤ ì¶”ì ì€ ì œí•œì  (trace_id ë²”ìœ„ ë‚´)

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "IP 203.0.113.45 ì‚¬ìš©ìì˜ í™œë™ ë‚´ì—­"
    2. "ì´ ì‚¬ìš©ìê°€ ì–´ë–¤ APIë¥¼ í˜¸ì¶œí–ˆì–´?"
    3. "íŠ¹ì • IPì˜ ì—ëŸ¬ íˆìŠ¤í† ë¦¬"
    4. "ì‚¬ìš©ìë³„ ì—ëŸ¬ ë°œìƒ íŒ¨í„´"
    5. "ì•…ì„± ì‚¬ìš©ì íƒì§€" (ë¹„ì •ìƒ ìš”ì²­ íŒ¨í„´)

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - requester_ipê°€ nullì¸ ë¡œê·¸ëŠ” ì œì™¸ë©ë‹ˆë‹¤
    - ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ IPë§Œ ì‚¬ìš© (ì‚¬ìš©ì ID ë¯¸í¬í•¨)

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        requester_ip: ì¶”ì í•  IP ì£¼ì†Œ (í•„ìˆ˜, ì˜ˆ: "203.0.113.45")
        time_hours: ì¶”ì  ê¸°ê°„ (ê¸°ë³¸ 24ì‹œê°„)
        include_info: INFO ë¡œê·¸ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ False, ERROR/WARNë§Œ)

    ê´€ë ¨ ë„êµ¬:
    - get_logs_by_trace_id: íŠ¹ì • ìš”ì²­ì˜ ì „ì²´ ë¡œê·¸ ì¶”ì 
    - get_recent_errors: ìµœê·¼ ì—ëŸ¬ ëª©ë¡
    - analyze_error_by_layer: ë ˆì´ì–´ë³„ ì—ëŸ¬ ë¶„ì„

    Returns:
        ì‚¬ìš©ì í™œë™ ë‚´ì—­ (ì‹œê°„ìˆœ, API í†µê³„, ì—ëŸ¬ íŒ¨í„´)
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    # Query êµ¬ì„±
    must_clauses = [
        {"exists": {"field": "requester_ip"}},
        {"match_phrase": {"requester_ip": requester_ip}},  # IP íƒ€ì… í•„ë“œ ì§€ì›ì„ ìœ„í•´ match_phrase ì‚¬ìš©
        {"range": {
            "timestamp": {
                "gte": start_time.isoformat() + "Z",
                "lte": end_time.isoformat() + "Z"
            }
        }}
    ]

    # INFO ë¡œê·¸ ì œì™¸ ì˜µì…˜
    if not include_info:
        must_clauses.append({
            "bool": {
                "should": [
                    {"term": {"level": "ERROR"}},
                    {"term": {"level": "WARN"}}
                ],
                "minimum_should_match": 1
            }
        })

    try:
        # OpenSearch ê²€ìƒ‰ + ì§‘ê³„
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 50,  # ìµœê·¼ 50ê°œ í™œë™
                "query": {"bool": {"must": must_clauses}},
                "sort": [{"timestamp": "asc"}],  # ì‹œê°„ìˆœ ì •ë ¬
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS + ["ai_analysis.summary"],
                "aggs": {
                    "api_stats": {
                        "terms": {
                            "field": "log_details.request_uri",
                            "size": 10,
                            "missing": "__no_api__"
                        },
                        "aggs": {
                            "http_methods": {
                                "terms": {"field": "log_details.http_method", "size": 5}
                            }
                        }
                    },
                    "error_stats": {
                        "filter": {"term": {"level": "ERROR"}},
                        "aggs": {
                            "by_service": {
                                "terms": {"field": "service_name", "size": 5}
                            }
                        }
                    },
                    "total_requests": {
                        "cardinality": {"field": "trace_id"}
                    }
                }
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_logs = results.get("hits", {}).get("total", {}).get("value", 0)
        aggs = results.get("aggregations", {})

        if total_logs == 0:
            return (
                f"IP {requester_ip}ì˜ í™œë™ ë¡œê·¸ê°€ ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì—†ìŠµë‹ˆë‹¤.\n\n"
                f"ğŸ’¡ í™•ì¸ ì‚¬í•­:\n"
                f"1. IP ì£¼ì†Œê°€ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”\n"
                f"2. ì‹œê°„ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš” (time_hours ì¦ê°€)\n"
                f"3. include_info=Trueë¡œ INFO ë¡œê·¸ë„ í¬í•¨í•´ë³´ì„¸ìš”"
            )

        # ê²°ê³¼ í¬ë§·íŒ…
        lines = [
            f"## ğŸ“Š IP {requester_ip} í™œë™ ë¶„ì„",
            f"**ê¸°ê°„:** ìµœê·¼ {time_hours}ì‹œê°„",
            f"**ì´ ë¡œê·¸:** {total_logs}ê±´",
            f"**ê³ ìœ  ìš”ì²­:** {aggs.get('total_requests', {}).get('value', 0)}ê±´",
            ""
        ]

        # API í˜¸ì¶œ í†µê³„
        api_buckets = aggs.get("api_stats", {}).get("buckets", [])
        if api_buckets:
            lines.append("### ğŸŒ í˜¸ì¶œí•œ API TOP 10")
            lines.append("")
            lines.append("| API | í˜¸ì¶œ ìˆ˜ | ì£¼ìš” ë©”ì„œë“œ |")
            lines.append("|-----|---------|-------------|")

            for bucket in api_buckets[:10]:
                api = bucket.get("key", "")
                if api == "__no_api__":
                    api = "(Non-HTTP ë¡œê·¸)"
                count = bucket.get("doc_count", 0)
                methods = bucket.get("http_methods", {}).get("buckets", [])
                method_str = ", ".join([m["key"] for m in methods[:3]])

                lines.append(f"| {api} | **{count}ê±´** | {method_str} |")

            lines.append("")

        # ì—ëŸ¬ í†µê³„
        error_count = aggs.get("error_stats", {}).get("doc_count", 0)
        if error_count > 0:
            lines.append("### ğŸ”´ ì—ëŸ¬ ë°œìƒ í˜„í™©")
            lines.append(f"**ì´ ì—ëŸ¬:** {error_count}ê±´")
            lines.append("")

            error_services = aggs.get("error_stats", {}).get("by_service", {}).get("buckets", [])
            if error_services:
                lines.append("**ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬:**")
                for svc in error_services:
                    service = svc.get("key", "unknown")
                    svc_count = svc.get("doc_count", 0)
                    lines.append(f"- {service}: {svc_count}ê±´")
                lines.append("")

        # ìµœê·¼ í™œë™ ë‚´ì—­ (ì‹œê°„ìˆœ)
        lines.append("### ğŸ“‹ ìµœê·¼ í™œë™ ë‚´ì—­ (ì‹œê°„ìˆœ)")
        lines.append("")

        prev_timestamp = None
        for i, hit in enumerate(hits[:20], 1):  # ìµœê·¼ 20ê°œë§Œ
            source = hit["_source"]
            timestamp_str = source.get("timestamp", "")[:19].replace("T", " ")
            level = source.get("level", "INFO")
            service = source.get("service_name", "unknown")
            message = source.get("message", "")[:150]

            log_details = source.get("log_details", {})
            http_method = log_details.get("http_method", "")
            request_uri = log_details.get("request_uri", "")
            response_status = log_details.get("response_status")

            trace_id = source.get("trace_id", "")

            # ì‹œê°„ ê°„ê²© ê³„ì‚° (ì´ì „ ë¡œê·¸ì™€ì˜ ì°¨ì´)
            time_gap = ""
            if prev_timestamp:
                try:
                    prev_dt = datetime.fromisoformat(prev_timestamp.replace("Z", ""))
                    curr_dt = datetime.fromisoformat(source.get("timestamp", "").replace("Z", ""))
                    gap_seconds = (curr_dt - prev_dt).total_seconds()
                    if gap_seconds > 60:
                        time_gap = f" (+{int(gap_seconds/60)}ë¶„)"
                    elif gap_seconds > 1:
                        time_gap = f" (+{int(gap_seconds)}ì´ˆ)"
                except:
                    pass

            prev_timestamp = source.get("timestamp", "")

            # ë ˆë²¨ë³„ ì´ëª¨ì§€
            level_emoji = {"ERROR": "ğŸ”´", "WARN": "ğŸŸ¡", "INFO": "ğŸŸ¢"}.get(level, "âšª")

            lines.append(f"{i}. [{level_emoji} {level}] {timestamp_str}{time_gap}")

            # HTTP ìš”ì²­ ì •ë³´
            if http_method and request_uri:
                status_str = f" â†’ {response_status}" if response_status else ""
                lines.append(f"   ğŸŒ {http_method} {request_uri}{status_str}")

            # ì„œë¹„ìŠ¤
            lines.append(f"   ğŸ“¦ {service}")

            # trace_id (ì—°ê´€ ìš”ì²­ ì¶”ì  ê°€ëŠ¥)
            if trace_id:
                lines.append(f"   ğŸ”— trace_id: {trace_id}")

            # ë©”ì‹œì§€
            if message:
                lines.append(f"   ğŸ’¬ {message}")

            lines.append("")

        # ë¶„ì„ ìš”ì•½
        lines.append("### ğŸ’¡ ë¶„ì„ ìš”ì•½")
        lines.append("")

        # ì—ëŸ¬ìœ¨ ê³„ì‚°
        if total_logs > 0:
            error_rate = (error_count / total_logs * 100) if error_count else 0
            lines.append(f"- **ì—ëŸ¬ìœ¨:** {error_rate:.1f}% ({error_count}/{total_logs})")

        # í™œë™ íŒ¨í„´
        if total_logs > 20:
            lines.append(f"- **í™œë™ëŸ‰:** ë†’ìŒ ({total_logs}ê±´)")
        elif total_logs > 5:
            lines.append(f"- **í™œë™ëŸ‰:** ë³´í†µ ({total_logs}ê±´)")
        else:
            lines.append(f"- **í™œë™ëŸ‰:** ë‚®ìŒ ({total_logs}ê±´)")

        # ê¶Œì¥ ì‚¬í•­
        if error_count > 3:
            lines.append("")
            lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
            lines.append(f"1. ì—ëŸ¬ {error_count}ê±´ ë°œìƒ - ìƒì„¸ ë¶„ì„ ê¶Œì¥ (get_recent_errors)")
            if error_services:
                top_error_service = error_services[0].get("key", "")
                lines.append(f"2. {top_error_service} ì„œë¹„ìŠ¤ ì§‘ì¤‘ ì ê²€")

        return "\n".join(lines)

    except Exception as e:
        return f"ì‚¬ìš©ì ì„¸ì…˜ ì¶”ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def analyze_parameter_distribution(
    project_uuid: str,
    class_name: str,
    method_name: str,
    time_hours: int = 168
) -> str:
    """
    íŠ¹ì • ë©”ì„œë“œì˜ parameters í•„ë“œ ê°’ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… parameters JSON í•„ë“œ íŒŒì‹± ë° ë¶„ì„
    - âœ… íŒŒë¼ë¯¸í„°ë³„ ê°’ ë²”ìœ„, null ë¹„ìœ¨ ì§‘ê³„
    - âœ… ì—ëŸ¬ ë°œìƒ íŒŒë¼ë¯¸í„° íŒ¨í„´ ì‹ë³„
    - âœ… ê°’ ë¶„í¬ í†µê³„ (ìµœë¹ˆê°’, ì´ìƒì¹˜)
    - âŒ ë³µì¡í•œ nested JSON êµ¬ì¡°ëŠ” ì œí•œì  ë¶„ì„

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "UserService.createUserëŠ” ì£¼ë¡œ ì–´ë–¤ ê°’ìœ¼ë¡œ í˜¸ì¶œë¼?"
    2. "ì´ ë©”ì„œë“œì˜ íŒŒë¼ë¯¸í„° null ë¹„ìœ¨"
    3. "ì—ëŸ¬ ë°œìƒ ì‹œ íŒŒë¼ë¯¸í„° íŒ¨í„´"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - parameters í•„ë“œê°€ ì—†ëŠ” ë¡œê·¸ëŠ” ì œì™¸ë©ë‹ˆë‹¤
    - ê¸°ë³¸ ë¶„ì„ ê¸°ê°„ì€ 7ì¼ì…ë‹ˆë‹¤

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        class_name: í´ë˜ìŠ¤ ì´ë¦„ (í•„ìˆ˜, ì˜ˆ: "UserService")
        method_name: ë©”ì„œë“œ ì´ë¦„ (í•„ìˆ˜, ì˜ˆ: "createUser")
        time_hours: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸ 168ì‹œê°„=7ì¼)

    ê´€ë ¨ ë„êµ¬:
    - analyze_request_patterns: HTTP Request Body íŒ¨í„´ ë¶„ì„
    - analyze_errors_unified: ì—ëŸ¬ í†µí•© ë¶„ì„

    Returns:
        íŒŒë¼ë¯¸í„° ê°’ ë¶„í¬, null ë¹„ìœ¨, ì—ëŸ¬ íŒ¨í„´
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=time_hours)

    try:
        # OpenSearch ê²€ìƒ‰
        results = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 100,  # íŒŒë¼ë¯¸í„° ë¶„ì„ìš© ìƒ˜í”Œ
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"class_name": class_name}},
                            {"term": {"method_name": method_name}},
                            {"exists": {"field": "parameters"}},
                            {"range": {
                                "timestamp": {
                                    "gte": start_time.isoformat() + "Z",
                                    "lte": end_time.isoformat() + "Z"
                                }
                            }}
                        ]
                    }
                },
                "sort": [{"timestamp": "desc"}],
                "_source": ["parameters", "level", "timestamp", "log_id"]
            }
        )

        hits = results.get("hits", {}).get("hits", [])
        total_count = results.get("hits", {}).get("total", {}).get("value", 0)

        if total_count == 0:
            return (
                f"{class_name}.{method_name}ì˜ íŒŒë¼ë¯¸í„° ë°ì´í„°ê°€ ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ì—†ìŠµë‹ˆë‹¤.\n\n"
                f"ğŸ’¡ í™•ì¸ ì‚¬í•­:\n"
                f"1. í´ë˜ìŠ¤/ë©”ì„œë“œ ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”\n"
                f"2. ë¡œê·¸ì— parameters í•„ë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\n"
                f"3. ì‹œê°„ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš” (time_hours ì¦ê°€)"
            )

        # íŒŒë¼ë¯¸í„° ë¶„ì„
        param_stats = {}  # {param_name: {values: [], null_count: 0, error_count: 0}}
        error_params = []  # ì—ëŸ¬ ë°œìƒ ì‹œ íŒŒë¼ë¯¸í„° ê°’ë“¤

        for hit in hits:
            source = hit["_source"]
            params = source.get("parameters")
            level = source.get("level", "INFO")

            if not isinstance(params, dict):
                continue

            # íŒŒë¼ë¯¸í„°ë³„ í†µê³„ ìˆ˜ì§‘
            for key, value in params.items():
                if key not in param_stats:
                    param_stats[key] = {"values": [], "null_count": 0, "error_count": 0}

                if value is None:
                    param_stats[key]["null_count"] += 1
                else:
                    param_stats[key]["values"].append(value)

                if level == "ERROR":
                    param_stats[key]["error_count"] += 1

            # ì—ëŸ¬ ì‹œ íŒŒë¼ë¯¸í„° ì €ì¥
            if level == "ERROR":
                error_params.append(params)

        # ê²°ê³¼ í¬ë§·íŒ…
        lines = [
            f"## ğŸ“Š {class_name}.{method_name} íŒŒë¼ë¯¸í„° ë¶„ì„",
            f"**ê¸°ê°„:** ìµœê·¼ {time_hours}ì‹œê°„",
            f"**ë¶„ì„ ìƒ˜í”Œ:** {len(hits)}ê±´",
            ""
        ]

        # íŒŒë¼ë¯¸í„°ë³„ í†µê³„
        if param_stats:
            lines.append("### ğŸ“‹ íŒŒë¼ë¯¸í„°ë³„ í†µê³„")
            lines.append("")
            lines.append("| íŒŒë¼ë¯¸í„° | ìƒ˜í”Œ ìˆ˜ | Null ë¹„ìœ¨ | ì—ëŸ¬ ë°œìƒ | ì£¼ìš” ê°’ |")
            lines.append("|----------|---------|-----------|-----------|---------|")

            for param_name, stats in sorted(param_stats.items()):
                sample_count = len(stats["values"]) + stats["null_count"]
                null_ratio = (stats["null_count"] / sample_count * 100) if sample_count > 0 else 0
                error_count = stats["error_count"]

                # ì£¼ìš” ê°’ (ìµœë¹ˆê°’ 3ê°œ)
                from collections import Counter
                value_counts = Counter([str(v)[:30] for v in stats["values"]])
                top_values = value_counts.most_common(3)
                top_values_str = ", ".join([f"{v}" for v, c in top_values]) if top_values else "-"

                lines.append(
                    f"| {param_name} | {sample_count} | "
                    f"**{null_ratio:.1f}%** | {error_count}ê±´ | {top_values_str} |"
                )

            lines.append("")

        # ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
        if error_params:
            lines.append("### ğŸ”´ ì—ëŸ¬ ë°œìƒ íŒŒë¼ë¯¸í„° íŒ¨í„´")
            lines.append(f"**ì—ëŸ¬ ìƒ˜í”Œ:** {len(error_params)}ê±´")
            lines.append("")

            # ê³µí†µ íŒ¨í„´ ì°¾ê¸°
            common_issues = []

            for param_name, stats in param_stats.items():
                # Null ì—ëŸ¬
                if stats["null_count"] > 0 and stats["error_count"] > 0:
                    null_error_ratio = (stats["error_count"] / stats["null_count"] * 100) if stats["null_count"] > 0 else 0
                    if null_error_ratio > 50:
                        common_issues.append(f"- **{param_name}**: null ê°’ ì‹œ ì—ëŸ¬ ë¹ˆë²ˆ (null {stats['null_count']}ê±´ ì¤‘ ì—ëŸ¬ {stats['error_count']}ê±´)")

            if common_issues:
                lines.append("**ê³µí†µ ë¬¸ì œì :**")
                lines.extend(common_issues)
            else:
                lines.append("íŠ¹ë³„í•œ íŒ¨í„´ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            lines.append("")

        # ê¶Œì¥ ì¡°ì¹˜
        lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
        lines.append("")

        high_null_params = [name for name, stats in param_stats.items()
                           if (stats["null_count"] / (len(stats["values"]) + stats["null_count"]) * 100) > 30]

        if high_null_params:
            lines.append(f"1. **Null ì²´í¬ ê°•í™”:** {', '.join(high_null_params[:3])}")

        if error_params:
            lines.append(f"2. **ì…ë ¥ ê²€ì¦ ì¶”ê°€:** ì—ëŸ¬ ë°œìƒ íŒŒë¼ë¯¸í„° íŒ¨í„´ ê²€í† ")

        lines.append(f"3. **íŒŒë¼ë¯¸í„° íƒ€ì… í™•ì¸:** ì˜ˆìƒ íƒ€ì…ê³¼ ì‹¤ì œ ê°’ ì¼ì¹˜ ì—¬ë¶€")

        return "\n".join(lines)

    except Exception as e:
        return f"íŒŒë¼ë¯¸í„° ë¶„í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
async def trace_error_propagation(
    project_uuid: str,
    log_id: int,
    time_window: int = 60
) -> str:
    """
    íŠ¹ì • ì—ëŸ¬ê°€ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë¡œ ì „íŒŒë˜ëŠ” ê²½ë¡œë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

    ì´ ë„êµ¬ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - âœ… log_idì˜ timestamp ê¸°ì¤€ Â±time_windowì´ˆ ë²”ìœ„ ë¡œê·¸ ìˆ˜ì§‘
    - âœ… trace_id ë˜ëŠ” requester_ipê°€ ê°™ì€ ì—°ê´€ ë¡œê·¸ ì¶”ì¶œ
    - âœ… timestamp ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì—°ì‡„ ë°˜ì‘ íŒŒì•…
    - âœ… ë ˆì´ì–´ ê°„ ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ ì‹œê°í™”
    - âŒ ë¹„ë™ê¸° ì‘ì—… ì „íŒŒëŠ” ì¶”ì  ì œí•œì 

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    1. "ì´ ì—ëŸ¬ê°€ ì–´ë–»ê²Œ ì „íŒŒëì–´?"
    2. "log_id 12345ì˜ ì—°ì‡„ ì—ëŸ¬ ì°¾ê¸°"
    3. "ì—ëŸ¬ê°€ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¡œ í¼ì¡Œì–´?"

    âš ï¸ ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
    - 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
    - log_idê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¶„ì„ ë¶ˆê°€
    - trace_idê°€ ì—†ìœ¼ë©´ IP ê¸°ë°˜ ì¶”ì  (ì •í™•ë„ ë‚®ìŒ)

    ì…ë ¥ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹):
        log_id: ë¶„ì„í•  ë¡œê·¸ ID (í•„ìˆ˜)
        time_window: ì „í›„ ì‹œê°„ ë²”ìœ„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ 60ì´ˆ)

    ê´€ë ¨ ë„êµ¬:
    - get_logs_by_trace_id: trace_idë¡œ ì—°ê´€ ë¡œê·¸ ì¡°íšŒ
    - correlate_logs: ì‹œê°„ ê¸°ë°˜ ë¡œê·¸ ìƒê´€ê´€ê³„
    - detect_cascading_failures: ì—°ì‡„ ì¥ì•  íƒì§€

    Returns:
        ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ (ì‹œê°„ìˆœ, ë ˆì´ì–´ë³„)
    """
    # ì¸ë±ìŠ¤ íŒ¨í„´
    index_pattern = f"{project_uuid.replace('-', '_')}_*"

    try:
        # 1. ì›ë³¸ ë¡œê·¸ ì¡°íšŒ
        origin_result = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 1,
                "query": {"term": {"log_id": log_id}},
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS
            }
        )

        origin_hits = origin_result.get("hits", {}).get("hits", [])

        if not origin_hits:
            return (
                f"log_id {log_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                f"ğŸ’¡ í™•ì¸ ì‚¬í•­:\n"
                f"1. log_idê°€ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”\n"
                f"2. get_log_detail ë„êµ¬ë¡œ ë¡œê·¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"
            )

        origin_source = origin_hits[0]["_source"]
        origin_timestamp = origin_source.get("timestamp", "")
        origin_trace_id = origin_source.get("trace_id")
        origin_ip = origin_source.get("requester_ip")

        # 2. ì‹œê°„ ë²”ìœ„ ê³„ì‚°
        try:
            origin_dt = datetime.fromisoformat(origin_timestamp.replace("Z", ""))
            start_time = origin_dt - timedelta(seconds=time_window)
            end_time = origin_dt + timedelta(seconds=time_window)
        except:
            return f"log_id {log_id}ì˜ timestamp íŒŒì‹± ì‹¤íŒ¨"

        # 3. ì—°ê´€ ë¡œê·¸ ê²€ìƒ‰
        must_clauses = [
            {"range": {
                "timestamp": {
                    "gte": start_time.isoformat() + "Z",
                    "lte": end_time.isoformat() + "Z"
                }
            }}
        ]

        # trace_id ìš°ì„ , ì—†ìœ¼ë©´ IP
        if origin_trace_id:
            must_clauses.append({"term": {"trace_id": origin_trace_id}})
            correlation_type = "trace_id"
        elif origin_ip:
            must_clauses.append({"match_phrase": {"requester_ip": origin_ip}})  # IP íƒ€ì… í•„ë“œ ì§€ì›
            correlation_type = "requester_ip"
        else:
            return (
                f"log_id {log_id}ì— trace_idì™€ requester_ipê°€ ëª¨ë‘ ì—†ì–´ ì „íŒŒ ê²½ë¡œë¥¼ ì¶”ì í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                f"ğŸ’¡ ì´ ë¡œê·¸ëŠ” ë…ë¦½ì ì¸ ì—ëŸ¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
            )

        related_result = opensearch_client.search(
            index=index_pattern,
            body={
                "size": 50,
                "query": {"bool": {"must": must_clauses}},
                "sort": [{"timestamp": "asc"}],
                "_source": BASE_FIELDS + LOG_DETAILS_FIELDS + ["ai_analysis.summary"]
            }
        )

        related_hits = related_result.get("hits", {}).get("hits", [])

        if len(related_hits) <= 1:
            return (
                f"log_id {log_id} ì „í›„ {time_window}ì´ˆ ë²”ìœ„ì—ì„œ ì—°ê´€ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                f"ğŸ’¡ ì´ ì—ëŸ¬ëŠ” ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí–ˆê±°ë‚˜, ì‹œê°„ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        # ê²°ê³¼ í¬ë§·íŒ…
        lines = [
            f"## ğŸ”— ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ ë¶„ì„ (log_id: {log_id})",
            f"**ì—°ê´€ ê¸°ì¤€:** {correlation_type}",
            f"**ì‹œê°„ ë²”ìœ„:** Â±{time_window}ì´ˆ",
            f"**ë°œê²¬ ë¡œê·¸:** {len(related_hits)}ê±´",
            ""
        ]

        # ì „íŒŒ ê²½ë¡œ ì‹œê°í™”
        lines.append("### ğŸ“ ì‹œê°„ìˆœ ì „íŒŒ ê²½ë¡œ")
        lines.append("")

        origin_found = False
        error_chain = []

        for i, hit in enumerate(related_hits, 1):
            source = hit["_source"]
            is_origin = source.get("log_id") == log_id

            timestamp_str = source.get("timestamp", "")[:19].replace("T", " ")
            level = source.get("level", "INFO")
            service = source.get("service_name", "unknown")
            layer = source.get("layer", "")
            class_name = source.get("class_name", "")
            method_name = source.get("method_name", "")
            message = source.get("message", "")[:100]

            log_details = source.get("log_details", {})
            http_method = log_details.get("http_method", "")
            request_uri = log_details.get("request_uri", "")
            response_status = log_details.get("response_status")
            exception_type = log_details.get("exception_type", "")

            # ë ˆë²¨ë³„ ì´ëª¨ì§€
            level_emoji = {"ERROR": "ğŸ”´", "WARN": "ğŸŸ¡", "INFO": "ğŸŸ¢"}.get(level, "âšª")

            # ì›ë³¸ í‘œì‹œ
            origin_marker = " â¬…ï¸ **ì›ë³¸ ì—ëŸ¬**" if is_origin else ""

            # ë ˆì´ì–´ í‘œì‹œ
            layer_str = f"[{layer}] " if layer else ""

            lines.append(f"{i}. {level_emoji} **{timestamp_str}**{origin_marker}")
            lines.append(f"   {layer_str}{service}")

            if class_name and method_name:
                lines.append(f"   ğŸ“¦ {class_name}.{method_name}")

            if http_method and request_uri:
                status_str = f" â†’ {response_status}" if response_status else ""
                lines.append(f"   ğŸŒ {http_method} {request_uri}{status_str}")

            if exception_type:
                lines.append(f"   âŒ {exception_type}")

            if message:
                lines.append(f"   ğŸ’¬ {message}")

            # ì—ëŸ¬ ì²´ì¸ êµ¬ì„±
            if level == "ERROR":
                error_chain.append({
                    "service": service,
                    "layer": layer,
                    "exception": exception_type or "Unknown"
                })

            lines.append("")

        # ì—ëŸ¬ ì²´ì¸ ìš”ì•½
        if len(error_chain) > 1:
            lines.append("### ğŸš¨ ì—ëŸ¬ ì—°ì‡„ ë°˜ì‘")
            lines.append("")
            lines.append("```")
            for j, err in enumerate(error_chain, 1):
                arrow = " â†“" if j < len(error_chain) else ""
                layer_tag = f"[{err['layer']}] " if err['layer'] else ""
                lines.append(f"{j}. {layer_tag}{err['service']} - {err['exception']}{arrow}")
            lines.append("```")
            lines.append("")

        # ë¶„ì„ ìš”ì•½
        lines.append("### ğŸ’¡ ì „íŒŒ ë¶„ì„")
        lines.append("")

        if len(error_chain) > 1:
            lines.append(f"- **ì—°ì‡„ ì—ëŸ¬:** {len(error_chain)}ê°œ ì»´í¬ë„ŒíŠ¸ì—ì„œ ì—ëŸ¬ ë°œìƒ")
            first_service = error_chain[0]['service']
            last_service = error_chain[-1]['service']
            if first_service != last_service:
                lines.append(f"- **ì „íŒŒ ê²½ë¡œ:** {first_service} â†’ {last_service}")
        else:
            lines.append("- **ë…ë¦½ ì—ëŸ¬:** ì—°ì‡„ ë°˜ì‘ ì—†ìŒ")

        lines.append(f"- **ì—°ê´€ ê¸°ì¤€:** {correlation_type} ë§¤ì¹­")

        # ê¶Œì¥ ì¡°ì¹˜
        if len(error_chain) > 2:
            lines.append("")
            lines.append("### âœ… ê¶Œì¥ ì¡°ì¹˜")
            lines.append(f"1. ìµœì´ˆ ì—ëŸ¬ ì›ì¸ ìˆ˜ì •: {error_chain[0]['service']} ì„œë¹„ìŠ¤")
            lines.append(f"2. ì—ëŸ¬ ì „íŒŒ ì°¨ë‹¨: ì¤‘ê°„ ë ˆì´ì–´ ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”")
            lines.append(f"3. ì—°ì‡„ ì¥ì•  ëª¨ë‹ˆí„°ë§: detect_cascading_failures ë„êµ¬ í™œìš©")

        return "\n".join(lines)

    except Exception as e:
        return f"ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ ì¶”ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
