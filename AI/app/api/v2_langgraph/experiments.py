"""
V2 LangGraph API - Vector AI vs DB Experiment Endpoints

Vector ê²€ìƒ‰ + AI ì¶”ë¡ ì´ OpenSearch ì§‘ê³„ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦
"""

import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query
from typing import List
from datetime import datetime

from app.models.experiment import (
    ExperimentComparison,
    VectorAIExperiment,
    VectorAIResult,
    AccuracyMetrics,
    DBStatistics,
    ExperimentConclusion
)
# ê¸°ì¡´ Vector AI vs DB ì‹¤í—˜ì€ ERROR íŒ¨í„´ ë¶„ì„ ì‹¤í—˜ìœ¼ë¡œ ë³€ê²½ë¨
# from app.tools.vector_experiment_tools import (
#     _get_db_error_statistics,
#     ...
# )

logger = logging.getLogger(__name__)


router = APIRouter(prefix="/v2-langgraph", tags=["Vector AI Experiments"])


# ê¸°ì¡´ Vector AI vs DB ì‹¤í—˜ ì—”ë“œí¬ì¸íŠ¸ ë¹„í™œì„±í™” (ERROR íŒ¨í„´ ë¶„ì„ ì‹¤í—˜ìœ¼ë¡œ ëŒ€ì²´ë¨)
# @router.get("/experiments/vector-vs-db", response_model=ExperimentComparison)
async def compare_vector_ai_vs_db_DEPRECATED(
    project_uuid: str = Query(..., description="í”„ë¡œì íŠ¸ UUID"),
    time_hours: int = Query(24, ge=1, le=168, description="ë¶„ì„ ê¸°ê°„ (ì‹œê°„)"),
    k_values: str = Query("100,500,1000", description="í…ŒìŠ¤íŠ¸í•  k ê°’ (ì½¤ë§ˆë¡œ êµ¬ë¶„, ì˜ˆ: 100,500,1000)")
): # -> ExperimentComparison:
    """
    DEPRECATED: ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ERROR íŒ¨í„´ ë¶„ì„ ì‹¤í—˜ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.
    test_experiment_with_localhost.pyë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
    """
    raise HTTPException(
        status_code=410,
        detail="This endpoint is deprecated. Use ERROR pattern analysis experiment instead."
    )

    try:
        # k ê°’ íŒŒì‹±
        try:
            k_list = [int(k.strip()) for k in k_values.split(",")]
            if not k_list:
                raise ValueError("k ê°’ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            if any(k <= 0 for k in k_list):
                raise ValueError("k ê°’ì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            if any(k > 10000 for k in k_list):
                raise ValueError("k ê°’ì€ 10000 ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        except ValueError as e:
            raise HTTPException(
                status_code=400,
                detail=f"k_values íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
            )

        logger.info(f"ğŸ“Š ì‹¤í—˜ k ê°’: {k_list}")

        # 1. Ground Truth: OpenSearch ì§‘ê³„ë¡œ ì •í™•í•œ í†µê³„ ì¡°íšŒ
        logger.debug(f"1ë‹¨ê³„: OpenSearch ì§‘ê³„ë¡œ Ground Truth ì¡°íšŒ")
        import time
        db_start = time.time()
        db_stats = _get_db_statistics_for_experiment(project_uuid, time_hours)
        db_time_ms = (time.time() - db_start) * 1000
        logger.info(f"âœ… Ground Truth ì¡°íšŒ ì™„ë£Œ: total_logs={db_stats['total_logs']}, time={db_time_ms:.2f}ms")

        if db_stats["total_logs"] == 0:
            logger.warning(f"âš ï¸ ë¡œê·¸ ë°ì´í„° ì—†ìŒ: project_uuid={project_uuid}")
            raise HTTPException(
                status_code=404,
                detail=f"ìµœê·¼ {time_hours}ì‹œê°„ ë™ì•ˆ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        # 2. ê° k ê°’ì— ëŒ€í•´ Vector AI ì‹¤í—˜ ìˆ˜í–‰
        vector_ai_experiments = []

        for k in k_list:
            logger.info(f"ğŸ”¬ k={k} ì‹¤í—˜ ì‹œì‘")

            # Vector ê²€ìƒ‰ìœ¼ë¡œ ìƒ˜í”Œ ìˆ˜ì§‘
            logger.debug(f"  - Vector KNN ê²€ìƒ‰ (k={k})")
            ai_start = time.time()
            samples = await _vector_search_all(project_uuid, k, time_hours)
            logger.info(f"  âœ… ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ: {len(samples)}ê°œ")

            if not samples:
                logger.warning(f"  âš ï¸ k={k}ì—ì„œ ìƒ˜í”Œ ìˆ˜ì§‘ ì‹¤íŒ¨")
                continue

            # ìƒ˜í”Œ ë¶„í¬ ê³„ì‚°
            sample_distribution = {}
            for sample in samples:
                level = sample.get("level", "UNKNOWN")
                sample_distribution[level] = sample_distribution.get(level, 0) + 1

            logger.debug(f"  - ìƒ˜í”Œ ë¶„í¬: {sample_distribution}")

            # LLMì´ ìƒ˜í”Œë§Œ ë³´ê³  ì „ì²´ í†µê³„ ì¶”ë¡  (level_counts íŒíŠ¸ ì—†ìŒ!)
            logger.debug(f"  - LLM í†µê³„ ì¶”ë¡  ì‹œì‘")
            ai_stats = await _llm_estimate_from_vectors(
                samples,
                db_stats["total_logs"],  # ì´ ê°œìˆ˜ë§Œ íŒíŠ¸ë¡œ ì œê³µ
                time_hours
            )
            ai_time_ms = (time.time() - ai_start) * 1000
            logger.info(f"  âœ… LLM ì¶”ë¡  ì™„ë£Œ: estimated_error_count={ai_stats['estimated_error_count']}, time={ai_time_ms:.2f}ms")

            # ì •í™•ë„ ê³„ì‚°
            logger.debug(f"  - ì •í™•ë„ ê³„ì‚° ì¤‘")
            accuracy_metrics = _calculate_accuracy_for_experiment(db_stats, ai_stats)
            logger.info(f"  âœ… ì •í™•ë„: overall={accuracy_metrics['overall_accuracy']:.2f}%")

            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
            vector_ai_experiments.append(VectorAIExperiment(
                k=k,
                ai_result=VectorAIResult(
                    k=k,
                    estimated_total_logs=ai_stats["estimated_total_logs"],
                    estimated_error_count=ai_stats["estimated_error_count"],
                    estimated_warn_count=ai_stats["estimated_warn_count"],
                    estimated_info_count=ai_stats["estimated_info_count"],
                    estimated_error_rate=ai_stats["estimated_error_rate"],
                    reasoning=ai_stats["reasoning"],
                    confidence_score=ai_stats["confidence_score"],
                    processing_time_ms=ai_time_ms
                ),
                accuracy_metrics=AccuracyMetrics(
                    total_logs_accuracy=accuracy_metrics["total_logs_accuracy"],
                    error_count_accuracy=accuracy_metrics["error_count_accuracy"],
                    warn_count_accuracy=accuracy_metrics["warn_count_accuracy"],
                    info_count_accuracy=accuracy_metrics["info_count_accuracy"],
                    error_rate_accuracy=accuracy_metrics["error_rate_accuracy"],
                    overall_accuracy=accuracy_metrics["overall_accuracy"]
                ),
                sample_distribution=sample_distribution
            ))

        if not vector_ai_experiments:
            logger.error(f"ğŸ”´ ëª¨ë“  k ê°’ì—ì„œ ì‹¤í—˜ ì‹¤íŒ¨")
            raise HTTPException(
                status_code=500,
                detail="Vector AI ì‹¤í—˜ì—ì„œ ìƒ˜í”Œì„ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # 3. ìµœì  k ê°’ ì°¾ê¸° ë° ê²°ë¡  ë„ì¶œ
        logger.debug(f"3ë‹¨ê³„: ìµœì  k ê°’ ë° ê²°ë¡  ë„ì¶œ")
        best_experiment = max(vector_ai_experiments, key=lambda e: e.accuracy_metrics.overall_accuracy)
        best_k = best_experiment.k
        best_accuracy = best_experiment.accuracy_metrics.overall_accuracy

        logger.info(f"ğŸ† ìµœì  k ê°’: k={best_k}, accuracy={best_accuracy:.2f}%")

        # ë“±ê¸‰ ë° ì‹¤ìš©ì„± í‰ê°€
        if best_accuracy >= 95:
            grade = "ë§¤ìš° ìš°ìˆ˜"
            feasible = True
            recommendation = "Vector AIê°€ DB ì§‘ê³„ë¥¼ ì™„ì „íˆ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif best_accuracy >= 90:
            grade = "ìš°ìˆ˜"
            feasible = True
            recommendation = "Vector AIê°€ DB ì§‘ê³„ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì¡°ì • í›„ í”„ë¡œë•ì…˜ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif best_accuracy >= 80:
            grade = "ì–‘í˜¸"
            feasible = False
            recommendation = "Vector AIë¥¼ ë³´ì¡° ë„êµ¬ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” í†µê³„ëŠ” DB ì§‘ê³„ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        elif best_accuracy >= 70:
            grade = "ë³´í†µ"
            feasible = False
            recommendation = "Vector AIì˜ ì •í™•ë„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ íŠœë‹ ë° ìƒ˜í”Œë§ ì „ëµ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            grade = "ë¯¸í¡"
            feasible = False
            recommendation = "Vector AIê°€ DB ì§‘ê³„ë¥¼ ëŒ€ì²´í•˜ê¸°ì—ëŠ” ì •í™•ë„ê°€ ë§¤ìš° ë¶€ì¡±í•©ë‹ˆë‹¤. ê·¼ë³¸ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."

        # ì„±ëŠ¥ ë¹„êµ
        performance_vs_db = {
            "db_processing_time_ms": round(db_time_ms, 2),
            "best_ai_processing_time_ms": round(best_experiment.ai_result.processing_time_ms, 2),
            "speed_ratio": round(best_experiment.ai_result.processing_time_ms / db_time_ms, 2) if db_time_ms > 0 else 0,
            "accuracy_trade_off": round(100 - best_accuracy, 2),
            "verdict": "DBê°€ ë” ë¹ ë¥´ê³  ì •í™•í•¨" if best_experiment.ai_result.processing_time_ms > db_time_ms else "Vector AIê°€ ë” ë¹ ë¦„ (ë‹¨, ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡´ì¬)"
        }

        conclusion = ExperimentConclusion(
            best_k=best_k,
            best_accuracy=best_accuracy,
            feasible=feasible,
            grade=grade,
            recommendation=recommendation,
            performance_vs_db=performance_vs_db
        )

        # 4. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []

        # k ê°’ì— ë”°ë¥¸ ì •í™•ë„ ë³€í™”
        if len(vector_ai_experiments) > 1:
            accuracies = [e.accuracy_metrics.overall_accuracy for e in vector_ai_experiments]
            k_vals = [e.k for e in vector_ai_experiments]
            if accuracies[-1] > accuracies[0]:
                insights.append(f"k ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ ì •í™•ë„ê°€ í–¥ìƒë¨ (k={k_vals[0]}: {accuracies[0]:.1f}% â†’ k={k_vals[-1]}: {accuracies[-1]:.1f}%)")
            else:
                insights.append(f"k={k_vals[0]}ì—ì„œë„ ì¶©ë¶„í•œ ì •í™•ë„ ë‹¬ì„± (ì¶”ê°€ ìƒ˜í”Œë§ ë¶ˆí•„ìš”)")

        # ì—ëŸ¬ íƒì§€ ì •í™•ë„
        error_accuracy = best_experiment.accuracy_metrics.error_count_accuracy
        if error_accuracy >= 90:
            insights.append(f"ERROR ë¡œê·¸ ê°œìˆ˜ ì¶”ë¡  ì •í™•ë„ {error_accuracy:.1f}% (ì´ìƒ íƒì§€ì— í™œìš© ê°€ëŠ¥)")
        else:
            insights.append(f"ERROR ë¡œê·¸ ì¶”ë¡  ì •í™•ë„ {error_accuracy:.1f}% (ê°œì„  í•„ìš”)")

        # ìƒ˜í”Œ íš¨ìœ¨ì„±
        sample_ratio = best_k / db_stats["total_logs"] * 100 if db_stats["total_logs"] > 0 else 0
        insights.append(f"ì „ì²´ ë¡œê·¸ì˜ {sample_ratio:.2f}%ë§Œ ì¡°íšŒí•˜ì—¬ {best_accuracy:.1f}% ì •í™•ë„ ë‹¬ì„±")

        # ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„
        if performance_vs_db["speed_ratio"] < 1:
            insights.append(f"Vector AIê°€ DB ì§‘ê³„ ëŒ€ë¹„ {(1-performance_vs_db['speed_ratio'])*100:.0f}% ë” ë¹ ë¦„")
        else:
            insights.append(f"DB ì§‘ê³„ê°€ Vector AI ëŒ€ë¹„ {(performance_vs_db['speed_ratio']-1)*100:.0f}% ë” ë¹ ë¦„")

        # AI ì‹ ë¢°ë„
        ai_confidence = best_experiment.ai_result.confidence_score
        insights.append(f"AI ìì²´ ì‹ ë¢°ë„: {ai_confidence}/100 (ì‹¤ì œ ì •í™•ë„: {best_accuracy:.1f}%)")

        # 5. ì‘ë‹µ êµ¬ì„±
        logger.info(f"âœ… ì‹¤í—˜ ì™„ë£Œ: best_k={best_k}, best_accuracy={best_accuracy:.2f}%, feasible={feasible}")

        return ExperimentComparison(
            project_uuid=project_uuid,
            experiment_time=datetime.utcnow(),
            time_range_hours=time_hours,
            ground_truth=DBStatistics(
                total_logs=db_stats["total_logs"],
                error_count=db_stats["error_count"],
                warn_count=db_stats["warn_count"],
                info_count=db_stats["info_count"],
                error_rate=db_stats["error_rate"],
                processing_time_ms=db_time_ms
            ),
            vector_ai_experiments=vector_ai_experiments,
            conclusion=conclusion,
            insights=insights
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ”´ Vector AI vs DB ì‹¤í—˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ: project_uuid={project_uuid}, error={str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )
