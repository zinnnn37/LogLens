#!/usr/bin/env python3
"""
Models Validation Test
V2 í•„ë“œ(sources, validation, analysis_metadata)ê°€ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
"""

import sys
from dotenv import load_dotenv
load_dotenv()


def test_chat_response_model():
    """ChatResponseì— sources, validation í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸"""
    print("=" * 80)
    print("Test 1: ChatResponse Model Validation")
    print("=" * 80)

    from app.models.chat import ChatResponse
    from app.models.experiment import LogSource, ValidationInfo

    # LogSource ìƒì„±
    source = LogSource(
        log_id="12345",
        timestamp="2024-01-15T10:30:00Z",
        level="ERROR",
        message="Test NullPointerException in UserService",
        service_name="test-service",
        relevance_score=0.95,
        class_name="UserService",
        method_name="getUser"
    )

    print(f"âœ… LogSource ìƒì„± ì„±ê³µ")
    print(f"   log_id: {source.log_id}")
    print(f"   level: {source.level}")
    print(f"   relevance_score: {source.relevance_score}")

    # ValidationInfo ìƒì„±
    validation = ValidationInfo(
        confidence=85,
        sample_count=10,
        sampling_strategy="proportional_vector_knn",
        coverage="ìµœê·¼ 24ì‹œê°„ ERROR ë¡œê·¸ ë¶„ì„",
        data_quality="high",
        limitation="Vector ê²€ìƒ‰ì€ ERROR ë¡œê·¸ë§Œ ì§€ì›. WARN/INFOëŠ” ê¸°ë³¸ í•„í„° ì‚¬ìš©",
        note="Agentê°€ ììœ¨ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ë¶„ì„"
    )

    print(f"âœ… ValidationInfo ìƒì„± ì„±ê³µ")
    print(f"   confidence: {validation.confidence}/100")
    print(f"   sample_count: {validation.sample_count}ê°œ")
    print(f"   sampling_strategy: {validation.sampling_strategy}")
    print(f"   data_quality: {validation.data_quality}")

    # ChatResponse ìƒì„± (V2 í•„ë“œ í¬í•¨)
    response = ChatResponse(
        answer="í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤",
        from_cache=False,
        related_logs=[],
        sources=[source],
        validation=validation
    )

    # ê²€ì¦
    assert response.sources is not None, "sources í•„ë“œê°€ Noneì…ë‹ˆë‹¤"
    assert len(response.sources) == 1, f"sources ê°œìˆ˜ê°€ ì˜ëª»ë¨: {len(response.sources)}"
    assert response.validation is not None, "validation í•„ë“œê°€ Noneì…ë‹ˆë‹¤"
    assert response.validation.confidence == 85, f"confidence ê°’ ë¶ˆì¼ì¹˜: {response.validation.confidence}"

    print(f"âœ… ChatResponse ëª¨ë¸ ê²€ì¦ ì„±ê³µ")
    print(f"   sources: {len(response.sources)}ê°œ")
    print(f"   validation.confidence: {response.validation.confidence}")
    print()

    return True


def test_analysis_response_model():
    """LogAnalysisResponseì— sources, validation í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸"""
    print("=" * 80)
    print("Test 2: LogAnalysisResponse Model Validation")
    print("=" * 80)

    from app.models.analysis import LogAnalysisResponse, LogAnalysisResult
    from app.models.experiment import LogSource, ValidationInfo

    # LogAnalysisResult ìƒì„±
    analysis = LogAnalysisResult(
        summary="UserServiceì˜ getUser() ë©”ì„œë“œì—ì„œ NullPointerException ë°œìƒ",
        error_cause="user_idì— í•´ë‹¹í•˜ëŠ” User ê°ì²´ê°€ null",
        solution="[ì¦‰ì‹œ] null ì²´í¬ ì¶”ê°€",
        tags=["SEVERITY_HIGH", "NullPointerException", "UserService"]
    )

    print(f"âœ… LogAnalysisResult ìƒì„± ì„±ê³µ")
    print(f"   summary: {analysis.summary[:50]}...")

    # LogSource ìƒì„±
    source = LogSource(
        log_id="12345",
        timestamp="2024-01-15T10:30:00Z",
        level="ERROR",
        message="NullPointerException in UserService.getUser()",
        service_name="user-service",
        relevance_score=0.98
    )

    # ValidationInfo ìƒì„±
    validation = ValidationInfo(
        confidence=90,
        sample_count=5,
        sampling_strategy="trace_id_filter",
        coverage="Trace ê¸°ë°˜ 5ê°œ ë¡œê·¸ ë¶„ì„",
        data_quality="high"
    )

    # LogAnalysisResponse ìƒì„± (V2 í•„ë“œ í¬í•¨)
    response = LogAnalysisResponse(
        log_id=12345,
        analysis=analysis,
        from_cache=False,
        similar_log_id=None,
        similarity_score=None,
        sources=[source],
        validation=validation
    )

    # ê²€ì¦
    assert response.log_id == 12345
    assert response.sources is not None
    assert len(response.sources) == 1
    assert response.validation is not None
    assert response.validation.sampling_strategy == "trace_id_filter"

    print(f"âœ… LogAnalysisResponse ëª¨ë¸ ê²€ì¦ ì„±ê³µ")
    print(f"   log_id: {response.log_id}")
    print(f"   sources: {len(response.sources)}ê°œ")
    print(f"   validation.sampling_strategy: {response.validation.sampling_strategy}")
    print()

    return True


def test_document_metadata_model():
    """AiDocumentMetadataì— analysis_metadata í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸"""
    print("=" * 80)
    print("Test 3: AiDocumentMetadata Model Validation")
    print("=" * 80)

    from app.models.document import AiDocumentMetadata
    from app.models.experiment import AnalysisMetadata
    from datetime import datetime

    # AnalysisMetadata ìƒì„±
    analysis_metadata = AnalysisMetadata(
        generated_at=datetime.utcnow(),
        data_range="2025-11-17 ~ 2025-11-18",
        total_logs_analyzed=1000,
        error_logs=50,
        warn_logs=150,
        info_logs=800,
        sample_strategy={
            "error_strategy": "proportional_vector_knn",
            "warn_strategy": "random_filter",
            "info_strategy": "random_filter"
        },
        limitations=[
            "Vector ê²€ìƒ‰ì€ ERROR ë¡œê·¸ë§Œ ì§€ì›",
            "WARN/INFOëŠ” ë²¡í„°í™”ë˜ì§€ ì•ŠìŒ"
        ]
    )

    print(f"âœ… AnalysisMetadata ìƒì„± ì„±ê³µ")
    print(f"   total_logs_analyzed: {analysis_metadata.total_logs_analyzed:,}")
    print(f"   error_logs: {analysis_metadata.error_logs}")

    # AiDocumentMetadata ìƒì„± (V2 í•„ë“œ í¬í•¨)
    metadata = AiDocumentMetadata(
        word_count=1500,
        estimated_reading_time="7 minutes",
        sections_generated=["overview", "errors", "recommendations"],
        generation_time=3.5,
        health_score=85,
        critical_issues=5,
        analysis_metadata=analysis_metadata
    )

    # ê²€ì¦
    assert metadata.word_count == 1500
    assert metadata.analysis_metadata is not None
    assert metadata.analysis_metadata.total_logs_analyzed == 1000
    assert metadata.analysis_metadata.error_logs == 50

    print(f"âœ… AiDocumentMetadata ëª¨ë¸ ê²€ì¦ ì„±ê³µ")
    print(f"   word_count: {metadata.word_count:,}")
    print(f"   analysis_metadata.error_logs: {metadata.analysis_metadata.error_logs}")
    print(f"   analysis_metadata.limitations: {len(metadata.analysis_metadata.limitations)}ê°œ")
    print()

    return True


def test_sources_tracker():
    """SourcesTracker ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("Test 4: SourcesTracker Utility")
    print("=" * 80)

    from app.utils.sources_tracker import SourcesTracker

    tracker = SourcesTracker()

    # ë¡œê·¸ ì¶”ê°€
    logs = [
        {
            "log_id": 1,
            "level": "ERROR",
            "message": "NullPointerException in UserService",
            "timestamp": "2024-01-15T10:00:00Z",
            "service_name": "user-service",
            "class_name": "UserService",
            "method_name": "getUser"
        },
        {
            "log_id": 2,
            "level": "ERROR",
            "message": "TimeoutException in PaymentService",
            "timestamp": "2024-01-15T10:01:00Z",
            "service_name": "payment-service",
            "class_name": "PaymentService",
            "method_name": "processPayment"
        },
        {
            "log_id": 3,
            "level": "WARN",
            "message": "Slow query detected",
            "timestamp": "2024-01-15T10:02:00Z",
            "service_name": "db-service"
        }
    ]

    relevance_scores = [0.95, 0.88, 0.72]

    tracker.add_sources(logs, search_type="vector_knn", relevance_scores=relevance_scores)

    # ê²€ì¦
    assert len(tracker.sources) == 3, f"sources ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(tracker.sources)}"
    assert tracker.error_logs_count == 2, f"ERROR ê°œìˆ˜ ë¶ˆì¼ì¹˜: {tracker.error_logs_count}"
    assert tracker.warn_logs_count == 1, f"WARN ê°œìˆ˜ ë¶ˆì¼ì¹˜: {tracker.warn_logs_count}"
    assert tracker.used_vector_search is True, "Vector ê²€ìƒ‰ ì‚¬ìš© í”Œë˜ê·¸ ë¯¸ì„¤ì •"

    print(f"âœ… ì¶œì²˜ ì¶”ì  ì„±ê³µ")
    print(f"   ì´ sources: {len(tracker.sources)}ê°œ")
    print(f"   ERROR ë¡œê·¸: {tracker.error_logs_count}ê°œ")
    print(f"   WARN ë¡œê·¸: {tracker.warn_logs_count}ê°œ")
    print(f"   Vector ê²€ìƒ‰ ì‚¬ìš©: {tracker.used_vector_search}")

    # ValidationInfo ìƒì„±
    validation = tracker.get_validation_info()

    assert validation.sample_count == 3
    assert validation.confidence > 50
    assert validation.data_quality in ["high", "medium", "low"]

    print(f"âœ… ValidationInfo ìƒì„± ì„±ê³µ")
    print(f"   confidence: {validation.confidence}/100")
    print(f"   sample_count: {validation.sample_count}")
    print(f"   sampling_strategy: {validation.sampling_strategy}")
    print(f"   data_quality: {validation.data_quality}")
    print(f"   coverage: {validation.coverage}")

    # ìƒìœ„ ì¶œì²˜ ê°€ì ¸ì˜¤ê¸°
    top_sources = tracker.get_top_sources(limit=2)

    assert len(top_sources) == 2
    # ì²« ë²ˆì§¸ê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜ì—¬ì•¼ í•¨
    assert top_sources[0].relevance_score >= top_sources[1].relevance_score

    print(f"âœ… ìƒìœ„ ì¶œì²˜ ì •ë ¬ ì„±ê³µ")
    print(f"   Top 1: log_id={top_sources[0].log_id}, score={top_sources[0].relevance_score}")
    print(f"   Top 2: log_id={top_sources[1].log_id}, score={top_sources[1].relevance_score}")
    print()

    return True


def main():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print()
    print("ğŸ§ª V2 Models & Utilities Validation Test Suite")
    print()

    results = {}

    try:
        results["ChatResponse Model"] = test_chat_response_model()
    except Exception as e:
        print(f"âŒ ChatResponse í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        results["ChatResponse Model"] = False

    try:
        results["LogAnalysisResponse Model"] = test_analysis_response_model()
    except Exception as e:
        print(f"âŒ LogAnalysisResponse í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        results["LogAnalysisResponse Model"] = False

    try:
        results["AiDocumentMetadata Model"] = test_document_metadata_model()
    except Exception as e:
        print(f"âŒ AiDocumentMetadata í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        results["AiDocumentMetadata Model"] = False

    try:
        results["SourcesTracker Utility"] = test_sources_tracker()
    except Exception as e:
        print(f"âŒ SourcesTracker í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        results["SourcesTracker Utility"] = False

    # ê²°ê³¼ ìš”ì•½
    print("=" * 80)
    print("ğŸ“Š Test Results Summary")
    print("=" * 80)
    print()

    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {status}  {test_name}")

    print()

    all_passed = all(results.values())
    if all_passed:
        print("âœ… All tests passed!")
        print()
        print("ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. AI Service ì‹œì‘ (.envì— OPENAI_API_KEY í•„ìš”)")
        print("   2. Chatbot V2 API í†µí•© í…ŒìŠ¤íŠ¸")
        print("   3. Log ë¶„ì„ V2 API í†µí•© í…ŒìŠ¤íŠ¸")
        print("   4. ERROR íŒ¨í„´ ë¶„ì„ ì‹¤í—˜ ì‹¤í–‰")
    else:
        print("âŒ Some tests failed")
        failed_tests = [name for name, passed in results.items() if not passed]
        print(f"   Failed: {', '.join(failed_tests)}")

    print()

    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
